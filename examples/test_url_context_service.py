"""
URL Context Service í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
URL Context API ë°©ì‹ìœ¼ë¡œ ì›¹í˜ì´ì§€ ë¶„ì„ (í¬ë¡¤ë§ ë¶ˆí•„ìš”)
"""
import sys
import os

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from app.utils.url_context_service import URLContextService


def test_single_webpage():
    """ë‹¨ì¼ ì›¹í˜ì´ì§€ ë¶„ì„ í…ŒìŠ¤íŠ¸"""
    print("=" * 60)
    print("ğŸŒ ë‹¨ì¼ ì›¹í˜ì´ì§€ ë¶„ì„ í…ŒìŠ¤íŠ¸")
    print("=" * 60)

    service = URLContextService()

    # í…ŒìŠ¤íŠ¸í•  URL (ê³µê°œ ë‰´ìŠ¤ ê¸°ì‚¬)
    url = "https://www.bbc.com/news"

    try:
        result = service.analyze_webpage(url)
        print("\nâœ… ë¶„ì„ ê²°ê³¼:")
        print(f"ì œëª©: {result.get('title')}")
        print(f"ìš”ì•½: {result.get('summary')}")
        print(f"ì£¼ìš” ì£¼ì¥:")
        for claim in result.get('key_claims', []):
            print(f"  - {claim}")
        print(f"ì£¼ì œ: {', '.join(result.get('topics', []))}")
        print(f"ê´€ë ¨ êµ­ê°€: {', '.join(result.get('related_countries', []))}")

    except Exception as e:
        print(f"âŒ ì—ëŸ¬ ë°œìƒ: {e}")


def test_multiple_urls():
    """ì—¬ëŸ¬ URL ë¹„êµ ë¶„ì„ í…ŒìŠ¤íŠ¸"""
    print("\n" + "=" * 60)
    print("ğŸ“Š ë‹¤ì¤‘ URL ë¹„êµ ë¶„ì„ í…ŒìŠ¤íŠ¸")
    print("=" * 60)

    service = URLContextService()

    urls = [
        "https://www.bbc.com/news",
        "https://www.cnn.com",
        "https://www.reuters.com"
    ]

    try:
        result = service.analyze_multiple_urls(urls)
        print("\nâœ… ë¹„êµ ë¶„ì„ ê²°ê³¼:")
        print(f"ê³µí†µ ì£¼ì œ: {', '.join(result.get('common_topics', []))}")
        print(f"\nì „ì²´ ìš”ì•½:")
        print(result.get('summary'))

        print(f"\nê´€ì  ì°¨ì´:")
        for perspective in result.get('different_perspectives', []):
            print(f"\nì£¼ì œ: {perspective.get('topic')}")
            print(f"  - URL 1: {perspective.get('url1_view')}")
            print(f"  - URL 2: {perspective.get('url2_view')}")

    except Exception as e:
        print(f"âŒ ì—ëŸ¬ ë°œìƒ: {e}")


def test_article_extraction():
    """ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ í…ŒìŠ¤íŠ¸"""
    print("\n" + "=" * 60)
    print("ğŸ“° ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ í…ŒìŠ¤íŠ¸")
    print("=" * 60)

    service = URLContextService()

    url = "https://www.bbc.com/news"

    try:
        content = service.extract_article_content(url)
        print("\nâœ… ì¶”ì¶œëœ ë³¸ë¬¸:")
        print(content[:500] + "..." if len(content) > 500 else content)

    except Exception as e:
        print(f"âŒ ì—ëŸ¬ ë°œìƒ: {e}")


# ============================================================
# ë¹„ë™ê¸° ë²„ì „ í…ŒìŠ¤íŠ¸ (ì£¼ì„ í•´ì œ ì‹œ ì‚¬ìš© ê°€ëŠ¥)
# ============================================================

# import asyncio
# from app.utils.url_context_service import AsyncURLContextService
#
# async def test_multiple_webpages_async():
#     """ì—¬ëŸ¬ ì›¹í˜ì´ì§€ ë³‘ë ¬ ë¶„ì„ í…ŒìŠ¤íŠ¸"""
#     print("\n" + "=" * 60)
#     print("ğŸš€ ë¹„ë™ê¸° ë³‘ë ¬ ë¶„ì„ í…ŒìŠ¤íŠ¸")
#     print("=" * 60)
#
#     service = AsyncURLContextService()
#
#     urls = [
#         "https://www.bbc.com/news/world",
#         "https://www.cnn.com/world",
#         "https://www.reuters.com/world",
#     ]
#
#     prompt = """
#     ì´ ë‰´ìŠ¤ í˜ì´ì§€ì˜ ì£¼ìš” í—¤ë“œë¼ì¸ì„ ì¶”ì¶œí•´ì£¼ì„¸ìš”.
#
#     ì‘ë‹µ í˜•ì‹:
#     {
#       "headlines": ["í—¤ë“œë¼ì¸ 1", "í—¤ë“œë¼ì¸ 2", ...],
#       "top_story": "ê°€ì¥ ì¤‘ìš”í•œ ë‰´ìŠ¤"
#     }
#     """
#
#     try:
#         results = await service.analyze_multiple_webpages(
#             urls,
#             analysis_prompt=prompt,
#             max_concurrent=3
#         )
#
#         print(f"\nâœ… {len(results)}ê°œ í˜ì´ì§€ ë¶„ì„ ì™„ë£Œ:")
#         for i, result in enumerate(results, 1):
#             print(f"\n[í˜ì´ì§€ {i}]")
#             print(f"í—¤ë“œë¼ì¸: {', '.join(result.get('headlines', []))}")
#             print(f"í†± ìŠ¤í† ë¦¬: {result.get('top_story')}")
#
#     except Exception as e:
#         print(f"âŒ ì—ëŸ¬ ë°œìƒ: {e}")


if __name__ == "__main__":
    # ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    test_single_webpage()
    test_multiple_urls()
    test_article_extraction()

    # ë¹„ë™ê¸° í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ì£¼ì„ í•´ì œ ì‹œ)
    # asyncio.run(test_multiple_webpages_async())
