"""
Content extractors for different media types
"""
from abc import ABC, abstractmethod
import os
import uuid
import tempfile
import requests
from bs4 import BeautifulSoup
from youtube_transcript_api import YouTubeTranscriptApi
import yt_dlp

import vertexai
from vertexai.preview.generative_models import GenerativeModel, Part
from google.cloud import storage

from app.config import config


class BaseExtractor(ABC):
    """ì½˜í…ì¸  ì¶”ì¶œê¸° ê¸°ë³¸ í´ë˜ìŠ¤"""

    @abstractmethod
    def extract(self, url: str) -> str:
        """URLì—ì„œ í…ìŠ¤íŠ¸ ì½˜í…ì¸ ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        pass


class YoutubeExtractor(BaseExtractor):
    """ìœ íŠœë¸Œ ìë§‰ ì¶”ì¶œ ì „ëµ (3ë‹¨ê³„ í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹)"""

    def __init__(self):
        """GCS, Gemini, YouTube Video Service ì´ˆê¸°í™”"""
        # GCS ë° Gemini ëª¨ë¸ ì´ˆê¸°í™” (yt-dlp ë°©ì‹ìš©)
        self.storage_client = None
        self.bucket = None
        self.gemini_model = None

        if config.GCS_BUCKET_NAME:
            try:
                self.storage_client = storage.Client(project=config.GCP_PROJECT)
                self.bucket = self.storage_client.bucket(config.GCS_BUCKET_NAME)

                # Gemini 2.0 ëª¨ë¸ ì´ˆê¸°í™”
                vertexai.init(project=config.GCP_PROJECT, location=config.GCP_REGION)
                self.gemini_model = GenerativeModel('gemini-2.0-flash-exp')
                print("âœ… (YoutubeExtractor) GCS ë° Gemini ì—°ê²° ì„±ê³µ")
            except Exception as e:
                print(f"âš ï¸ (YoutubeExtractor) GCS/Gemini ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

        # Direct URL Processing ì„œë¹„ìŠ¤ ì´ˆê¸°í™” (í´ë°±ìš©)
        self.video_service = None
        try:
            from app.utils.youtube_video_service import YouTubeVideoService
            self.video_service = YouTubeVideoService()
            print("âœ… (YoutubeExtractor) YouTube Video Service ì—°ê²° ì„±ê³µ")
        except Exception as e:
            print(f"âš ï¸ (YoutubeExtractor) YouTube Video Service ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

    def extract(self, url: str) -> str:
        """3ë‹¨ê³„ í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹: ìë§‰ â†’ yt-dlp+GCS â†’ Direct URL Processing"""

        errors = []

        # 1ë‹¨ê³„: ìë§‰ ì¶”ì¶œ ì‹œë„ (ê°€ì¥ ë¹ ë¦„)
        try:
            print("ğŸ“ [1/3] ìë§‰ ì¶”ì¶œ ì‹œë„ ì¤‘...")
            transcript_text = self._extract_transcript(url)
            print(f"âœ… ìë§‰ ì¶”ì¶œ ì„±ê³µ: {len(transcript_text)} ê¸€ì")
            return transcript_text
        except Exception as transcript_error:
            error_msg = f"ìë§‰ ì¶”ì¶œ ì‹¤íŒ¨: {transcript_error}"
            print(f"âš ï¸ {error_msg}")
            errors.append(error_msg)

        # 2ë‹¨ê³„: yt-dlp + GCS ë°©ì‹ ì‹œë„ (ì´ì „ ì‘ë™ ë°©ì‹)
        if self.gemini_model and self.bucket:
            try:
                print("ğŸ¬ [2/3] yt-dlp + GCS ë°©ì‹ ì‹œë„ ì¤‘...")
                video_analysis = self._analyze_video_with_ytdlp_gcs(url)
                print(f"âœ… yt-dlp ë¶„ì„ ì„±ê³µ: {len(video_analysis)} ê¸€ì")
                return video_analysis
            except Exception as ytdlp_error:
                error_msg = f"yt-dlp ë°©ì‹ ì‹¤íŒ¨: {ytdlp_error}"
                print(f"âš ï¸ {error_msg}")
                errors.append(error_msg)

        # 3ë‹¨ê³„: Direct URL Processing ì‹œë„ (ìµœí›„ ìˆ˜ë‹¨)
        if self.video_service:
            try:
                print("ğŸŒ [3/3] Direct URL Processing ì‹œë„ ì¤‘...")
                result = self.video_service.analyze_video(url, analysis_type="transcript")

                transcript = result.get('transcript', '')
                if transcript:
                    print(f"âœ… Direct URL ë¶„ì„ ì„±ê³µ: {len(transcript)} ê¸€ì")
                    return transcript
                else:
                    raise Exception("ì˜ìƒ ë¶„ì„ ê²°ê³¼ì— transcriptê°€ ì—†ìŠµë‹ˆë‹¤")

            except Exception as direct_error:
                error_msg = f"Direct URL ë°©ì‹ ì‹¤íŒ¨: {direct_error}"
                print(f"âŒ {error_msg}")
                errors.append(error_msg)

        # ëª¨ë“  ë°©ë²• ì‹¤íŒ¨
        raise Exception(
            f"ëª¨ë“  ì˜ìƒ ë¶„ì„ ë°©ë²•ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤:\n" + "\n".join(f"- {e}" for e in errors)
        )

    def _extract_transcript(self, url: str) -> str:
        """ìœ íŠœë¸Œ ìë§‰ ì¶”ì¶œ (ê¸°ì¡´ ë¡œì§)"""
        if 'v=' in url:
            video_id = url.split('v=')[1].split('&')[0]
        elif 'youtu.be/' in url:
            video_id = url.split('youtu.be/')[1].split('?')[0]
        else:
            raise ValueError("ìœ íš¨í•˜ì§€ ì•Šì€ ìœ íŠœë¸Œ URL")

        transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)

        try:
            # 1. í•œêµ­ì–´ ìë§‰ ì‹œë„
            transcript = transcript_list.find_transcript(['ko'])
        except:
            # 2. ì˜ì–´ ìë§‰ ì‹œë„
            transcript = transcript_list.find_transcript(['en'])

        text = ' '.join([item['text'] for item in transcript.fetch()])
        return text

    def _analyze_video_with_ytdlp_gcs(self, url: str) -> str:
        """yt-dlp + GCS ë°©ì‹ìœ¼ë¡œ ì˜ìƒ ë¶„ì„ (ì´ì „ ì‘ë™ ë°©ì‹)"""
        local_video_path = None
        gcs_blob_name = None

        try:
            # 1. ì˜ìƒ ë‹¤ìš´ë¡œë“œ (progressive=Trueë¡œ ë³‘í•©ëœ ìŠ¤íŠ¸ë¦¼ë§Œ ì„ íƒ)
            temp_dir = tempfile.gettempdir()
            unique_filename = f"{uuid.uuid4()}.mp4"
            local_video_path = os.path.join(temp_dir, unique_filename)

            ydl_opts = {
                'format': 'bestvideo[ext=mp4][progressive=True][height<=720]/best[ext=mp4][progressive=True]/best[ext=mp4]',
                'outtmpl': local_video_path,
                'quiet': False,  # ë””ë²„ê¹…ìš©
            }

            print(f"ğŸ“¥ yt-dlpë¡œ ì˜ìƒ ë‹¤ìš´ë¡œë“œ ì¤‘...")
            with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                ydl.download([url])

            if not os.path.exists(local_video_path):
                raise Exception("yt-dlp ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ (íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•ŠìŒ)")

            print(f"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {local_video_path}")

            # 2. GCS ì—…ë¡œë“œ
            gcs_blob_name = f"video-analysis/{unique_filename}"
            blob = self.bucket.blob(gcs_blob_name)

            print(f"â˜ï¸ GCS ì—…ë¡œë“œ ì¤‘...")
            blob.upload_from_filename(local_video_path)
            gcs_uri = f"gs://{config.GCS_BUCKET_NAME}/{gcs_blob_name}"
            print(f"âœ… GCS ì—…ë¡œë“œ ì™„ë£Œ: {gcs_uri}")

            # 3. Gemini API í˜¸ì¶œ
            print(f"ğŸ¤– Geminië¡œ ì˜ìƒ ë¶„ì„ ì¤‘...")
            video_part = Part.from_uri(uri=gcs_uri, mime_type="video/mp4")

            prompt = """
            ì´ ì˜ìƒì˜ ë‚´ìš©ì„ ìƒì„¸íˆ ë¶„ì„í•˜ì—¬ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”.

            ë‹¤ìŒ ì •ë³´ë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”:
            1. ì˜ìƒì˜ ì£¼ìš” ì£¼ì œì™€ í•µì‹¬ ë©”ì‹œì§€
            2. ì–¸ê¸‰ëœ êµ¬ì²´ì ì¸ ì‚¬ì‹¤, í†µê³„, ì£¼ì¥
            3. í™”ìì˜ ì£¼ìš” ë…¼ì ê³¼ ê·¼ê±°
            4. ì¤‘ìš”í•œ ë§¥ë½ì´ë‚˜ ë°°ê²½ ì •ë³´

            ê°€ëŠ¥í•œ í•œ ìƒì„¸í•˜ê³  ì •í™•í•˜ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”.
            """

            response = self.gemini_model.generate_content([prompt, video_part])
            return response.text

        finally:
            # 4. ì •ë¦¬ (ë¡œì»¬ íŒŒì¼ ë° GCS íŒŒì¼ ì‚­ì œ)
            try:
                if local_video_path and os.path.exists(local_video_path):
                    os.remove(local_video_path)
                    print(f"ğŸ—‘ï¸ ë¡œì»¬ íŒŒì¼ ì‚­ì œ ì™„ë£Œ")

                if gcs_blob_name and self.bucket:
                    blob = self.bucket.blob(gcs_blob_name)
                    if blob.exists():
                        blob.delete()
                        print(f"ğŸ—‘ï¸ GCS íŒŒì¼ ì‚­ì œ ì™„ë£Œ")
            except Exception as cleanup_error:
                print(f"âš ï¸ ì •ë¦¬ ì‘ì—… ì‹¤íŒ¨: {cleanup_error}")


class ArticleExtractor(BaseExtractor):
    """ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ ì „ëµ (í–¥ìƒëœ ë´‡ ë°©ì–´ ìš°íšŒ)"""

    def extract_with_title(self, url: str) -> dict:
        """URLì—ì„œ ì œëª©ê³¼ ë³¸ë¬¸ì„ ëª¨ë‘ ì¶”ì¶œí•©ë‹ˆë‹¤.

        Returns:
            {'title': str, 'content': str}
        """
        try:
            # ë´‡ íƒì§€ ìš°íšŒë¥¼ ìœ„í•œ í˜„ëŒ€ì ì¸ ë¸Œë¼ìš°ì € í—¤ë”
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.5',
                'Accept-Encoding': 'gzip, deflate, br',
                'Referer': 'https://www.google.com/',
                'DNT': '1',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1'
            }

            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            response.encoding = response.apparent_encoding  # ì¸ì½”ë”© ìë™ ê°ì§€

            soup = BeautifulSoup(response.content, 'html.parser')

            # ì œëª© ì¶”ì¶œ ì‹œë„
            title = ''
            title_tag = (
                soup.find('h1')
                or soup.find('title')
                or soup.find(class_='title')
                or soup.find(class_='article-title')
                or soup.find(property='og:title')
            )
            if title_tag:
                if title_tag.get('content'):  # og:titleì˜ ê²½ìš°
                    title = title_tag.get('content')
                else:
                    title = title_tag.get_text(strip=True)

            # 1ë‹¨ê³„: trafilatura ì‚¬ìš© (ê³ í’ˆì§ˆ í…ìŠ¤íŠ¸ ì¶”ì¶œ)
            try:
                import trafilatura
                text = trafilatura.extract(response.text)
                if text and len(text) > 100:
                    return {'title': title, 'content': text}
            except ImportError:
                pass  # trafilatura ì—†ìœ¼ë©´ BeautifulSoup ì‚¬ìš©
            except Exception as e:
                print(f"âš ï¸ trafilatura ì‹¤íŒ¨, BeautifulSoup ì‚¬ìš©: {e}")

            # 2ë‹¨ê³„: BeautifulSoup í´ë°±
            # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
            for tag in soup(
                ['script', 'style', 'nav', 'header', 'footer', 'aside', 'form', 'iframe']
            ):
                tag.decompose()

            # ê¸°ì‚¬ ë³¸ë¬¸ ìœ ë ¥ íƒœê·¸ íƒìƒ‰
            article = (
                soup.find('article')
                or soup.find('main')
                or soup.find(id='content')
                or soup.find(class_='content')
                or soup.find(class_='article-body')
                or soup.body
            )

            if article:
                text = article.get_text(separator='\n', strip=True)
                # ê³µë°± ì •ë¦¬
                lines = (line.strip() for line in text.splitlines())
                chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
                text = '\n'.join(chunk for chunk in chunks if chunk)

                # ìµœì†Œ ê¸¸ì´ ì²´í¬
                if len(text) > 100:
                    return {'title': title, 'content': text}

            return {'title': title, 'content': ''}

        except requests.RequestException as e:
            print(f"âš ï¸ ê¸°ì‚¬ ìš”ì²­ ì‹¤íŒ¨: {e}")
            return {'title': '', 'content': ''}
        except Exception as e:
            print(f"âš ï¸ ê¸°ì‚¬ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return {'title': '', 'content': ''}

    def extract(self, url: str) -> str:
        try:
            # ë´‡ íƒì§€ ìš°íšŒë¥¼ ìœ„í•œ í˜„ëŒ€ì ì¸ ë¸Œë¼ìš°ì € í—¤ë”
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.5',
                'Accept-Encoding': 'gzip, deflate, br',
                'Referer': 'https://www.google.com/',
                'DNT': '1',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1'
            }

            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            response.encoding = response.apparent_encoding  # ì¸ì½”ë”© ìë™ ê°ì§€

            # 1ë‹¨ê³„: trafilatura ì‚¬ìš© (ê³ í’ˆì§ˆ í…ìŠ¤íŠ¸ ì¶”ì¶œ)
            try:
                import trafilatura
                text = trafilatura.extract(response.text)
                if text and len(text) > 100:
                    return text
            except ImportError:
                pass  # trafilatura ì—†ìœ¼ë©´ BeautifulSoup ì‚¬ìš©
            except Exception as e:
                print(f"âš ï¸ trafilatura ì‹¤íŒ¨, BeautifulSoup ì‚¬ìš©: {e}")

            # 2ë‹¨ê³„: BeautifulSoup í´ë°±
            soup = BeautifulSoup(response.content, 'html.parser')

            # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
            for tag in soup(
                ['script', 'style', 'nav', 'header', 'footer', 'aside', 'form', 'iframe']
            ):
                tag.decompose()

            # ê¸°ì‚¬ ë³¸ë¬¸ ìœ ë ¥ íƒœê·¸ íƒìƒ‰
            article = (
                soup.find('article')
                or soup.find('main')
                or soup.find(id='content')
                or soup.find(class_='content')
                or soup.find(class_='article-body')
                or soup.body
            )

            if article:
                text = article.get_text(separator='\n', strip=True)
                # ê³µë°± ì •ë¦¬
                lines = (line.strip() for line in text.splitlines())
                chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
                text = '\n'.join(chunk for chunk in chunks if chunk)

                # ìµœì†Œ ê¸¸ì´ ì²´í¬
                if len(text) > 100:
                    return text

            return ""

        except requests.RequestException as e:
            print(f"âš ï¸ ê¸°ì‚¬ ìš”ì²­ ì‹¤íŒ¨: {e}")
            return ""  # ì˜ˆì™¸ ë°œìƒ ëŒ€ì‹  ë¹ˆ ë¬¸ìì—´ ë°˜í™˜ (ë³‘ë ¬ ì²˜ë¦¬ ì‹œ ì•ˆì •ì )
        except Exception as e:
            print(f"âš ï¸ ê¸°ì‚¬ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return ""
