"""
ë¶„ì„ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ì„ ì²˜ë¦¬í•˜ëŠ” ì„œë¹„ìŠ¤ (Global Insight Explorer v2)
- í•œêµ­ì–´ ì‚¬ìš©ì ìµœì í™” (ì…ë ¥/ì¶œë ¥: í•œêµ­ì–´, ë‚´ë¶€ê²€ìƒ‰: ì˜ì–´)
- Google Search Grounding ì˜¤ë¥˜ ìˆ˜ì • ë° ìµœì í™”
"""
import os
import json
import hashlib
from datetime import datetime

import vertexai
from vertexai.generative_models import GenerativeModel, Tool, grounding
from google.cloud import firestore
from google.api_core.exceptions import GoogleAPICallError

from app.models.extractor import BaseExtractor, YoutubeExtractor, ArticleExtractor
from app.models.media import get_media_credibility
from app.config import config
from app.utils.gdelt_search import GDELTSearcher
from app.prompts.analysis_prompts import QUERY_OPTIMIZATION_PROMPT
from concurrent.futures import ThreadPoolExecutor, as_completed

# --- ì´ˆê¸°í™” ---
gemini = None
try:
    vertexai.init(project=config.GCP_PROJECT, location=config.GCP_REGION)
    # 1ì°¨/2ì°¨ ë¶„ì„ìš© ì¼ë°˜ ëª¨ë¸ (ë¹„ìš© íš¨ìœ¨ì ì¸ Flash ëª¨ë¸ ê¶Œì¥)
    gemini = GenerativeModel('gemini-2.0-flash') 
    print("âœ… (Service) Gemini API ì—°ê²° ì„±ê³µ")
except Exception as e:
    print(f"âš ï¸ (Service) Gemini API ì—°ê²° ì‹¤íŒ¨: {e}")

db = None
try:
    db = firestore.Client(project=config.GCP_PROJECT)
    print("âœ… (Service) Firestore ì—°ê²° ì„±ê³µ")
except Exception as e:
    print(f"âš ï¸ (Service) Firestore ì—°ê²° ì‹¤íŒ¨: {e}")


class AnalysisService:
    def __init__(self):
        self.extractors = {
            'youtube': YoutubeExtractor(),
            'article': ArticleExtractor(),
        }
        self.gdelt = GDELTSearcher()  # GDELT ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™”

    def _get_extractor(self, input_type: str) -> BaseExtractor:
        extractor = self.extractors.get(input_type)
        if not extractor:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì…ë ¥ íƒ€ì…: {input_type}")
        return extractor

    # ==================================================================
    # 1ï¸âƒ£ 1ì°¨ ë¶„ì„ (Initial Analysis) - í•œêµ­ì–´ ì‚¬ìš©ì ìµœì í™”
    # ==================================================================
    def analyze_content(self, url: str, input_type: str):
        # ìºì‹œ í™•ì¸
        cached = self._get_cache(url)
        if cached:
            return cached, True

        # ì½˜í…ì¸  ì¶”ì¶œ
        print(f"ğŸ“¥ ì½˜í…ì¸  ì¶”ì¶œ ì¤‘: {url[:50]}...")
        extractor = self._get_extractor(input_type)
        content = extractor.extract(url)
        
        if not content or len(content) < 50:
            raise Exception("ì½˜í…ì¸ ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ê±°ë‚˜ ë‚´ìš©ì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤.")
            
        print(f"âœ… ì¶”ì¶œ ì™„ë£Œ: {len(content)} ê¸€ì")

        # AI ë¶„ì„
        print("ğŸ¤– Geminië¡œ 1ì°¨ ë¶„ì„ ì¤‘ (í•œê¸€ ìš”ì•½ + ì˜ì–´ ê²€ìƒ‰ì–´ ìƒì„±)...")
        result = self._analyze_with_gemini_bridge(content)
        print("âœ… 1ì°¨ ë¶„ì„ ì™„ë£Œ")

        # ìºì‹œ ì €ì¥
        self._set_cache(url, result)
        return result, False

    def _analyze_with_gemini_bridge(self, content: str):
        """
        Gemini í”„ë¡¬í”„íŠ¸: í•œêµ­ì–´ ì…ë ¥ì„ ë°›ì•„ 'í•œêµ­ì–´ ìš”ì•½'ê³¼ 'ì˜ì–´ ê²€ìƒ‰ì–´'ë¥¼ ë™ì‹œ ìƒì„±
        """
        if not gemini:
            raise Exception("Gemini APIë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        content = content[:config.MAX_CONTENT_LENGTH_FIRST_ANALYSIS]

        prompt = f"""
        ë‹¹ì‹ ì€ êµ­ì œ ì •ì„¸ ë° ë¯¸ë””ì–´ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
        ì£¼ì–´ì§„ í…ìŠ¤íŠ¸(ì˜ìƒ ìë§‰ ë˜ëŠ” ê¸°ì‚¬)ë¥¼ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ì •ë³´ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œí•˜ì„¸ìš”.

        [ë¶„ì„ ëŒ€ìƒ í…ìŠ¤íŠ¸]
        {content}

        [í•„ìˆ˜ ìš”êµ¬ì‚¬í•­]
        1. **ì‚¬ìš©ìëŠ” í•œêµ­ì¸ì…ë‹ˆë‹¤.** ëª¨ë“  ì„¤ëª…ê³¼ ìš”ì•½ì€ ìì—°ìŠ¤ëŸ¬ìš´ **í•œêµ­ì–´**ë¡œ ì‘ì„±í•˜ì„¸ìš”.
        2. **ê²€ìƒ‰ í‚¤ì›Œë“œ(search_keywords_en)**ëŠ” ë°˜ë“œì‹œ **ì˜ì–´(English)**ë¡œ ì‘ì„±í•˜ì„¸ìš”. 
           (ì´ìœ : ì „ ì„¸ê³„ ë‰´ìŠ¤(GDELT, Google) ê²€ìƒ‰ ì •í™•ë„ë¥¼ ë†’ì´ê¸° ìœ„í•¨)
        3. **ê´€ë ¨ êµ­ê°€(target_country_codes)**ëŠ” í•´ë‹¹ ì´ìŠˆì™€ ì´í•´ê´€ê³„ê°€ ìˆëŠ” êµ­ê°€ë“¤ì˜ **2ìë¦¬ ISO ì½”ë“œ**ë¡œ ì‘ì„±í•˜ì„¸ìš”.
           (ì˜ˆ: í•œêµ­='KR', ë¯¸êµ­='US', ì¤‘êµ­='CN', ë¶í•œ='KP', ëŸ¬ì‹œì•„='RU', ì¼ë³¸='JP')

        [ì¶œë ¥ í˜•ì‹ (JSON Only)]
        {{
          "title_kr": "ì½˜í…ì¸  ì œëª© ë˜ëŠ” ì£¼ì œ (í•œêµ­ì–´)",
          "summary_kr": "ì „ì²´ ë‚´ìš© ìš”ì•½ (í•œêµ­ì–´ 3ë¬¸ì¥ ë‚´ì™¸)",
          "topics": ["ì£¼ì œ1", "ì£¼ì œ2"], 
          "key_claims": [
            {{
              "claim_kr": "í•µì‹¬ ì£¼ì¥ 1 (í•œêµ­ì–´)",
              "search_keywords_en": ["keyword1", "keyword2", "specific term"],
              "target_country_codes": ["CN", "US"] // ì´ ì£¼ì¥ì— ëŒ€í•´ ì…ì¥ì„ í™•ì¸í•´ë´ì•¼ í•  êµ­ê°€ë“¤
            }},
            {{
              "claim_kr": "í•µì‹¬ ì£¼ì¥ 2 (í•œêµ­ì–´)",
              "search_keywords_en": ["keyword3", "keyword4"],
              "target_country_codes": ["RU", "UA"]
            }}
          ]
        }}

        JSON ì™¸ì— ë‹¤ë¥¸ ë§ì€ í•˜ì§€ ë§ˆì„¸ìš”.
        """

        try:
            response = gemini.generate_content(prompt)
            result_text = response.text.strip().replace('```json', '').replace('```', '').strip()
            return json.loads(result_text)
        except Exception as e:
            print(f"âŒ AI 1ì°¨ ë¶„ì„ ì‹¤íŒ¨: {e}")
            raise Exception(f"AI ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

    def optimize_search_query(self, user_input: str, context: dict):
        """
        [Step 1] ì‚¬ìš©ì ì…ë ¥ì„ GDELT ê²€ìƒ‰ ì „ëµìœ¼ë¡œ ë³€í™˜ (Gemini ì‚¬ìš©)

        Args:
            user_input: ì‚¬ìš©ìì˜ ìì—°ì–´ ì§ˆë¬¸
            context: ë¶„ì„ ì»¨í…ìŠ¤íŠ¸ {'title_kr', 'key_claims'}

        Returns:
            {
                "success": True/False,
                "data": {
                    "interpreted_intent": "...",
                    "search_keywords_en": [...],
                    "search_keywords_kr": [...],
                    "target_country_codes": [...],
                    "confidence": 0.95
                },
                "error": "..." (ì‹¤íŒ¨ ì‹œ)
            }
        """
        try:
            if not gemini:
                raise Exception("Gemini APIë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

            # ë¬¸ë§¥ ì •ë³´ ì¶”ì¶œ (ì—†ìœ¼ë©´ ê¸°ë³¸ê°’)
            context_title = context.get('title_kr', '')
            context_claims = context.get('key_claims', [])

            # í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = QUERY_OPTIMIZATION_PROMPT.format(
                user_input=user_input,
                context_title=context_title,
                context_claims=str(context_claims)[:1000]  # ê¸¸ì´ ì œí•œ
            )

            print(f"ğŸ¤– ê²€ìƒ‰ ì¿¼ë¦¬ ìµœì í™” ì¤‘: '{user_input[:50]}...'")

            # Gemini í˜¸ì¶œ
            response = gemini.generate_content(prompt)
            result_text = response.text.strip().replace('```json', '').replace('```', '').strip()

            # JSON íŒŒì‹±
            optimized_data = json.loads(result_text)

            print(f"âœ… ì¿¼ë¦¬ ìµœì í™” ì™„ë£Œ (confidence: {optimized_data.get('confidence', 0)})")

            return {
                "success": True,
                "data": optimized_data
            }

        except Exception as e:
            print(f"âš ï¸ ì¿¼ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")

            # Fallback: ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ ê·¸ëŒ€ë¡œ í‚¤ì›Œë“œë¡œ ì‚¬ìš©
            return {
                "success": False,
                "error": str(e),
                "data": {
                    "interpreted_intent": "Fallback raw search",
                    "search_keywords_en": [user_input],
                    "search_keywords_kr": [user_input],
                    "target_country_codes": [],
                    "confidence": 0.1
                }
            }

    # ==================================================================
    # 2ï¸âƒ£ 2ì°¨ ë¶„ì„ (Find Sources) - AI ì¶”ë¡  ì—†ì´ ê²€ìƒ‰ë§Œ ìˆ˜í–‰
    # ==================================================================
    def find_sources_for_claims(
        self, url: str, input_type: str, claims_data: list
    ):
        """
        [Step 2] í™•ì •ëœ ê²€ìƒ‰ ì „ëµ(claims_data)ìœ¼ë¡œ ì‹¤ì œ GDELT ê²€ìƒ‰ ìˆ˜í–‰
        * ì´ì œ ì´ í•¨ìˆ˜ëŠ” AI ì¶”ë¡ ì„ í•˜ì§€ ì•Šê³ , ì „ë‹¬ë°›ì€ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰ ìˆ˜í–‰ì—ë§Œ ì§‘ì¤‘í•©ë‹ˆë‹¤.

        Args:
            url: ì›ë³¸ ì½˜í…ì¸  URL (í˜„ì¬ëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
            input_type: ì½˜í…ì¸  íƒ€ì… (í˜„ì¬ëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
            claims_data: ì£¼ì¥ ì •ë³´ ë¦¬ìŠ¤íŠ¸
                [
                    {
                        "claim_kr": "í•œêµ­ì–´ ì£¼ì¥",
                        "search_keywords_en": ["keyword1", "keyword2"],
                        "target_country_codes": ["US", "CN"]
                    },
                    ...
                ]

        Returns:
            (result, articles) tuple
            - result: ê° ì£¼ì¥ë³„ ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
            - articles: ëª¨ë“  ê¸°ì‚¬ë¥¼ í‰íƒ„í™”í•œ ë¦¬ìŠ¤íŠ¸
        """
        all_results = []
        all_articles = []

        # ê° ì£¼ì¥ë³„ë¡œ ë…ë¦½ì ì¸ ê²€ìƒ‰ ìˆ˜í–‰
        for claim_data in claims_data:
            claim_kr = claim_data.get('claim_kr', '')
            search_keywords = claim_data.get('search_keywords_en', [])
            target_countries = claim_data.get('target_country_codes', [])

            # í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ ìŠ¤í‚µ (AI ìƒì„±í•˜ì§€ ì•ŠìŒ)
            if not search_keywords:
                print(f"âš ï¸ í‚¤ì›Œë“œ ì—†ìŒ - ìŠ¤í‚µ: '{claim_kr[:30]}...'")
                continue

            # GDELT ê²€ìƒ‰ ì‹¤í–‰ (ì˜ì–´ í‚¤ì›Œë“œ + íƒ€ê²Ÿ êµ­ê°€)
            print(f"ğŸ” '{claim_kr[:15]}...' ê²€ìƒ‰ ì‹œì‘ (í‚¤ì›Œë“œ: {search_keywords}, êµ­ê°€: {target_countries})")
            articles = self._search_real_articles(search_keywords, target_countries)

            # ê²°ê³¼ êµ¬ì¡°í™”
            result_entry = {
                "claim": claim_kr,
                "searched_keywords": search_keywords,
                "articles": articles
            }
            all_results.append(result_entry)
            all_articles.extend(articles)

        # ì¤‘ë³µ ì œê±° (URL ê¸°ì¤€)
        unique_articles = {v['url']: v for v in all_articles}.values()
        final_articles = list(unique_articles)

        print(f"âœ… ê²€ìƒ‰ ì™„ë£Œ: {len(final_articles)}ê°œ ê¸°ì‚¬ ë°œê²¬")

        # AI ë¶„ì„ ì—†ì´ ê²€ìƒ‰ ê²°ê³¼ë§Œ ë°˜í™˜
        return {"results": all_results}, final_articles

    def _generate_keywords_on_the_fly(self, claim_kr: str):
        """ì‚¬ìš©ì ì…ë ¥ ì£¼ì¥ì„ ìœ„í•œ ì˜ì–´ í‚¤ì›Œë“œ ë° íƒ€ê²Ÿ êµ­ê°€ ìƒì„±"""
        if not gemini:
            return {"keywords": [claim_kr], "countries": []}

        try:
            prompt = f"""
            Translate this Korean claim into 2-3 English search keywords for news verification.
            Also suggest 2 relevant country codes (ISO 3166-1 alpha-2).
            Claim: "{claim_kr}"
            Output JSON: {{"keywords": ["kw1", "kw2"], "countries": ["US", "KR"]}}
            """
            response = gemini.generate_content(prompt)
            text = response.text.strip().replace('```json', '').replace('```', '').strip()
            return json.loads(text)
        except Exception as e:
            print(f"âš ï¸ í‚¤ì›Œë“œ ìƒì„± ì‹¤íŒ¨: {e}")
            return {"keywords": [claim_kr], "countries": []}

    def _search_real_articles(self, keywords: list, target_countries: list = None):
        """
        GDELT Hybrid ê²€ìƒ‰: GDELT (ë¬´ë£Œ) â†’ Google Search (ìœ ë£Œ í´ë°±)

        Args:
            keywords: ì˜ì–´ ê²€ìƒ‰ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
            target_countries: íƒ€ê²Ÿ êµ­ê°€ ì½”ë“œ ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: ["US", "CN"])
        """
        if not keywords:
            return []

        # í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ í‰íƒ„í™”
        flat_keywords = []
        for k in keywords:
            if isinstance(k, list):
                flat_keywords.extend(k)
            else:
                flat_keywords.append(k)

        # 1ï¸âƒ£ GDELT ê²€ìƒ‰ ì‹œë„ (ë¬´ë£Œ, ë¹ ë¦„, ê¸€ë¡œë²Œ)
        print(f"ğŸ“Š [1/2] GDELT ê²€ìƒ‰ ì¤‘... (í‚¤ì›Œë“œ: {flat_keywords[:3]}, êµ­ê°€: {target_countries})")
        gdelt_results = []
        try:
            gdelt_results = self.gdelt.search(
                keywords=flat_keywords[:5],  # ìµœëŒ€ 5ê°œ í‚¤ì›Œë“œ
                target_countries=target_countries,
                days=7,  # ìµœê·¼ 7ì¼
                limit=30  # ìµœëŒ€ 30ê°œ
            )
        except Exception as e:
            print(f"âš ï¸ GDELT ê²€ìƒ‰ ì‹¤íŒ¨: {e}")

        # 2ï¸âƒ£ ë³‘ë ¬ ë³¸ë¬¸ ì¶”ì¶œ (ThreadPool 10ê°œ ì›Œì»¤)
        if gdelt_results:
            print(f"ğŸ”„ ë³‘ë ¬ ë³¸ë¬¸ ì¶”ì¶œ ì¤‘... ({len(gdelt_results)}ê°œ ê¸°ì‚¬)")
            extracted = self._extract_contents_parallel(gdelt_results)
            print(f"âœ… ì¶”ì¶œ ì™„ë£Œ: {len(extracted)}ê°œ")
            return extracted

        # GDELT ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ë¹ˆ ë°°ì—´ ë°˜í™˜
        print(f"âš ï¸ GDELT ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
        return []

    def _extract_contents_parallel(self, articles_meta: list):
        """
        ë³‘ë ¬ ì²˜ë¦¬ë¡œ ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ (ThreadPool)

        Args:
            articles_meta: GDELT ê²€ìƒ‰ ê²°ê³¼ [{url, source, title, date, tone, country}, ...]

        Returns:
            ë³¸ë¬¸ì´ ì¶”ì¶œëœ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
        """
        extracted = []
        extractor = self.extractors['article']

        def fetch_one(meta):
            """ë‹¨ì¼ ê¸°ì‚¬ ì¶”ì¶œ (ë³‘ë ¬ ì‹¤í–‰ í•¨ìˆ˜)"""
            try:
                url = meta.get('url', '')
                if not url or url == '#':
                    return None

                # ì œëª©ê³¼ ë³¸ë¬¸ ì¶”ì¶œ
                result = extractor.extract_with_title(url)
                title = result.get('title', '')
                content = result.get('content', '')

                # ë„ˆë¬´ ì§§ìœ¼ë©´ ë¬´ì‹œ
                if not content or len(content) < 100:
                    return None

                # ë©”íƒ€ë°ì´í„°ì— ì œëª©ê³¼ ë³¸ë¬¸ ì¶”ê°€
                meta['title'] = title if title else meta.get('source', 'No title')  # ì œëª©ì´ ì—†ìœ¼ë©´ ì¶œì²˜ë¥¼ ì œëª©ìœ¼ë¡œ
                meta['content'] = content
                meta['snippet'] = content[:500]  # ë¯¸ë¦¬ë³´ê¸°

                # ì‹ ë¢°ë„ ì¶”ê°€ (êµ­ê°€/ì¶œì²˜ ê¸°ë°˜)
                if 'credibility' not in meta:
                    meta['credibility'] = get_media_credibility(
                        meta.get('source', ''),
                        meta.get('country', '')
                    )

                print(f"âœ… ì¶”ì¶œ ì„±ê³µ: {meta.get('source', 'Unknown')} ({meta.get('country', 'Unknown')})")
                return meta

            except Exception as e:
                print(f"âš ï¸ ì¶”ì¶œ ì‹¤íŒ¨: {meta.get('url', 'unknown')} - {e}")
                return None

        # ThreadPool ë³‘ë ¬ ì‹¤í–‰ (max_workers=10)
        with ThreadPoolExecutor(max_workers=10) as executor:
            futures = [executor.submit(fetch_one, item) for item in articles_meta]

            for future in as_completed(futures):
                result = future.result()
                if result:
                    extracted.append(result)

        return extracted

    def _search_google_fallback(self, keywords: list, target_countries: list = None):
        """
        Google Search í´ë°± (GDELT ì‹¤íŒ¨ ì‹œ)

        Args:
            keywords: ì˜ì–´ ê²€ìƒ‰ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
            target_countries: íƒ€ê²Ÿ êµ­ê°€ ì½”ë“œ ë¦¬ìŠ¤íŠ¸

        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        if not keywords:
            return []

        # ê¸°ë³¸ ì¿¼ë¦¬
        base_query = " ".join(keywords[:7])  # ìµœëŒ€ 7ë‹¨ì–´
        query = base_query

        # íƒ€ê²Ÿ êµ­ê°€ê°€ ìˆìœ¼ë©´ ì¿¼ë¦¬ì— ì¶”ê°€
        if target_countries and len(target_countries) > 0:
            country_query = " OR ".join(target_countries)
            query = f"{base_query} ({country_query})"

        print(f"ğŸ” Google Search Query: {query}")

        try:
            # Google Search Grounding ì‹œë„
            search_tool = Tool(
                google_search=grounding.GoogleSearchRetrieval()
            )

            model = GenerativeModel(
                'gemini-2.0-flash',
                tools=[search_tool]
            )

            prompt = f"Search for latest news articles about: {query}. Provide details."
            response = model.generate_content(prompt)

            # TODO: Grounding Metadataì—ì„œ ì‹¤ì œ URL ì¶”ì¶œ
            # í˜„ì¬ëŠ” êµ¬ì¡°ì  URL ì¶”ì¶œì´ ì–´ë ¤ìš°ë¯€ë¡œ ìƒ˜í”Œ ë°˜í™˜
            print("âš ï¸ Google Search ì™„ë£Œ (URL ì¶”ì¶œ ë¡œì§ ë³´ì™„ í•„ìš”)")
            return self._get_sample_articles(keywords, target_countries)

        except Exception as e:
            print(f"âš ï¸ Google Search ì‹¤íŒ¨: {e}")
            return self._get_sample_articles(keywords, target_countries)

    def _compare_perspectives_with_gemini(
        self, original_content: str, claims: list, articles: list
    ):
        if not gemini:
            raise Exception("Gemini APIë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        original_content = original_content[:config.MAX_CONTENT_LENGTH_SECOND_ANALYSIS]
        
        # ê¸°ì‚¬ ëª©ë¡ í…ìŠ¤íŠ¸í™”
        articles_text = "\n".join([
            f"- [{a['source']} ({a['country']})] {a['title']}: {a['snippet']}"
            for a in articles
        ])

        prompt = f"""
        ë‹¹ì‹ ì€ ì¤‘ë¦½ì ì¸ 'êµ­ì œ ë‰´ìŠ¤ ë¶„ì„ê°€'ì…ë‹ˆë‹¤.
        ì‚¬ìš©ìê°€ ì„ íƒí•œ ì£¼ì¥ì— ëŒ€í•´, ì„¸ê³„ ê°êµ­ì˜ ì–¸ë¡ ì´ ì–´ë–»ê²Œ ë³´ë„í•˜ê³  ìˆëŠ”ì§€ ê°ê´€ì ìœ¼ë¡œ ë¹„êµ ë¶„ì„í•´ì£¼ì„¸ìš”.
        **ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.**

        **ì ˆëŒ€ë¡œ íŠ¹ì • ì£¼ì¥ì´ ì‚¬ì‹¤ì¸ì§€ ê±°ì§“ì¸ì§€ ë‹¨ì • ì§“ì§€ ë§ˆì„¸ìš”.**
        ì˜¤ì§ 'A ì–¸ë¡ ì‚¬ëŠ” ì´ë ‡ê²Œ ë³´ë„í–ˆê³ , B ì–¸ë¡ ì‚¬ëŠ” ì €ë ‡ê²Œ ë³´ë„í–ˆë‹¤'ëŠ” ì°¨ì´ì ê³¼ ë§¥ë½ì„ ë³´ì—¬ì£¼ëŠ” ë° ì§‘ì¤‘í•˜ì„¸ìš”.

        [ë¶„ì„ ëŒ€ìƒ ì£¼ì¥]
        {chr(10).join([f'- {c}' for c in claims])}

        [ìˆ˜ì§‘ëœ ê¸°ì‚¬ ë°ì´í„°]
        {articles_text}

        [ì§€ì‹œì‚¬í•­]
        1. ê° ì£¼ì¥ì— ëŒ€í•´ ìˆ˜ì§‘ëœ ê¸°ì‚¬ë“¤ì´ **ì§€ì§€(Supporting)**í•˜ëŠ”ì§€, **ë°˜ë°•(Opposing)**í•˜ëŠ”ì§€, ë˜ëŠ” **ì¤‘ë¦½/ê´€ë ¨ì—†ìŒ**ì¸ì§€ ë¶„ì„í•˜ì„¸ìš”.
        2. êµ­ê°€ë³„ ì–¸ë¡ ì˜ ì‹œê° ì°¨ì´ê°€ ìˆë‹¤ë©´ ì§€ì í•´ì£¼ì„¸ìš” (ì˜ˆ: ë¯¸êµ­ ì–¸ë¡ ì€ ê²½ì œì  ì¸¡ë©´ì„, ì¤‘êµ­ ì–¸ë¡ ì€ ì •ì¹˜ì  ì¸¡ë©´ì„ ê°•ì¡°).
        3. **íŒë‹¨ì€ ì‚¬ìš©ìì—ê²Œ ë§¡ê¸°ê³ **, ë‹¤ì–‘í•œ ê´€ì ì´ ìˆë‹¤ëŠ” ê²ƒë§Œ ë³´ì—¬ì£¼ì„¸ìš”.

        [ì‘ë‹µ í˜•ì‹ (JSON)]
        {{
          "results": [
            {{
              "claim": "ì£¼ì¥ ë‚´ìš©",
              "perspectives": [
                 {{
                   "country": "US",
                   "media": "CNN",
                   "stance": "Supporting",
                   "viewpoint": "ì´ ê¸°ì‚¬ëŠ” ~~í•œ ê·¼ê±°ë¥¼ ë“¤ì–´ í•´ë‹¹ ì£¼ì¥ì„ ì§€ì§€í•˜ëŠ” ë…¼ì¡°ì…ë‹ˆë‹¤."
                 }},
                 {{
                   "country": "CN",
                   "media": "Global Times",
                   "stance": "Opposing",
                   "viewpoint": "ë°˜ë©´ ì´ ê¸°ì‚¬ëŠ” ~~ë¼ë©° ë‹¤ë¥¸ ê´€ì ì„ ì œì‹œí•©ë‹ˆë‹¤."
                 }}
              ],
              "summary_kr": "ì¢…í•©í•´ë³´ë©´ ë¯¸êµ­ ì–¸ë¡ ì€ ê²½ì œì  ì¸¡ë©´ì„, ì¤‘êµ­ ì–¸ë¡ ì€ ì •ì¹˜ì  ì¸¡ë©´ì„ ê°•ì¡°í•˜ê³  ìˆìŠµë‹ˆë‹¤. (íŒë‹¨ì€ ì‚¬ìš©ì ëª«)"
            }}
          ]
        }}
        """
        
        try:
            response = gemini.generate_content(prompt)
            result_text = response.text.strip().replace('```json', '').replace('```', '').strip()
            return json.loads(result_text)
        except Exception as e:
            print(f"âŒ AI 2ì°¨ ë¶„ì„ ì‹¤íŒ¨: {e}")
            # ë¹ˆ ê²°ê³¼ ë°˜í™˜í•˜ì—¬ í”„ë¡ íŠ¸ì—”ë“œ ì—ëŸ¬ ë°©ì§€
            return {"results": []}

    # --- ìºì‹œ ë° ìœ í‹¸ë¦¬í‹° ---
    def _get_cache(self, url: str):
        if not db: return None
        try:
            cache_key = hashlib.md5(url.encode()).hexdigest()
            doc = db.collection('cache').document(cache_key).get()
            if doc.exists:
                print(f"âœ… ìºì‹œ íˆíŠ¸: {url[:30]}...")
                return doc.to_dict().get('result')
        except: pass
        return None

    def _set_cache(self, url: str, result):
        if not db: return
        try:
            cache_key = hashlib.md5(url.encode()).hexdigest()
            db.collection('cache').document(cache_key).set({
                'url': url, 'result': result, 'cached_at': datetime.now()
            })
        except: pass

    def _get_sample_articles(self, keywords: list, target_countries: list = None):
        """ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ë°ì´í„°"""
        k = keywords[0] if keywords else "ì´ìŠˆ"

        # íƒ€ê²Ÿ êµ­ê°€ì— ë§ëŠ” ìƒ˜í”Œ ë°ì´í„° ìƒì„±
        sample_sources = []
        if target_countries and len(target_countries) > 0:
            # íƒ€ê²Ÿ êµ­ê°€ë³„ ëŒ€í‘œ ì–¸ë¡ ì‚¬ ë§¤í•‘
            country_media = {
                'US': {'source': 'CNN', 'credibility': 80},
                'UK': {'source': 'BBC', 'credibility': 85},
                'CN': {'source': 'Xinhua', 'credibility': 60},
                'RU': {'source': 'RT', 'credibility': 55},
                'JP': {'source': 'NHK', 'credibility': 75},
                'KR': {'source': 'Yonhap', 'credibility': 75},
                'FR': {'source': 'France 24', 'credibility': 80},
                'DE': {'source': 'DW', 'credibility': 80},
            }
            for country in target_countries[:3]:  # ìµœëŒ€ 3ê°œêµ­
                media = country_media.get(country, {'source': f'{country} News', 'credibility': 70})
                sample_sources.append({
                    'title': f'{media["source"]}: {k} coverage',
                    'snippet': f'{country} perspective on {k}...',
                    'url': '#',
                    'source': media['source'],
                    'country': country,
                    'credibility': media['credibility']
                })
        else:
            # ê¸°ë³¸ ìƒ˜í”Œ (íƒ€ê²Ÿ êµ­ê°€ ì—†ì„ ë•Œ)
            sample_sources = [
                {'title': f'Global view on {k}', 'snippet': 'Western media perspective...', 'url': '#', 'source': 'CNN', 'country': 'US', 'credibility': 80},
                {'title': f'Alternative view on {k}', 'snippet': 'Eastern media perspective...', 'url': '#', 'source': 'Xinhua', 'country': 'CN', 'credibility': 60},
            ]

        return sample_sources