"""
ë¶„ì„ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ì„ ì²˜ë¦¬í•˜ëŠ” ì„œë¹„ìŠ¤ (Facade íŒ¨í„´)
"""
import os
import json
import hashlib
from datetime import datetime

import vertexai
from vertexai.preview.generative_models import GenerativeModel
from google.cloud import firestore

from app.models.extractor import BaseExtractor, YoutubeExtractor, ArticleExtractor
from app.models.media import get_media_credibility
from app.config import config

# Gemini ëª¨ë¸ ë¡œë“œ
gemini = None
try:
    vertexai.init(project=config.GCP_PROJECT, location=config.GCP_REGION)
    gemini = GenerativeModel('gemini-2.5-flash')
    print("âœ… (Service) Gemini API ì—°ê²° ì„±ê³µ")
except Exception as e:
    print(f"âš ï¸ (Service) Gemini API ì—°ê²° ì‹¤íŒ¨: {e}")

# Firestore í´ë¼ì´ì–¸íŠ¸
db = None
try:
    db = firestore.Client(project=config.GCP_PROJECT)
    print("âœ… (Service) Firestore ì—°ê²° ì„±ê³µ")
except Exception as e:
    print(f"âš ï¸ (Service) Firestore ì—°ê²° ì‹¤íŒ¨: {e}")


class AnalysisService:
    def __init__(self):
        self.extractors = {
            'youtube': YoutubeExtractor(),
            'article': ArticleExtractor(),
        }

    def _get_extractor(self, input_type: str) -> BaseExtractor:
        extractor = self.extractors.get(input_type)
        if not extractor:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì…ë ¥ íƒ€ì…: {input_type}")
        return extractor

    # --- 1ì°¨ ë¶„ì„ ---
    def analyze_content(self, url: str, input_type: str):
        # ìºì‹œ í™•ì¸
        cached = self._get_cache(url)
        if cached:
            return cached, True

        # ì½˜í…ì¸  ì¶”ì¶œ
        print(f"ğŸ“¥ ì½˜í…ì¸  ì¶”ì¶œ ì¤‘: {url[:50]}...")
        extractor = self._get_extractor(input_type)
        content = extractor.extract(url)
        print(f"âœ… ì¶”ì¶œ ì™„ë£Œ: {len(content)} ê¸€ì")

        # AI ë¶„ì„
        print("ğŸ¤– Geminië¡œ 1ì°¨ ë¶„ì„ ì¤‘...")
        result = self._analyze_with_gemini(content)
        print("âœ… 1ì°¨ ë¶„ì„ ì™„ë£Œ")

        # ìºì‹œ ì €ì¥
        self._set_cache(url, result)
        return result, False

    def _analyze_with_gemini(self, content: str):
        if not gemini:
            raise Exception("Gemini APIë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        content = content[:config.MAX_CONTENT_LENGTH_FIRST_ANALYSIS]  # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì¡°ì ˆ

        prompt = f"""
        ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.

        í…ìŠ¤íŠ¸:
        ---
        {content}
        ---

        ìš”êµ¬ì‚¬í•­:
        1. key_claims: í…ìŠ¤íŠ¸ì—ì„œ ì–¸ê¸‰ëœ í•µì‹¬ì ì¸ ì£¼ì¥ì´ë‚˜ ì‚¬ì‹¤ (3-7ê°œ, ê°ê° ëª…í™•í•˜ê³  ê°„ê²°í•œ í•œ ë¬¸ì¥)
        2. related_countries: ì£¼ëœ ê´€ë ¨ êµ­ê°€ (ìµœëŒ€ 5ê°œ, êµ­ê°€ ì½”ë“œê°€ ì•„ë‹Œ ì´ë¦„ìœ¼ë¡œ)
        3. search_keywords: ê° ì£¼ì¥ì„ ê²€ì¦í•˜ê¸° ìœ„í•œ ì˜ì–´ ê²€ìƒ‰ í‚¤ì›Œë“œ (ì£¼ì¥ë‹¹ 2-3ê°œ)
        4. topics: ì£¼ì œ ë¶„ë¥˜ (ì˜ˆ: ì •ì¹˜, ê²½ì œ, ì‚¬íšŒ, ê³¼í•™ê¸°ìˆ , êµ­ì œê´€ê³„)
        5. summary: ì „ì²´ ë‚´ìš©ì„ ìš”ì•½ (2-3 ë¬¸ì¥)

        ì‘ë‹µ í˜•ì‹ (JSON):
        {{
          "key_claims": ["ì£¼ì¥ 1", "ì£¼ì¥ 2"],
          "related_countries": ["êµ­ê°€1", "êµ­ê°€2"],
          "search_keywords": [["keyword1", "keyword2"], ["keyword3", "keyword4"]],
          "topics": ["ì£¼ì œ1", "ì£¼ì œ2"],
          "summary": "ìš”ì•½ ë‚´ìš©"
        }}

        ì£¼ì˜: ë°˜ë“œì‹œ JSON ê°ì²´ë§Œ ì¶œë ¥í•˜ê³  ë‹¤ë¥¸ ì„¤ëª…ì€ ë§ë¶™ì´ì§€ ë§ˆì„¸ìš”.
        """

        try:
            response = gemini.generate_content(prompt)
            result_text = (
                response.text.strip().replace('```json', '').replace('```', '').strip()
            )
            return json.loads(result_text)
        except Exception as e:
            print(f"âŒ AI 1ì°¨ ë¶„ì„ ì‹¤íŒ¨: {e}")
            raise Exception(f"AI ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

    # --- 2ì°¨ ë¶„ì„ ---
    def find_sources_for_claims(
        self, url: str, input_type: str, selected_claims: list, search_keywords: list
    ):
        # ì›ë³¸ ì½˜í…ì¸  ë‹¤ì‹œ ì¶”ì¶œ
        extractor = self._get_extractor(input_type)
        original_content = extractor.extract(url)

        # ì‹¤ì œ ê¸°ì‚¬ ê²€ìƒ‰
        articles = self._search_real_articles(search_keywords)

        # AIë¡œ ê´€ë ¨ì„± ë¶„ì„
        print("ğŸ¤– Geminië¡œ 2ì°¨ ë¶„ì„ (ê´€ë ¨ ê¸°ì‚¬ ë§¤ì¹­) ì¤‘...")
        analysis_result = self._find_related_articles_with_gemini(
            original_content, selected_claims, articles
        )
        print("âœ… 2ì°¨ ë¶„ì„ ì™„ë£Œ")

        return analysis_result, articles

    def _search_real_articles(self, keywords: list):
        """Gemini Google Search Groundingì„ ì‚¬ìš©í•œ ì‹¤ì œ ê¸°ì‚¬ ê²€ìƒ‰"""
        if not keywords:
            return []
        try:
            from vertexai.preview.generative_models import GenerativeModel, Tool, grounding

            query = " ".join(keywords[:5])  # ìµœëŒ€ 5ê°œ í‚¤ì›Œë“œ ê²°í•©
            print(f"ğŸ” Gemini Google Searchë¡œ ê²€ìƒ‰ ì¤‘: {query}")

            # Google Search Grounding ë„êµ¬ ì„¤ì •
            search_tool = Tool.from_google_search_retrieval(
                grounding.GoogleSearchRetrieval()
            )

            # Gemini ëª¨ë¸ì— ê²€ìƒ‰ ë„êµ¬ ì¶”ê°€
            search_model = GenerativeModel(
                'gemini-2.0-flash-exp',
                tools=[search_tool]
            )

            # ê²€ìƒ‰ ì¿¼ë¦¬ ì‹¤í–‰
            search_prompt = f"""
            ë‹¤ìŒ í‚¤ì›Œë“œì™€ ê´€ë ¨ëœ ìµœì‹  ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ê²€ìƒ‰í•˜ê³ , ê° ê¸°ì‚¬ì— ëŒ€í•´ ë‹¤ìŒ ì •ë³´ë¥¼ JSON ë°°ì—´ë¡œ ë°˜í™˜í•´ì£¼ì„¸ìš”:
            - title: ê¸°ì‚¬ ì œëª©
            - snippet: ê¸°ì‚¬ ìš”ì•½ (2-3ë¬¸ì¥)
            - url: ê¸°ì‚¬ URL
            - source: ì–¸ë¡ ì‚¬ëª…
            - published_date: ë°œí–‰ì¼ (YYYY-MM-DD í˜•ì‹)

            í‚¤ì›Œë“œ: {query}

            ì‘ë‹µ í˜•ì‹:
            {{"articles": [...]}}
            """

            response = search_model.generate_content(search_prompt)

            # JSON íŒŒì‹±
            import json
            result_text = response.text.strip().replace('```json', '').replace('```', '').strip()
            search_data = json.loads(result_text)

            articles = []
            for result in search_data.get('articles', [])[:15]:  # ìµœëŒ€ 15ê°œ
                source = result.get('source', 'ì¶œì²˜ ë¶ˆëª…')
                credibility_info = get_media_credibility(source)

                articles.append({
                    'title': result.get('title', 'ì œëª© ì—†ìŒ'),
                    'snippet': result.get('snippet', 'ë‚´ìš© ì—†ìŒ'),
                    'url': result.get('url', '#'),
                    'source': source,
                    'country': credibility_info.get('country', 'Unknown'),
                    'credibility': credibility_info.get('credibility', 50),
                    'bias': credibility_info.get('bias', 'ì•Œ ìˆ˜ ì—†ìŒ'),
                    'published_date': result.get('published_date', 'ë‚ ì§œ ì—†ìŒ'),
                })

            print(f"âœ… {len(articles)}ê°œ ê¸°ì‚¬ ê²€ìƒ‰ ì™„ë£Œ")
            return articles

        except Exception as e:
            print(f"âš ï¸ ê¸°ì‚¬ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            print(f"âš ï¸ Fallback: ìƒ˜í”Œ ê¸°ì‚¬ ë°˜í™˜")
            # Fallback: ìƒ˜í”Œ ë°ì´í„° ë°˜í™˜
            return self._get_sample_articles(keywords)

    def _find_related_articles_with_gemini(
        self, original_content: str, claims: list, articles: list
    ):
        """ì…ì¥ ê¸°ë°˜ ë¶„ì„ - êµ­ë‚´/êµ­ì œ ì´ìŠˆ ëª¨ë‘ ì ìš© ê°€ëŠ¥"""
        if not gemini:
            raise Exception("Gemini APIë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        original_content = original_content[:config.MAX_CONTENT_LENGTH_SECOND_ANALYSIS]
        articles_text = "\n\n".join(
            [
                f"[ê¸°ì‚¬ {i+1}]\nì œëª©: {article.get('title', '')}\nì¶œì²˜: {article.get('source', '')}\në‚´ìš©: {article.get('snippet', '')}"
                for i, article in enumerate(articles[:15])  # ì»¨í…ìŠ¤íŠ¸ ì¡°ì ˆ
            ]
        )

        prompt = f"""
        ë‹¹ì‹ ì€ í¸ê²¬ ì—†ëŠ” ì •ë³´ ë¶„ì„ê°€ì…ë‹ˆë‹¤.
        **ê° ê¸°ì‚¬ê°€ ì„ íƒí•œ ì£¼ì¥ì— ëŒ€í•´ ì–´ë–¤ ì…ì¥ì¸ì§€ ë¶„ì„í•´ì£¼ì„¸ìš”.**

        ì¤‘ìš”: ì–¸ë¡ ì‚¬ì˜ "ê³ ì •ëœ ì„±í–¥"ì´ ì•„ë‹ˆë¼, "ì´ ê¸°ì‚¬ì˜ ë‚´ìš©"ë§Œ ê¸°ì¤€ìœ¼ë¡œ íŒë‹¨í•˜ì„¸ìš”.
        - ê°™ì€ ì–¸ë¡ ì‚¬ë„ ì´ìŠˆë§ˆë‹¤ ë‹¤ë¥¸ ì…ì¥ì„ ê°€ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        - ì ˆëŒ€ë¡œ ì‚¬ì „ì— ì •í•´ì§„ ë¼ë²¨(ë³´ìˆ˜/ì§„ë³´)ì„ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.

        [ì›ë³¸ ì½˜í…ì¸  ìš”ì•½]
        {original_content}

        [ì‚¬ìš©ìê°€ ì„ íƒí•œ ê²€ì¦ ëŒ€ìƒ ì£¼ì¥]
        {chr(10).join([f'- {c}' for c in claims])}

        [ìˆ˜ì§‘ëœ ìµœì‹  ê¸°ì‚¬ ëª©ë¡]
        {articles_text}

        [ìš”ì²­ ì‘ì—…]
        ê° ê¸°ì‚¬ì— ëŒ€í•´ ë‹¤ìŒì„ ë¶„ì„í•˜ì„¸ìš”:
        1. ì´ ê¸°ì‚¬ëŠ” ì£¼ì¥ì— ëŒ€í•´ ì–´ë–¤ ì…ì¥ì¸ê°€?
           - supporting: ì£¼ì¥ì„ ì§€ì§€í•˜ê±°ë‚˜ ë™ì˜í•˜ëŠ” ë‚´ìš©
           - opposing: ì£¼ì¥ì— ë°˜ëŒ€í•˜ê±°ë‚˜ ë¶€ì •í•˜ëŠ” ë‚´ìš©
           - neutral: ì…ì¥ ì—†ì´ ì‚¬ì‹¤ë§Œ ë³´ë„í•˜ê±°ë‚˜ ì–‘ìª½ ì…ì¥ ë³‘ê¸°

        2. í™•ì‹ ë„ (0.0 ~ 1.0): ì…ì¥ì´ ì–¼ë§ˆë‚˜ ëª…í™•í•œê°€?

        3. í•µì‹¬ ê·¼ê±°: ì´ ê¸°ì‚¬ê°€ ì œì‹œí•˜ëŠ” êµ¬ì²´ì ì¸ ì¦ê±°ë‚˜ ì¸ìš©ë¬¸ (1-2ê°œ)

        4. í”„ë ˆì´ë°: ì´ ê¸°ì‚¬ê°€ ì‚¬ìš©í•˜ëŠ” ì„œìˆ  ë°©ì‹ì´ë‚˜ ê´€ì 

        [ì‘ë‹µ í˜•ì‹ (JSON)]
        {{
          "results": [
            {{
              "claim": "ì‚¬ìš©ìê°€ ì„ íƒí•œ ì²« ë²ˆì§¸ ì£¼ì¥",
              "article_analyses": [
                {{
                  "article_index": 1,
                  "stance": "supporting",
                  "confidence": 0.85,
                  "key_evidence": [
                    "ì²« ë²ˆì§¸ í•µì‹¬ ì¦ê±°ë‚˜ ì¸ìš©ë¬¸",
                    "ë‘ ë²ˆì§¸ í•µì‹¬ ì¦ê±°ë‚˜ ì¸ìš©ë¬¸"
                  ],
                  "framing": "ì´ ê¸°ì‚¬ê°€ ì‚¬ìš©í•˜ëŠ” í”„ë ˆì„ ì„¤ëª…"
                }},
                {{
                  "article_index": 2,
                  "stance": "opposing",
                  "confidence": 0.80,
                  "key_evidence": [
                    "ì²« ë²ˆì§¸ í•µì‹¬ ì¦ê±°ë‚˜ ì¸ìš©ë¬¸",
                    "ë‘ ë²ˆì§¸ í•µì‹¬ ì¦ê±°ë‚˜ ì¸ìš©ë¬¸"
                  ],
                  "framing": "ì´ ê¸°ì‚¬ê°€ ì‚¬ìš©í•˜ëŠ” í”„ë ˆì„ ì„¤ëª…"
                }},
                {{
                  "article_index": 3,
                  "stance": "neutral",
                  "confidence": 0.70,
                  "key_evidence": [
                    "ì²« ë²ˆì§¸ í•µì‹¬ ì¦ê±°ë‚˜ ì¸ìš©ë¬¸",
                    "ë‘ ë²ˆì§¸ í•µì‹¬ ì¦ê±°ë‚˜ ì¸ìš©ë¬¸"
                  ],
                  "framing": "ì´ ê¸°ì‚¬ê°€ ì‚¬ìš©í•˜ëŠ” í”„ë ˆì„ ì„¤ëª…"
                }}
              ],
              "stance_summary": {{
                "supporting_count": 0,
                "opposing_count": 0,
                "neutral_count": 0,
                "common_supporting_arguments": [
                  "ì§€ì§€ ì…ì¥ì˜ ê³µí†µ ë…¼ê±°",
                  "ë˜ ë‹¤ë¥¸ ê³µí†µ ë…¼ê±°"
                ],
                "common_opposing_arguments": [
                  "ë°˜ëŒ€ ì…ì¥ì˜ ê³µí†µ ë…¼ê±°",
                  "ë˜ ë‹¤ë¥¸ ê³µí†µ ë…¼ê±°"
                ]
              }}
            }}
          ]
        }}

        [ì£¼ì˜ì‚¬í•­]
        - ì ˆëŒ€ë¡œ ì£¼ì¥ì˜ ì°¸/ê±°ì§“ì„ íŒë‹¨í•˜ì§€ ë§ˆì„¸ìš”.
        - ì˜¤ì§ "ì´ ê¸°ì‚¬ì˜ ë‚´ìš©"ì„ ê¸°ì¤€ìœ¼ë¡œ ì…ì¥ì„ ë¶„ì„í•˜ì„¸ìš”.
        - ì–¸ë¡ ì‚¬ ì´ë¦„ìœ¼ë¡œ ì…ì¥ì„ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”.
        - ë°˜ë“œì‹œ JSON ê°ì²´ë§Œ ì¶œë ¥í•˜ê³  ë‹¤ë¥¸ ì„¤ëª…ì€ ë§ë¶™ì´ì§€ ë§ˆì„¸ìš”.
        """

        try:
            response = gemini.generate_content(prompt)
            result_text = (
                response.text.strip().replace('```json', '').replace('```', '').strip()
            )
            parsed_result = json.loads(result_text)

            # ìœ íš¨ì„± ê²€ì¦
            if 'results' not in parsed_result:
                raise ValueError("AI ì‘ë‹µì— 'results' í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤.")

            if not isinstance(parsed_result['results'], list):
                raise ValueError("AI ì‘ë‹µì˜ 'results'ê°€ ë°°ì—´ì´ ì•„ë‹™ë‹ˆë‹¤.")

            # ê²°ê³¼ë¥¼ ì‚¬ìš©ì ì¹œí™”ì ìœ¼ë¡œ ì¬êµ¬ì¡°í™”
            return self._restructure_by_stance(parsed_result, articles)

        except json.JSONDecodeError as e:
            print(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
            print(f"AI ì‘ë‹µ ì›ë³¸ (ì²˜ìŒ 500ì): {result_text[:500]}")
            raise Exception(f"AI ì‘ë‹µì„ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
        except Exception as e:
            print(f"âŒ AI 2ì°¨ ë¶„ì„ ì‹¤íŒ¨: {e}")
            raise Exception(f"ì…ì¥ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

    def _restructure_by_stance(self, analysis_result, articles):
        """
        AI ë¶„ì„ ê²°ê³¼ë¥¼ ì…ì¥ë³„ë¡œ ê·¸ë£¹í™”
        êµ­ë‚´/êµ­ì œ êµ¬ë¶„ ì—†ì´ ë™ì¼í•˜ê²Œ ì‘ë™
        """
        restructured = []

        for claim_result in analysis_result.get('results', []):
            supporting_articles = []
            opposing_articles = []
            neutral_articles = []

            # ê° ê¸°ì‚¬ë¥¼ ì…ì¥ë³„ë¡œ ë¶„ë¥˜
            for analysis in claim_result.get('article_analyses', []):
                article_idx = analysis.get('article_index') - 1
                if article_idx < 0 or article_idx >= len(articles):
                    continue

                article = articles[article_idx].copy()
                article['analysis'] = {
                    'stance': analysis.get('stance'),
                    'confidence': analysis.get('confidence'),
                    'key_evidence': analysis.get('key_evidence', []),
                    'framing': analysis.get('framing', '')
                }

                # ì…ì¥ë³„ë¡œ ë¶„ë¥˜
                stance = analysis.get('stance')
                if stance == 'supporting':
                    supporting_articles.append(article)
                elif stance == 'opposing':
                    opposing_articles.append(article)
                elif stance == 'neutral':
                    neutral_articles.append(article)

            # í™•ì‹ ë„ ìˆœìœ¼ë¡œ ì •ë ¬
            supporting_articles.sort(key=lambda x: x['analysis']['confidence'], reverse=True)
            opposing_articles.sort(key=lambda x: x['analysis']['confidence'], reverse=True)
            neutral_articles.sort(key=lambda x: x['analysis']['confidence'], reverse=True)

            restructured.append({
                'claim': claim_result.get('claim'),
                'supporting_evidence': {
                    'count': len(supporting_articles),
                    'articles': supporting_articles,
                    'common_arguments': claim_result.get('stance_summary', {}).get('common_supporting_arguments', [])
                },
                'opposing_evidence': {
                    'count': len(opposing_articles),
                    'articles': opposing_articles,
                    'common_arguments': claim_result.get('stance_summary', {}).get('common_opposing_arguments', [])
                },
                'neutral_coverage': {
                    'count': len(neutral_articles),
                    'articles': neutral_articles
                },
                'diversity_metrics': {
                    'total_sources': len(supporting_articles) + len(opposing_articles) + len(neutral_articles),
                    'stance_distribution': {
                        'supporting': len(supporting_articles),
                        'opposing': len(opposing_articles),
                        'neutral': len(neutral_articles)
                    }
                }
            })

        return {'results': restructured}

    # --- ìºì‹± í—¬í¼ ---
    def _get_cache(self, url: str):
        if not db:
            return None
        try:
            cache_key = hashlib.md5(url.encode()).hexdigest()
            doc = db.collection('cache').document(cache_key).get()
            if doc.exists:
                print(f"âœ… ìºì‹œ íˆíŠ¸: {url[:50]}...")
                return doc.to_dict().get('result')
            return None
        except Exception as e:
            print(f"âš ï¸ ìºì‹œ ì½ê¸° ì‹¤íŒ¨: {e}")
            return None

    def _set_cache(self, url: str, result):
        if not db:
            return
        try:
            cache_key = hashlib.md5(url.encode()).hexdigest()
            db.collection('cache').document(cache_key).set(
                {'url': url, 'result': result, 'cached_at': datetime.now()}
            )
            print(f"âœ… ìºì‹œ ì €ì¥: {url[:50]}...")
        except Exception as e:
            print(f"âš ï¸ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")

    def _get_sample_articles(self, keywords: list):
        """ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ ìƒ˜í”Œ ê¸°ì‚¬ ë°˜í™˜"""
        print("âš ï¸ ìƒ˜í”Œ ê¸°ì‚¬ ë°ì´í„° ë°˜í™˜ (ê²€ìƒ‰ ê¸°ëŠ¥ ë¹„í™œì„±í™”)")
        return [
            {
                'title': f'{" ".join(keywords[:2])}ì— ëŒ€í•œ ìƒ˜í”Œ ê¸°ì‚¬',
                'snippet': 'ì‹¤ì œ ê¸°ì‚¬ ê²€ìƒ‰ ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ë ¤ë©´ Google Search Grounding APIë¥¼ í™œì„±í™”í•˜ì„¸ìš”.',
                'url': '#',
                'source': 'Sample News',
                'country': 'Unknown',
                'credibility': 50,
                'bias': 'ì¤‘ë¦½',
                'published_date': '2024-01-01',
            }
        ]
