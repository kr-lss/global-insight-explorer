"""
ë¶„ì„ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ì„ ì²˜ë¦¬í•˜ëŠ” ì„œë¹„ìŠ¤ (Global Insight Explorer v2)
- í•œêµ­ì–´ ì‚¬ìš©ì ìµœì í™” (ì…ë ¥/ì¶œë ¥: í•œêµ­ì–´, ë‚´ë¶€ê²€ìƒ‰: ì˜ì–´)
- Google Search Grounding ì˜¤ë¥˜ ìˆ˜ì • ë° ìµœì í™”
"""
import os
import json
import hashlib
from datetime import datetime

import vertexai
from vertexai.generative_models import GenerativeModel, Tool, grounding
from google.cloud import firestore
from google.api_core.exceptions import GoogleAPICallError

from app.models.extractor import BaseExtractor, YoutubeExtractor, ArticleExtractor
from app.models.media import get_media_credibility
from app.config import config
from app.utils.gdelt_search import GDELTSearcher
from app.prompts.analysis_prompts import QUERY_OPTIMIZATION_PROMPT
from concurrent.futures import ThreadPoolExecutor, as_completed

# --- ì´ˆê¸°í™” ---
gemini = None
try:
    vertexai.init(project=config.GCP_PROJECT, location=config.GCP_REGION)
    # 1ì°¨/2ì°¨ ë¶„ì„ìš© ì¼ë°˜ ëª¨ë¸ (ë¹„ìš© íš¨ìœ¨ì ì¸ Flash ëª¨ë¸ ê¶Œì¥)
    gemini = GenerativeModel('gemini-2.0-flash') 
    print("âœ… (Service) Gemini API ì—°ê²° ì„±ê³µ")
except Exception as e:
    print(f"âš ï¸ (Service) Gemini API ì—°ê²° ì‹¤íŒ¨: {e}")

db = None
try:
    db = firestore.Client(project=config.GCP_PROJECT)
    print("âœ… (Service) Firestore ì—°ê²° ì„±ê³µ")
except Exception as e:
    print(f"âš ï¸ (Service) Firestore ì—°ê²° ì‹¤íŒ¨: {e}")


class AnalysisService:
    def __init__(self):
        self.extractors = {
            'youtube': YoutubeExtractor(),
            'article': ArticleExtractor(),
        }
        self.gdelt = GDELTSearcher()  # GDELT ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™”

    def _get_extractor(self, input_type: str) -> BaseExtractor:
        extractor = self.extractors.get(input_type)
        if not extractor:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì…ë ¥ íƒ€ì…: {input_type}")
        return extractor

    # ==================================================================
    # 1ï¸âƒ£ 1ì°¨ ë¶„ì„ (Initial Analysis) - í•œêµ­ì–´ ì‚¬ìš©ì ìµœì í™”
    # ==================================================================
    def analyze_content(self, url: str, input_type: str):
        # ìºì‹œ í™•ì¸
        cached = self._get_cache(url)
        if cached:
            return cached, True

        # ì½˜í…ì¸  ì¶”ì¶œ
        print(f"ğŸ“¥ ì½˜í…ì¸  ì¶”ì¶œ ì¤‘: {url[:50]}...")
        extractor = self._get_extractor(input_type)
        content = extractor.extract(url)
        
        if not content or len(content) < 50:
            raise Exception("ì½˜í…ì¸ ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ê±°ë‚˜ ë‚´ìš©ì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤.")
            
        print(f"âœ… ì¶”ì¶œ ì™„ë£Œ: {len(content)} ê¸€ì")

        # AI ë¶„ì„
        print("ğŸ¤– Geminië¡œ 1ì°¨ ë¶„ì„ ì¤‘ (í•œê¸€ ìš”ì•½ + ì˜ì–´ ê²€ìƒ‰ì–´ ìƒì„±)...")
        result = self._analyze_with_gemini_bridge(content)
        print("âœ… 1ì°¨ ë¶„ì„ ì™„ë£Œ")

        # ìºì‹œ ì €ì¥
        self._set_cache(url, result)
        return result, False

    def _analyze_with_gemini_bridge(self, content: str):
        """
        Gemini í”„ë¡¬í”„íŠ¸: í•œêµ­ì–´ ì…ë ¥ì„ ë°›ì•„ 'í•œêµ­ì–´ ìš”ì•½'ê³¼ 'ì˜ì–´ ê²€ìƒ‰ì–´'ë¥¼ ë™ì‹œ ìƒì„±
        """
        if not gemini:
            raise Exception("Gemini APIë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        content = content[:config.MAX_CONTENT_LENGTH_FIRST_ANALYSIS]

        prompt = f"""
        ë‹¹ì‹ ì€ êµ­ì œ ì •ì„¸ ë° ë¯¸ë””ì–´ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
        ì£¼ì–´ì§„ í…ìŠ¤íŠ¸(ì˜ìƒ ìë§‰ ë˜ëŠ” ê¸°ì‚¬)ë¥¼ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ì •ë³´ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œí•˜ì„¸ìš”.

        [ë¶„ì„ ëŒ€ìƒ í…ìŠ¤íŠ¸]
        {content}

        [í•„ìˆ˜ ìš”êµ¬ì‚¬í•­]
        1. **ì‚¬ìš©ìëŠ” í•œêµ­ì¸ì…ë‹ˆë‹¤.** ëª¨ë“  ì„¤ëª…ê³¼ ìš”ì•½ì€ ìì—°ìŠ¤ëŸ¬ìš´ **í•œêµ­ì–´**ë¡œ ì‘ì„±í•˜ì„¸ìš”.
        2. **ê²€ìƒ‰ í‚¤ì›Œë“œ(search_keywords_en)**ëŠ” ë°˜ë“œì‹œ **ì˜ì–´(English)**ë¡œ ì‘ì„±í•˜ì„¸ìš”.
           - âŒ ì˜ëª»ëœ ì˜ˆ: "ë¶í•œ ë¯¸ì‚¬ì¼", "ê²½ì œ ìœ„ê¸°"
           - âœ… ì˜¬ë°”ë¥¸ ì˜ˆ: "North Korea missile", "economic crisis"
           - ì´ìœ : ì „ ì„¸ê³„ ë‰´ìŠ¤(GDELT, Google) ê²€ìƒ‰ì€ ì˜ì–´ë¡œë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤
        3. **ê´€ë ¨ êµ­ê°€(target_country_codes)**ëŠ” í•´ë‹¹ ì´ìŠˆì™€ ì´í•´ê´€ê³„ê°€ ìˆëŠ” êµ­ê°€ë“¤ì˜ **2ìë¦¬ ISO ì½”ë“œ**ë¡œ ì‘ì„±í•˜ì„¸ìš”.
           (ì˜ˆ: í•œêµ­='KR', ë¯¸êµ­='US', ì¤‘êµ­='CN', ë¶í•œ='KP', ëŸ¬ì‹œì•„='RU', ì¼ë³¸='JP')

        [ì¶œë ¥ ì˜ˆì‹œ]
        {{
          "title_kr": "ë¶í•œ ì‹ í˜• ICBM ë°œì‚¬ì™€ êµ­ì œì‚¬íšŒ ë°˜ì‘",
          "summary_kr": "ë¶í•œì´ ì‹ í˜• ICBM í™”ì„±-18í˜¸ë¥¼ ë°œì‚¬í–ˆìŠµë‹ˆë‹¤. ë¯¸êµ­ê³¼ í•œêµ­ì€ ê°•ë ¥íˆ ê·œíƒ„í–ˆìœ¼ë©°, ìœ ì—” ì•ˆë³´ë¦¬ ê¸´ê¸‰íšŒì˜ê°€ ì†Œì§‘ë˜ì—ˆìŠµë‹ˆë‹¤.",
          "topics": ["ë¶í•œ", "ë¯¸ì‚¬ì¼", "êµ­ì œì •ì¹˜"],
          "key_claims": [
            {{
              "claim_kr": "ë¶í•œì˜ í™”ì„±-18í˜¸ëŠ” ì‚¬ê±°ë¦¬ 15,000kmì˜ ì‹ í˜• ICBMì´ë‹¤",
              "search_keywords_en": ["North Korea", "Hwasong-18", "ICBM", "intercontinental ballistic missile", "15000km range"],
              "target_country_codes": ["KR", "US", "JP", "CN"]
            }},
            {{
              "claim_kr": "ë¯¸êµ­ì€ ì¶”ê°€ ì œì¬ë¥¼ ê²€í†  ì¤‘ì´ë‹¤",
              "search_keywords_en": ["United States", "North Korea sanctions", "additional sanctions", "UN Security Council"],
              "target_country_codes": ["US", "KR", "CN", "RU"]
            }}
          ]
        }}

        [ì¶œë ¥ í˜•ì‹ (JSON Only)]
        ë°˜ë“œì‹œ ìœ„ ì˜ˆì‹œì™€ ë™ì¼í•œ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”.
        search_keywords_enì€ ì ˆëŒ€ë¡œ í•œêµ­ì–´ë¥¼ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
        JSON ì™¸ì— ë‹¤ë¥¸ ë§ì€ í•˜ì§€ ë§ˆì„¸ìš”.
        """

        try:
            response = gemini.generate_content(prompt)
            result_text = response.text.strip().replace('```json', '').replace('```', '').strip()
            analysis_result = json.loads(result_text)

            # âœ… ì˜ì–´ í‚¤ì›Œë“œ ê²€ì¦ ë° ìë™ ìƒì„±
            if 'key_claims' in analysis_result:
                for claim in analysis_result['key_claims']:
                    # search_keywords_enì´ ì—†ê±°ë‚˜ ë¹„ì–´ìˆìœ¼ë©´ claim_krë¡œ ìƒì„± (í´ë°±)
                    if not claim.get('search_keywords_en') or len(claim.get('search_keywords_en', [])) == 0:
                        print(f"âš ï¸ ì˜ì–´ í‚¤ì›Œë“œ ëˆ„ë½ ê°ì§€, claim_krë¡œ ëŒ€ì²´: {claim.get('claim_kr', '')[:30]}...")
                        claim['search_keywords_en'] = [claim.get('claim_kr', '')]
                    else:
                        # í•œê¸€ í¬í•¨ ì—¬ë¶€ í™•ì¸ (ê°„ë‹¨í•œ ìœ ë‹ˆì½”ë“œ ë²”ìœ„ ì²´í¬)
                        keywords = claim.get('search_keywords_en', [])
                        has_korean = any(
                            any('\uac00' <= char <= '\ud7a3' for char in keyword)
                            for keyword in keywords
                        )
                        if has_korean:
                            print(f"âš ï¸ search_keywords_enì— í•œê¸€ ê°ì§€! AIê°€ í”„ë¡¬í”„íŠ¸ë¥¼ ë”°ë¥´ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {keywords}")
                            print(f"   â†’ ì´ í‚¤ì›Œë“œë¡œ GDELT ê²€ìƒ‰ì´ ì‹¤íŒ¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. claim_kr: {claim.get('claim_kr', '')[:50]}")

                    # target_country_codesê°€ ì—†ìœ¼ë©´ ë¹ˆ ë°°ì—´ë¡œ ì´ˆê¸°í™”
                    if 'target_country_codes' not in claim:
                        claim['target_country_codes'] = []

            return analysis_result
        except Exception as e:
            print(f"âŒ AI 1ì°¨ ë¶„ì„ ì‹¤íŒ¨: {e}")
            raise Exception(f"AI ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

    def optimize_search_query(self, user_input: str, context: dict):
        """
        [Step 1] ì‚¬ìš©ì ì…ë ¥ì„ GDELT 5ëŒ€ ìš”ì†Œ ê²€ìƒ‰ ì „ëµìœ¼ë¡œ ë³€í™˜ (Gemini ì‚¬ìš©)

        Args:
            user_input: ì‚¬ìš©ìì˜ ìì—°ì–´ ì§ˆë¬¸
            context: ë¶„ì„ ì»¨í…ìŠ¤íŠ¸ {'title_kr', 'key_claims'}

        Returns:
            {
                "success": True/False,
                "data": {
                    "interpreted_intent": "...",
                    "gdelt_params": {
                        "keywords": [...],
                        "entities": [...],
                        "locations": [...],
                        "themes": [...],
                        "event_date": "YYYY-MM-DD"
                    },
                    "search_keywords_en": [...],  # í•˜ìœ„ í˜¸í™˜ì„±
                    "target_country_codes": [...],
                    "confidence": 0.95
                },
                "error": "..." (ì‹¤íŒ¨ ì‹œ)
            }
        """
        try:
            if not gemini:
                raise Exception("Gemini APIë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

            # ë¬¸ë§¥ ì •ë³´ ì¶”ì¶œ (ì—†ìœ¼ë©´ ê¸°ë³¸ê°’)
            context_title = context.get('title_kr', '')
            context_claims = context.get('key_claims', [])

            # 5ëŒ€ ìš”ì†Œ ì¶”ì¶œ í”„ë¡¬í”„íŠ¸
            prompt = f"""
            ë‹¹ì‹ ì€ ë°ì´í„° ì €ë„ë¦¬ì¦˜ ë° GDELT ê²€ìƒ‰ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
            ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ê¸€ë¡œë²Œ ë‰´ìŠ¤ ê²€ìƒ‰ì— í•„ìš”í•œ 5ëŒ€ ìš”ì†Œë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.

            [ì§ˆë¬¸] "{user_input}"

            [ë¬¸ë§¥ ì •ë³´]
            ì œëª©: {context_title}
            ê´€ë ¨ ì£¼ì¥: {str(context_claims)[:500]}

            [í•„ìˆ˜ ì§€ì‹œì‚¬í•­]
            1. **ëª¨ë“  ê²€ìƒ‰ ìš”ì†ŒëŠ” ë°˜ë“œì‹œ ì˜ì–´(English)**ë¡œ ì‘ì„±í•˜ì„¸ìš”.
            2. **event_date**: ì‚¬ê±´ì´ ë°œìƒí•œ ë‚ ì§œ (YYYY-MM-DD í˜•ì‹). ì •í™•í•œ ë‚ ì§œë¥¼ ëª¨ë¥´ë©´ ìµœê·¼ ë‚ ì§œ ì¶”ì •.
            3. **entities**: í•µì‹¬ ì¸ë¬¼/ì¡°ì§ ì˜ë¬¸ëª… (ì˜ˆ: ["Kim Jong Un", "NATO"])
            4. **locations**: ê´€ë ¨ ë„ì‹œ/êµ­ê°€ ì˜ë¬¸ëª… (ì˜ˆ: ["Seoul", "Ukraine", "Middle East"])
            5. **themes**: GDELT í…Œë§ˆ ì½”ë“œ (ì˜ˆ: ["ARMEDCONFLICT", "SCANDAL", "ECON_INFLATION"])
               - ì£¼ìš” í…Œë§ˆ: ARMEDCONFLICT, SCANDAL, HEALTH_PANDEMIC, ECON_INFLATION, TERROR, ENV_CLIMATECHANGE
            6. **keywords**: ì¼ë°˜ ê²€ìƒ‰ í‚¤ì›Œë“œ (ìœ„ì— í¬í•¨ë˜ì§€ ì•Šì€ ì¶”ê°€ ë‹¨ì–´)

            [ì¶œë ¥ í˜•ì‹ (JSON Only)]
            {{
                "interpreted_intent": "ì§ˆë¬¸ ì˜ë„ë¥¼ í•œêµ­ì–´ë¡œ ìš”ì•½",
                "gdelt_params": {{
                    "event_date": "2024-01-15",
                    "keywords": ["missile", "test"],
                    "entities": ["Kim Jong Un", "US Defense Department"],
                    "locations": ["North Korea", "Pacific Ocean"],
                    "themes": ["ARMEDCONFLICT", "WB_1678_SECURITY_THREAT"]
                }},
                "search_keywords_en": ["North Korea", "missile", "test"],
                "target_country_codes": ["KP", "US", "KR"],
                "confidence": 0.9
            }}

            [ì˜ˆì‹œ]
            ì§ˆë¬¸: "ë¶í•œì˜ ìµœê·¼ ë¯¸ì‚¬ì¼ ë°œì‚¬ì— ëŒ€í•œ ë¯¸êµ­ì˜ ë°˜ì‘ì€?"
            ì¶œë ¥:
            {{
                "interpreted_intent": "ë¶í•œ ë¯¸ì‚¬ì¼ ë°œì‚¬ì— ëŒ€í•œ ë¯¸êµ­ì˜ ê³µì‹ ì…ì¥ ë° ëŒ€ì‘ ì¡°ì¹˜",
                "gdelt_params": {{
                    "event_date": "2024-11-20",
                    "keywords": ["missile", "launch", "response"],
                    "entities": ["North Korea", "United States", "Pentagon"],
                    "locations": ["North Korea", "Washington"],
                    "themes": ["ARMEDCONFLICT", "WB_1678_SECURITY_THREAT"]
                }},
                "search_keywords_en": ["North Korea", "missile", "US response"],
                "target_country_codes": ["KP", "US", "KR"],
                "confidence": 0.95
            }}

            JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”. ë‹¤ë¥¸ ë§ì€ í•˜ì§€ ë§ˆì„¸ìš”.
            """

            print(f"ğŸ¤– 5ëŒ€ ìš”ì†Œ ê²€ìƒ‰ ì¿¼ë¦¬ ìµœì í™” ì¤‘: '{user_input[:50]}...'")

            # Gemini í˜¸ì¶œ
            response = gemini.generate_content(prompt)
            result_text = response.text.strip().replace('```json', '').replace('```', '').strip()

            # JSON íŒŒì‹±
            optimized_data = json.loads(result_text)

            # í•˜ìœ„ í˜¸í™˜ì„±: search_keywords_enì´ ì—†ìœ¼ë©´ keywordsì—ì„œ ìƒì„±
            if 'search_keywords_en' not in optimized_data and 'gdelt_params' in optimized_data:
                gdelt_params = optimized_data['gdelt_params']
                all_keywords = gdelt_params.get('keywords', []) + gdelt_params.get('entities', [])
                optimized_data['search_keywords_en'] = all_keywords[:config.MAX_KEYWORDS]

            print(f"âœ… 5ëŒ€ ìš”ì†Œ ì¶”ì¶œ ì™„ë£Œ (confidence: {optimized_data.get('confidence', 0)})")

            return {
                "success": True,
                "data": optimized_data
            }

        except Exception as e:
            print(f"âš ï¸ ì¿¼ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")

            # Fallback: ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ ê·¸ëŒ€ë¡œ í‚¤ì›Œë“œë¡œ ì‚¬ìš©
            return {
                "success": False,
                "error": str(e),
                "data": {
                    "interpreted_intent": "Fallback raw search",
                    "gdelt_params": {
                        "keywords": [user_input],
                        "entities": [],
                        "locations": [],
                        "themes": [],
                        "event_date": datetime.now().strftime('%Y-%m-%d')
                    },
                    "search_keywords_en": [user_input],
                    "target_country_codes": [],
                    "confidence": 0.1
                }
            }

    # ==================================================================
    # 2ï¸âƒ£ 2ì°¨ ë¶„ì„ (Find Sources) - AI ì¶”ë¡  ì—†ì´ ê²€ìƒ‰ë§Œ ìˆ˜í–‰
    # ==================================================================
    def find_sources_for_claims(
        self, url: str, input_type: str, claims_data: list
    ):
        """
        [Step 2] í™•ì •ëœ ê²€ìƒ‰ ì „ëµ(claims_data)ìœ¼ë¡œ ì‹¤ì œ GDELT 5ëŒ€ ìš”ì†Œ ê²€ìƒ‰ ìˆ˜í–‰
        * ì´ì œ ì´ í•¨ìˆ˜ëŠ” AI ì¶”ë¡ ì„ í•˜ì§€ ì•Šê³ , ì „ë‹¬ë°›ì€ íŒŒë¼ë¯¸í„°ë¡œ ê²€ìƒ‰ ìˆ˜í–‰ì—ë§Œ ì§‘ì¤‘í•©ë‹ˆë‹¤.

        Args:
            url: ì›ë³¸ ì½˜í…ì¸  URL (í˜„ì¬ëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
            input_type: ì½˜í…ì¸  íƒ€ì… (í˜„ì¬ëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
            claims_data: ì£¼ì¥ ì •ë³´ ë¦¬ìŠ¤íŠ¸
                [
                    {
                        "claim_kr": "í•œêµ­ì–´ ì£¼ì¥",
                        "gdelt_params": {  # 5ëŒ€ ìš”ì†Œ (ì‹ ê·œ)
                            "keywords": [...],
                            "entities": [...],
                            "locations": [...],
                            "themes": [...],
                            "event_date": "YYYY-MM-DD"
                        },
                        "search_keywords_en": [...],  # í•˜ìœ„ í˜¸í™˜ì„±
                        "target_country_codes": [...]
                    },
                    ...
                ]

        Returns:
            (result, articles) tuple
            - result: ê° ì£¼ì¥ë³„ ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
            - articles: ëª¨ë“  ê¸°ì‚¬ë¥¼ í‰íƒ„í™”í•œ ë¦¬ìŠ¤íŠ¸
        """
        all_results = []
        all_articles = []

        # ê° ì£¼ì¥ë³„ë¡œ ë…ë¦½ì ì¸ ê²€ìƒ‰ ìˆ˜í–‰
        for claim_data in claims_data:
            claim_kr = claim_data.get('claim_kr', '')

            # âœ… NEW: gdelt_params ìš°ì„  ì‚¬ìš© (5ëŒ€ ìš”ì†Œ ê²€ìƒ‰)
            gdelt_params = claim_data.get('gdelt_params')

            if not gdelt_params:
                # Fallback: ê¸°ì¡´ search_keywords_en ë°©ì‹ìœ¼ë¡œ ë³€í™˜
                search_keywords = claim_data.get('search_keywords_en', [])
                target_countries = claim_data.get('target_country_codes', [])

                if not search_keywords:
                    print(f"âš ï¸ ê²€ìƒ‰ íŒŒë¼ë¯¸í„° ì—†ìŒ - ìŠ¤í‚µ: '{claim_kr[:30]}...'")
                    continue

                gdelt_params = {
                    'keywords': search_keywords,
                    'entities': [],
                    'locations': [],
                    'themes': [],
                    'event_date': datetime.now().strftime('%Y-%m-%d')
                }
                print(f"ğŸ” '{claim_kr[:15]}...' ê²€ìƒ‰ (Legacy ëª¨ë“œ: keywords={search_keywords})")
            else:
                print(f"ğŸ” '{claim_kr[:15]}...' ê²€ìƒ‰ (5ëŒ€ ìš”ì†Œ ëª¨ë“œ)")
                print(f"   entities={gdelt_params.get('entities', [])} locations={gdelt_params.get('locations', [])}")
                print(f"   themes={gdelt_params.get('themes', [])} keywords={gdelt_params.get('keywords', [])}")

            # GDELT 5ëŒ€ ìš”ì†Œ ê²€ìƒ‰ ì‹¤í–‰
            articles = self._search_real_articles_with_params(gdelt_params)

            # ê²°ê³¼ êµ¬ì¡°í™”
            result_entry = {
                "claim": claim_kr,
                "searched_keywords": gdelt_params.get('keywords', []),
                "articles": articles
            }
            all_results.append(result_entry)
            all_articles.extend(articles)

        # ì¤‘ë³µ ì œê±° (URL ê¸°ì¤€)
        unique_articles = {v['url']: v for v in all_articles}.values()
        final_articles = list(unique_articles)

        print(f"âœ… ê²€ìƒ‰ ì™„ë£Œ: {len(final_articles)}ê°œ ê¸°ì‚¬ ë°œê²¬")

        # AI ë¶„ì„ ì—†ì´ ê²€ìƒ‰ ê²°ê³¼ë§Œ ë°˜í™˜
        return {"results": all_results}, final_articles

    def _generate_keywords_on_the_fly(self, claim_kr: str):
        """ì‚¬ìš©ì ì…ë ¥ ì£¼ì¥ì„ ìœ„í•œ ì˜ì–´ í‚¤ì›Œë“œ ë° íƒ€ê²Ÿ êµ­ê°€ ìƒì„±"""
        if not gemini:
            return {"keywords": [claim_kr], "countries": []}

        try:
            prompt = f"""
            Translate this Korean claim into 2-3 English search keywords for news verification.
            Also suggest 2 relevant country codes (ISO 3166-1 alpha-2).
            Claim: "{claim_kr}"
            Output JSON: {{"keywords": ["kw1", "kw2"], "countries": ["US", "KR"]}}
            """
            response = gemini.generate_content(prompt)
            text = response.text.strip().replace('```json', '').replace('```', '').strip()
            return json.loads(text)
        except Exception as e:
            print(f"âš ï¸ í‚¤ì›Œë“œ ìƒì„± ì‹¤íŒ¨: {e}")
            return {"keywords": [claim_kr], "countries": []}

    def _search_real_articles_with_params(self, gdelt_params: dict):
        """
        GDELT 5ëŒ€ ìš”ì†Œ ê²€ìƒ‰ with Google Search Fallback

        Args:
            gdelt_params: {
                'keywords': [...],
                'entities': [...],
                'locations': [...],
                'themes': [...],
                'event_date': 'YYYY-MM-DD'
            }

        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        if not gdelt_params:
            return []

        # 1ï¸âƒ£ GDELT 5ëŒ€ ìš”ì†Œ ê²€ìƒ‰ ì‹œë„ (ë¬´ë£Œ, ë¹ ë¦„, ê¸€ë¡œë²Œ)
        print(f"ğŸ“Š [1/2] GDELT 5ëŒ€ ìš”ì†Œ ê²€ìƒ‰ ì¤‘...")
        gdelt_results = []
        try:
            gdelt_results = self.gdelt.search(gdelt_params)
        except Exception as e:
            print(f"âš ï¸ GDELT ê²€ìƒ‰ ì‹¤íŒ¨: {e}")

        # 2ï¸âƒ£ ë³‘ë ¬ ë³¸ë¬¸ ì¶”ì¶œ (ThreadPool 10ê°œ ì›Œì»¤)
        if gdelt_results:
            print(f"ğŸ”„ ë³‘ë ¬ ë³¸ë¬¸ ì¶”ì¶œ ì¤‘... ({len(gdelt_results)}ê°œ ê¸°ì‚¬)")
            extracted = self._extract_contents_parallel(gdelt_results)
            print(f"âœ… ì¶”ì¶œ ì™„ë£Œ: {len(extracted)}ê°œ")
            return extracted

        # 3ï¸âƒ£ GDELT ì‹¤íŒ¨ ì‹œ Google Search Grounding í´ë°±
        print(f"âš ï¸ GDELT ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ, Google Search ì‹œë„...")

        # gdelt_paramsì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ (ëª¨ë“  ìš”ì†Œ ê²°í•©)
        all_keywords = []
        all_keywords.extend(gdelt_params.get('keywords', []))
        all_keywords.extend(gdelt_params.get('entities', []))
        all_keywords.extend(gdelt_params.get('locations', []))

        google_results = self._search_google_fallback(all_keywords[:config.MAX_KEYWORDS], [])

        if google_results:
            print(f"âœ… Google Search ì™„ë£Œ: {len(google_results)}ê°œ ë°œê²¬")
            return google_results

        print(f"âš ï¸ Google Searchë„ ê²°ê³¼ ì—†ìŒ")
        return []

    def _search_real_articles(self, keywords: list, target_countries: list = None):
        """
        GDELT Hybrid ê²€ìƒ‰ (Legacy Wrapper)

        ì´ í•¨ìˆ˜ëŠ” í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€ë©ë‹ˆë‹¤.
        ë‚´ë¶€ì ìœ¼ë¡œ _search_real_articles_with_params()ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.

        Args:
            keywords: ì˜ì–´ ê²€ìƒ‰ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
            target_countries: íƒ€ê²Ÿ êµ­ê°€ ì½”ë“œ ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: ["US", "CN"])

        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        if not keywords:
            return []

        # í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ í‰íƒ„í™”
        flat_keywords = []
        for k in keywords:
            if isinstance(k, list):
                flat_keywords.extend(k)
            else:
                flat_keywords.append(k)

        # Legacy íŒŒë¼ë¯¸í„°ë¥¼ 5ëŒ€ ìš”ì†Œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        gdelt_params = {
            'keywords': flat_keywords[:config.MAX_KEYWORDS],
            'entities': [],
            'locations': [],
            'themes': [],
            'event_date': datetime.now().strftime('%Y-%m-%d')
        }

        # ìƒˆë¡œìš´ 5ëŒ€ ìš”ì†Œ ê²€ìƒ‰ í•¨ìˆ˜ í˜¸ì¶œ
        return self._search_real_articles_with_params(gdelt_params)

    def _extract_contents_parallel(self, articles_meta: list):
        """
        ë³‘ë ¬ ì²˜ë¦¬ë¡œ ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ (ThreadPool)

        Args:
            articles_meta: GDELT ê²€ìƒ‰ ê²°ê³¼ [{url, source, title, date, tone, country}, ...]

        Returns:
            ë³¸ë¬¸ì´ ì¶”ì¶œëœ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
        """
        extracted = []
        extractor = self.extractors['article']

        def fetch_one(meta):
            """ë‹¨ì¼ ê¸°ì‚¬ ì¶”ì¶œ (ë³‘ë ¬ ì‹¤í–‰ í•¨ìˆ˜)"""
            try:
                url = meta.get('url', '')
                if not url or url == '#':
                    return None

                # ì œëª©ê³¼ ë³¸ë¬¸ ì¶”ì¶œ
                result = extractor.extract_with_title(url)
                title = result.get('title', '')
                content = result.get('content', '')

                # ë„ˆë¬´ ì§§ìœ¼ë©´ ë¬´ì‹œ
                if not content or len(content) < 100:
                    return None

                # ë©”íƒ€ë°ì´í„°ì— ì œëª©ê³¼ ë³¸ë¬¸ ì¶”ê°€
                meta['title'] = title if title else meta.get('source', 'No title')  # ì œëª©ì´ ì—†ìœ¼ë©´ ì¶œì²˜ë¥¼ ì œëª©ìœ¼ë¡œ
                meta['content'] = content
                meta['snippet'] = content[:500]  # ë¯¸ë¦¬ë³´ê¸°

                # ì–¸ë¡ ì‚¬ ì •ë³´ ì¶”ê°€ (êµ­ê°€/ì¶œì²˜ ê¸°ë°˜)
                media_info = get_media_credibility(
                    meta.get('source', ''),
                    meta.get('country', '')
                )

                # êµ­ì˜/ë¯¼ì˜ ì •ë³´ë§Œ ì¶”ê°€
                if media_info:
                    meta['media_type'] = media_info.get('type', 'ì•Œ ìˆ˜ ì—†ìŒ')
                    meta['media_category'] = media_info.get('category', 'ì•Œ ìˆ˜ ì—†ìŒ')

                print(f"âœ… ì¶”ì¶œ ì„±ê³µ: {meta.get('source', 'Unknown')} ({meta.get('country', 'Unknown')}) - {meta.get('media_type', 'Unknown')}")
                return meta

            except Exception as e:
                print(f"âš ï¸ ì¶”ì¶œ ì‹¤íŒ¨: {meta.get('url', 'unknown')} - {e}")
                return None

        # ThreadPool ë³‘ë ¬ ì‹¤í–‰
        with ThreadPoolExecutor(max_workers=config.THREAD_POOL_WORKERS) as executor:
            futures = [executor.submit(fetch_one, item) for item in articles_meta]

            for future in as_completed(futures):
                result = future.result()
                if result:
                    extracted.append(result)

        return extracted

    def _search_google_fallback(self, keywords: list, target_countries: list = None):
        """
        Google Search í´ë°± (GDELT ì‹¤íŒ¨ ì‹œ)

        Args:
            keywords: ì˜ì–´ ê²€ìƒ‰ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
            target_countries: íƒ€ê²Ÿ êµ­ê°€ ì½”ë“œ ë¦¬ìŠ¤íŠ¸

        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        if not keywords:
            return []

        # ê¸°ë³¸ ì¿¼ë¦¬
        base_query = " ".join(keywords[:config.MAX_KEYWORDS])
        query = base_query

        # íƒ€ê²Ÿ êµ­ê°€ê°€ ìˆìœ¼ë©´ ì¿¼ë¦¬ì— ì¶”ê°€
        if target_countries and len(target_countries) > 0:
            country_query = " OR ".join(target_countries)
            query = f"{base_query} ({country_query})"

        print(f"ğŸ” Google Search Query: {query}")

        try:
            # Google Search Grounding ì‹œë„ (Tool ê°ì²´ ì—†ì´ ì§ì ‘ grounding ì‚¬ìš©)
            model = GenerativeModel('gemini-2.0-flash')

            # tools íŒŒë¼ë¯¸í„°ëŠ” generate_contentì— ì§ì ‘ ì „ë‹¬

            prompt = f"""Find recent news articles about: {query}

            Return a JSON list of articles with this structure:
            [
              {{"title": "article title", "url": "https://...", "source": "source name"}},
              ...
            ]

            Only return valid JSON, no other text."""

            # Google Search Groundingì„ toolsë¡œ ì „ë‹¬
            response = model.generate_content(
                prompt,
                tools=[grounding.GoogleSearchRetrieval()]
            )

            # Grounding Metadataì—ì„œ URL ì¶”ì¶œ ì‹œë„
            articles = []

            # 1. ì‘ë‹µ í…ìŠ¤íŠ¸ì—ì„œ JSON íŒŒì‹± ì‹œë„
            try:
                import re
                text = response.text
                # JSON ì¶”ì¶œ
                json_match = re.search(r'\[.*\]', text, re.DOTALL)
                if json_match:
                    import json
                    parsed = json.loads(json_match.group())
                    for item in parsed[:10]:  # ìµœëŒ€ 10ê°œ
                        if isinstance(item, dict) and 'url' in item:
                            articles.append({
                                'title': item.get('title', 'No title'),
                                'url': item.get('url', '#'),
                                'source': item.get('source', 'Unknown'),
                                'snippet': item.get('snippet', '')[:500],
                                'country': target_countries[0] if target_countries else 'Unknown',
                                'content': ''
                            })
            except Exception as parse_error:
                print(f"âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨: {parse_error}")

            # 2. Grounding Metadata í™•ì¸ (ìˆë‹¤ë©´)
            if hasattr(response, 'candidates') and response.candidates:
                for candidate in response.candidates:
                    if hasattr(candidate, 'grounding_metadata'):
                        metadata = candidate.grounding_metadata
                        if hasattr(metadata, 'grounding_chunks'):
                            for chunk in metadata.grounding_chunks[:10]:
                                if hasattr(chunk, 'web') and hasattr(chunk.web, 'uri'):
                                    articles.append({
                                        'title': getattr(chunk.web, 'title', 'No title'),
                                        'url': chunk.web.uri,
                                        'source': 'Google Search',
                                        'snippet': '',
                                        'country': target_countries[0] if target_countries else 'Unknown',
                                        'content': ''
                                    })

            if articles:
                print(f"âœ… Google Searchì—ì„œ {len(articles)}ê°œ URL ì¶”ì¶œ ì„±ê³µ")
                return articles

            # 3. ì‹¤íŒ¨ ì‹œ ìƒ˜í”Œ ë°ì´í„° ë°˜í™˜ (ì™„ì „ ì‹¤íŒ¨ ë°©ì§€)
            print("âš ï¸ Google Search URL ì¶”ì¶œ ì‹¤íŒ¨, ìƒ˜í”Œ ë°ì´í„° ë°˜í™˜")
            return []  # ìƒ˜í”Œ ë°ì´í„° ëŒ€ì‹  ë¹ˆ ë°°ì—´ ë°˜í™˜

        except Exception as e:
            print(f"âš ï¸ Google Search ì‹¤íŒ¨: {e}")
            return []

    def _compare_perspectives_with_gemini(
        self, original_content: str, claims: list, articles: list
    ):
        if not gemini:
            raise Exception("Gemini APIë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        original_content = original_content[:config.MAX_CONTENT_LENGTH_SECOND_ANALYSIS]
        
        # ê¸°ì‚¬ ëª©ë¡ í…ìŠ¤íŠ¸í™”
        articles_text = "\n".join([
            f"- [{a['source']} ({a['country']})] {a['title']}: {a['snippet']}"
            for a in articles
        ])

        prompt = f"""
        ë‹¹ì‹ ì€ ì¤‘ë¦½ì ì¸ 'êµ­ì œ ë‰´ìŠ¤ ë¶„ì„ê°€'ì…ë‹ˆë‹¤.
        ì‚¬ìš©ìê°€ ì„ íƒí•œ ì£¼ì¥ì— ëŒ€í•´, ì„¸ê³„ ê°êµ­ì˜ ì–¸ë¡ ì´ ì–´ë–»ê²Œ ë³´ë„í•˜ê³  ìˆëŠ”ì§€ ê°ê´€ì ìœ¼ë¡œ ë¹„êµ ë¶„ì„í•´ì£¼ì„¸ìš”.
        **ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.**

        **ì ˆëŒ€ë¡œ íŠ¹ì • ì£¼ì¥ì´ ì‚¬ì‹¤ì¸ì§€ ê±°ì§“ì¸ì§€ ë‹¨ì • ì§“ì§€ ë§ˆì„¸ìš”.**
        ì˜¤ì§ 'A ì–¸ë¡ ì‚¬ëŠ” ì´ë ‡ê²Œ ë³´ë„í–ˆê³ , B ì–¸ë¡ ì‚¬ëŠ” ì €ë ‡ê²Œ ë³´ë„í–ˆë‹¤'ëŠ” ì°¨ì´ì ê³¼ ë§¥ë½ì„ ë³´ì—¬ì£¼ëŠ” ë° ì§‘ì¤‘í•˜ì„¸ìš”.

        [ë¶„ì„ ëŒ€ìƒ ì£¼ì¥]
        {chr(10).join([f'- {c}' for c in claims])}

        [ìˆ˜ì§‘ëœ ê¸°ì‚¬ ë°ì´í„°]
        {articles_text}

        [ì§€ì‹œì‚¬í•­]
        1. ê° ì£¼ì¥ì— ëŒ€í•´ ìˆ˜ì§‘ëœ ê¸°ì‚¬ë“¤ì´ **ì§€ì§€(Supporting)**í•˜ëŠ”ì§€, **ë°˜ë°•(Opposing)**í•˜ëŠ”ì§€, ë˜ëŠ” **ì¤‘ë¦½/ê´€ë ¨ì—†ìŒ**ì¸ì§€ ë¶„ì„í•˜ì„¸ìš”.
        2. êµ­ê°€ë³„ ì–¸ë¡ ì˜ ì‹œê° ì°¨ì´ê°€ ìˆë‹¤ë©´ ì§€ì í•´ì£¼ì„¸ìš” (ì˜ˆ: ë¯¸êµ­ ì–¸ë¡ ì€ ê²½ì œì  ì¸¡ë©´ì„, ì¤‘êµ­ ì–¸ë¡ ì€ ì •ì¹˜ì  ì¸¡ë©´ì„ ê°•ì¡°).
        3. **íŒë‹¨ì€ ì‚¬ìš©ìì—ê²Œ ë§¡ê¸°ê³ **, ë‹¤ì–‘í•œ ê´€ì ì´ ìˆë‹¤ëŠ” ê²ƒë§Œ ë³´ì—¬ì£¼ì„¸ìš”.

        [ì‘ë‹µ í˜•ì‹ (JSON)]
        {{
          "results": [
            {{
              "claim": "ì£¼ì¥ ë‚´ìš©",
              "perspectives": [
                 {{
                   "country": "US",
                   "media": "CNN",
                   "stance": "Supporting",
                   "viewpoint": "ì´ ê¸°ì‚¬ëŠ” ~~í•œ ê·¼ê±°ë¥¼ ë“¤ì–´ í•´ë‹¹ ì£¼ì¥ì„ ì§€ì§€í•˜ëŠ” ë…¼ì¡°ì…ë‹ˆë‹¤."
                 }},
                 {{
                   "country": "CN",
                   "media": "Global Times",
                   "stance": "Opposing",
                   "viewpoint": "ë°˜ë©´ ì´ ê¸°ì‚¬ëŠ” ~~ë¼ë©° ë‹¤ë¥¸ ê´€ì ì„ ì œì‹œí•©ë‹ˆë‹¤."
                 }}
              ],
              "summary_kr": "ì¢…í•©í•´ë³´ë©´ ë¯¸êµ­ ì–¸ë¡ ì€ ê²½ì œì  ì¸¡ë©´ì„, ì¤‘êµ­ ì–¸ë¡ ì€ ì •ì¹˜ì  ì¸¡ë©´ì„ ê°•ì¡°í•˜ê³  ìˆìŠµë‹ˆë‹¤. (íŒë‹¨ì€ ì‚¬ìš©ì ëª«)"
            }}
          ]
        }}
        """
        
        try:
            response = gemini.generate_content(prompt)
            result_text = response.text.strip().replace('```json', '').replace('```', '').strip()
            return json.loads(result_text)
        except Exception as e:
            print(f"âŒ AI 2ì°¨ ë¶„ì„ ì‹¤íŒ¨: {e}")
            # ë¹ˆ ê²°ê³¼ ë°˜í™˜í•˜ì—¬ í”„ë¡ íŠ¸ì—”ë“œ ì—ëŸ¬ ë°©ì§€
            return {"results": []}

    # --- ìºì‹œ ë° ìœ í‹¸ë¦¬í‹° ---
    def _get_cache(self, url: str):
        if not db: return None
        try:
            cache_key = hashlib.md5(url.encode()).hexdigest()
            doc = db.collection('cache').document(cache_key).get()
            if doc.exists:
                print(f"âœ… ìºì‹œ íˆíŠ¸: {url[:30]}...")
                return doc.to_dict().get('result')
        except: pass
        return None

    def _set_cache(self, url: str, result):
        if not db: return
        try:
            cache_key = hashlib.md5(url.encode()).hexdigest()
            db.collection('cache').document(cache_key).set({
                'url': url, 'result': result, 'cached_at': datetime.now()
            })
        except: pass

    def _get_sample_articles(self, keywords: list, target_countries: list = None):
        """ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ë°ì´í„°"""
        k = keywords[0] if keywords else "ì´ìŠˆ"

        # íƒ€ê²Ÿ êµ­ê°€ì— ë§ëŠ” ìƒ˜í”Œ ë°ì´í„° ìƒì„±
        sample_sources = []
        if target_countries and len(target_countries) > 0:
            # íƒ€ê²Ÿ êµ­ê°€ë³„ ëŒ€í‘œ ì–¸ë¡ ì‚¬ ë§¤í•‘
            country_media = {
                'US': {'source': 'CNN', 'credibility': 80},
                'UK': {'source': 'BBC', 'credibility': 85},
                'CN': {'source': 'Xinhua', 'credibility': 60},
                'RU': {'source': 'RT', 'credibility': 55},
                'JP': {'source': 'NHK', 'credibility': 75},
                'KR': {'source': 'Yonhap', 'credibility': 75},
                'FR': {'source': 'France 24', 'credibility': 80},
                'DE': {'source': 'DW', 'credibility': 80},
            }
            for country in target_countries[:3]:  # ìµœëŒ€ 3ê°œêµ­
                media = country_media.get(country, {'source': f'{country} News', 'credibility': 70})
                sample_sources.append({
                    'title': f'{media["source"]}: {k} coverage',
                    'snippet': f'{country} perspective on {k}...',
                    'url': '#',
                    'source': media['source'],
                    'country': country,
                    'credibility': media['credibility']
                })
        else:
            # ê¸°ë³¸ ìƒ˜í”Œ (íƒ€ê²Ÿ êµ­ê°€ ì—†ì„ ë•Œ)
            sample_sources = [
                {'title': f'Global view on {k}', 'snippet': 'Western media perspective...', 'url': '#', 'source': 'CNN', 'country': 'US', 'credibility': 80},
                {'title': f'Alternative view on {k}', 'snippet': 'Eastern media perspective...', 'url': '#', 'source': 'Xinhua', 'country': 'CN', 'credibility': 60},
            ]

        return sample_sources