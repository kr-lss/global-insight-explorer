"""
ë¶„ì„ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ì„ ì²˜ë¦¬í•˜ëŠ” ì„œë¹„ìŠ¤ (Facade íŒ¨í„´)
"""
import os
import json
import hashlib
from datetime import datetime

import vertexai
from vertexai.generative_models import GenerativeModel
from google.cloud import firestore

from app.models.extractor import BaseExtractor, YoutubeExtractor, ArticleExtractor
from app.models.media import get_media_credibility
from app.config import config

# Gemini ëª¨ë¸ ë¡œë“œ
gemini = None
try:
    vertexai.init(project=config.GCP_PROJECT, location=config.GCP_REGION)
    gemini = GenerativeModel('gemini-1.5-flash')
    print("âœ… (Service) Gemini API ì—°ê²° ì„±ê³µ")
except Exception as e:
    print(f"âš ï¸ (Service) Gemini API ì—°ê²° ì‹¤íŒ¨: {e}")

# Firestore í´ë¼ì´ì–¸íŠ¸
db = None
try:
    db = firestore.Client(project=config.GCP_PROJECT)
    print("âœ… (Service) Firestore ì—°ê²° ì„±ê³µ")
except Exception as e:
    print(f"âš ï¸ (Service) Firestore ì—°ê²° ì‹¤íŒ¨: {e}")


class AnalysisService:
    def __init__(self):
        self.extractors = {
            'youtube': YoutubeExtractor(),
            'article': ArticleExtractor(),
        }

    def _get_extractor(self, input_type: str) -> BaseExtractor:
        extractor = self.extractors.get(input_type)
        if not extractor:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì…ë ¥ íƒ€ì…: {input_type}")
        return extractor

    # --- 1ì°¨ ë¶„ì„ ---
    def analyze_content(self, url: str, input_type: str):
        # ìºì‹œ í™•ì¸
        cached = self._get_cache(url)
        if cached:
            return cached, True

        # ì½˜í…ì¸  ì¶”ì¶œ
        print(f"ğŸ“¥ ì½˜í…ì¸  ì¶”ì¶œ ì¤‘: {url[:50]}...")
        extractor = self._get_extractor(input_type)
        content = extractor.extract(url)
        print(f"âœ… ì¶”ì¶œ ì™„ë£Œ: {len(content)} ê¸€ì")

        # AI ë¶„ì„
        print("ğŸ¤– Geminië¡œ 1ì°¨ ë¶„ì„ ì¤‘...")
        result = self._analyze_with_gemini(content)
        print("âœ… 1ì°¨ ë¶„ì„ ì™„ë£Œ")

        # ìºì‹œ ì €ì¥
        self._set_cache(url, result)
        return result, False

    def _analyze_with_gemini(self, content: str):
        if not gemini:
            raise Exception("Gemini APIë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        content = content[:8000]  # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì¡°ì ˆ

        prompt = f"""
        ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.

        í…ìŠ¤íŠ¸:
        ---
        {content}
        ---

        ìš”êµ¬ì‚¬í•­:
        1. key_claims: í…ìŠ¤íŠ¸ì—ì„œ ì–¸ê¸‰ëœ í•µì‹¬ì ì¸ ì£¼ì¥ì´ë‚˜ ì‚¬ì‹¤ (3-7ê°œ, ê°ê° ëª…í™•í•˜ê³  ê°„ê²°í•œ í•œ ë¬¸ì¥)
        2. related_countries: ì£¼ëœ ê´€ë ¨ êµ­ê°€ (ìµœëŒ€ 5ê°œ, êµ­ê°€ ì½”ë“œê°€ ì•„ë‹Œ ì´ë¦„ìœ¼ë¡œ)
        3. search_keywords: ê° ì£¼ì¥ì„ ê²€ì¦í•˜ê¸° ìœ„í•œ ì˜ì–´ ê²€ìƒ‰ í‚¤ì›Œë“œ (ì£¼ì¥ë‹¹ 2-3ê°œ)
        4. topics: ì£¼ì œ ë¶„ë¥˜ (ì˜ˆ: ì •ì¹˜, ê²½ì œ, ì‚¬íšŒ, ê³¼í•™ê¸°ìˆ , êµ­ì œê´€ê³„)
        5. summary: ì „ì²´ ë‚´ìš©ì„ ìš”ì•½ (2-3 ë¬¸ì¥)

        ì‘ë‹µ í˜•ì‹ (JSON):
        {{
          "key_claims": ["ì£¼ì¥ 1", "ì£¼ì¥ 2"],
          "related_countries": ["êµ­ê°€1", "êµ­ê°€2"],
          "search_keywords": [["keyword1", "keyword2"], ["keyword3", "keyword4"]],
          "topics": ["ì£¼ì œ1", "ì£¼ì œ2"],
          "summary": "ìš”ì•½ ë‚´ìš©"
        }}

        ì£¼ì˜: ë°˜ë“œì‹œ JSON ê°ì²´ë§Œ ì¶œë ¥í•˜ê³  ë‹¤ë¥¸ ì„¤ëª…ì€ ë§ë¶™ì´ì§€ ë§ˆì„¸ìš”.
        """

        try:
            response = gemini.generate_content(prompt)
            result_text = (
                response.text.strip().replace('```json', '').replace('```', '').strip()
            )
            return json.loads(result_text)
        except Exception as e:
            print(f"âŒ AI 1ì°¨ ë¶„ì„ ì‹¤íŒ¨: {e}")
            raise Exception(f"AI ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

    # --- 2ì°¨ ë¶„ì„ ---
    def find_sources_for_claims(
        self, url: str, input_type: str, selected_claims: list, search_keywords: list
    ):
        # ì›ë³¸ ì½˜í…ì¸  ë‹¤ì‹œ ì¶”ì¶œ
        extractor = self._get_extractor(input_type)
        original_content = extractor.extract(url)

        # ì‹¤ì œ ê¸°ì‚¬ ê²€ìƒ‰
        articles = self._search_real_articles(search_keywords)

        # AIë¡œ ê´€ë ¨ì„± ë¶„ì„
        print("ğŸ¤– Geminië¡œ 2ì°¨ ë¶„ì„ (ê´€ë ¨ ê¸°ì‚¬ ë§¤ì¹­) ì¤‘...")
        analysis_result = self._find_related_articles_with_gemini(
            original_content, selected_claims, articles
        )
        print("âœ… 2ì°¨ ë¶„ì„ ì™„ë£Œ")

        return analysis_result, articles

    def _search_real_articles(self, keywords: list):
        if not keywords:
            return []
        try:
            query = " OR ".join(keywords)
            print(f"ğŸ” Google ê²€ìƒ‰ ì‹¤í–‰: {query}")

            # ë‚´ì¥ ì›¹ ê²€ìƒ‰ ë„êµ¬ í˜¸ì¶œ
            from Gemini.google_web_search import google_web_search

            search_results = google_web_search(query=query)

            articles = []
            if search_results and 'results' in search_results:
                for result in search_results['results']:
                    source = result.get('source', 'ì¶œì²˜ ë¶ˆëª…')
                    credibility_info = get_media_credibility(source)

                    articles.append(
                        {
                            'title': result.get('title', 'ì œëª© ì—†ìŒ'),
                            'snippet': result.get('snippet', 'ë‚´ìš© ì—†ìŒ'),
                            'url': result.get('url', '#'),
                            'source': source,
                            'country': credibility_info.get('country', 'Unknown'),
                            'credibility': credibility_info.get('credibility', 50),
                            'bias': credibility_info.get('bias', 'ì•Œ ìˆ˜ ì—†ìŒ'),
                            'published_date': result.get('publishDate', 'ë‚ ì§œ ì—†ìŒ'),
                        }
                    )
            return articles
        except Exception as e:
            print(f"âš ï¸ ì‹¤ì œ ê¸°ì‚¬ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []

    def _find_related_articles_with_gemini(
        self, original_content: str, claims: list, articles: list
    ):
        if not gemini:
            raise Exception("Gemini APIë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        original_content = original_content[:4000]
        articles_text = "\n\n".join(
            [
                f"[ê¸°ì‚¬ {i+1}]\nì œëª©: {article.get('title', '')}\nì¶œì²˜: {article.get('source', '')}\në‚´ìš©: {article.get('snippet', '')}"
                for i, article in enumerate(articles[:15])  # ì»¨í…ìŠ¤íŠ¸ ì¡°ì ˆ
            ]
        )

        prompt = f"""
        ë‹¹ì‹ ì€ í¸ê²¬ ì—†ëŠ” ì •ë³´ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ì›ë³¸ ì½˜í…ì¸ ì˜ ì£¼ì¥ê³¼ ìˆ˜ì§‘ëœ ê¸°ì‚¬ë“¤ì˜ ì—°ê´€ì„±ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.
        **ì ˆëŒ€ë¡œ ì£¼ì¥ì˜ ì°¸/ê±°ì§“ì„ íŒë‹¨í•˜ì§€ ë§ˆì„¸ìš”.**

        [ì›ë³¸ ì½˜í…ì¸  ìš”ì•½]
        {original_content}

        [ì‚¬ìš©ìê°€ ì„ íƒí•œ ê²€ì¦ ëŒ€ìƒ ì£¼ì¥]
        {chr(10).join([f'- {c}' for c in claims])}

        [ìˆ˜ì§‘ëœ ìµœì‹  ê¸°ì‚¬ ëª©ë¡]
        {articles_text}

        [ìš”ì²­ ì‘ì—…]
        ê° ì£¼ì¥ì— ëŒ€í•´, ìˆ˜ì§‘ëœ ê¸°ì‚¬ ëª©ë¡ ì¤‘ ì–´ë–¤ ê¸°ì‚¬ê°€ ê°€ì¥ ê´€ë ¨ ìˆëŠ”ì§€ ë¶„ì„í•˜ê³ , ê° ê¸°ì‚¬ê°€ ì–´ë–¤ ìƒˆë¡œìš´ ì •ë³´ë‚˜ ë‹¤ë¥¸ ê´€ì ì„ ì œê³µí•˜ëŠ”ì§€ ì„¤ëª…í•´ì£¼ì„¸ìš”.

        [ì‘ë‹µ í˜•ì‹ (JSON)]
        {{
          "results": [
            {{
              "claim": "ì‚¬ìš©ìê°€ ì„ íƒí•œ ì²« ë²ˆì§¸ ì£¼ì¥",
              "related_articles": [
                  {{
                      "article_index": 1, // ê¸°ì‚¬ ë²ˆí˜¸
                      "relevance_score": 90, // 0-100ì 
                      "perspective": "ì´ ê¸°ì‚¬ëŠ” ì£¼ì¥ì— ëŒ€í•´ êµ¬ì²´ì ì¸ í†µê³„ë¥¼ ì œê³µí•˜ë©°, ë‹¤ë¥¸ ì›ì¸ì„ ì œì‹œí•©ë‹ˆë‹¤."
                  }}
              ],
              "missing_context": "ì›ë³¸ ì½˜í…ì¸ ì—ëŠ” ì—†ì§€ë§Œ, ì´ ì£¼ì¥ì„ ì´í•´í•˜ëŠ” ë° í•„ìš”í•œ ì¶”ê°€ì ì¸ ë°°ê²½ì§€ì‹ì´ë‚˜ ë§¥ë½ì„ ì„œìˆ í•©ë‹ˆë‹¤.",
              "coverage_countries": ["ë¯¸êµ­", "ì˜êµ­"] // ì´ ì£¼ì¥ì„ ì£¼ë¡œ ë‹¤ë£¬ êµ­ê°€
            }}
          ]
        }}

        [ì£¼ì˜ì‚¬í•­]
        - `related_articles`ì—ëŠ” ê´€ë ¨ì„± ë†’ì€ ê¸°ì‚¬ë§Œ í¬í•¨ì‹œí‚¤ì„¸ìš”.
        - `perspective`ëŠ” ê¸°ì‚¬ì˜ ê°ê´€ì ì¸ ë‚´ìš©ì„ ìš”ì•½í•´ì•¼ í•©ë‹ˆë‹¤. (ì˜ˆ: ì§€ì§€/ë°˜ëŒ€/ì¤‘ë¦½ì´ ì•„ë‹Œ ì •ë³´ ìš”ì•½)
        - `missing_context`ëŠ” ëª¨ë“  ê¸°ì‚¬ë¥¼ ì¢…í•©í•˜ì—¬ ì¶”ë¡ í•œ ë‚´ìš©ì„ ë‹´ìŠµë‹ˆë‹¤.
        - ë°˜ë“œì‹œ JSON ê°ì²´ë§Œ ì¶œë ¥í•˜ê³  ë‹¤ë¥¸ ì„¤ëª…ì€ ë§ë¶™ì´ì§€ ë§ˆì„¸ìš”.
        """
        try:
            response = gemini.generate_content(prompt)
            result_text = (
                response.text.strip().replace('```json', '').replace('```', '').strip()
            )
            return json.loads(result_text)
        except Exception as e:
            print(f"âŒ AI 2ì°¨ ë¶„ì„ ì‹¤íŒ¨: {e}")
            raise Exception(f"ê´€ë ¨ ê¸°ì‚¬ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

    # --- ìºì‹± í—¬í¼ ---
    def _get_cache(self, url: str):
        if not db:
            return None
        try:
            cache_key = hashlib.md5(url.encode()).hexdigest()
            doc = db.collection('cache').document(cache_key).get()
            if doc.exists:
                print(f"âœ… ìºì‹œ íˆíŠ¸: {url[:50]}...")
                return doc.to_dict().get('result')
            return None
        except Exception as e:
            print(f"âš ï¸ ìºì‹œ ì½ê¸° ì‹¤íŒ¨: {e}")
            return None

    def _set_cache(self, url: str, result):
        if not db:
            return
        try:
            cache_key = hashlib.md5(url.encode()).hexdigest()
            db.collection('cache').document(cache_key).set(
                {'url': url, 'result': result, 'cached_at': datetime.now()}
            )
            print(f"âœ… ìºì‹œ ì €ì¥: {url[:50]}...")
        except Exception as e:
            print(f"âš ï¸ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
