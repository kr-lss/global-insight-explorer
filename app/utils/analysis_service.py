"""
ë¶„ì„ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ì„ ì²˜ë¦¬í•˜ëŠ” ì„œë¹„ìŠ¤ (Global Insight Explorer v2)
- í•œêµ­ì–´ ì‚¬ìš©ì ìµœì í™” (ì…ë ¥/ì¶œë ¥: í•œêµ­ì–´, ë‚´ë¶€ê²€ìƒ‰: ì˜ì–´)
- Google Search Grounding ì˜¤ë¥˜ ìˆ˜ì • ë° ìµœì í™”
"""
import os
import json
import hashlib
from datetime import datetime

import vertexai
from vertexai.generative_models import GenerativeModel, Tool, GoogleSearchRetrieval
from google.cloud import firestore
from google.api_core.exceptions import GoogleAPICallError

from app.models.extractor import BaseExtractor, YoutubeExtractor, ArticleExtractor
from app.models.media import get_media_credibility
from app.config import config

# --- ì´ˆê¸°í™” ---
gemini = None
try:
    vertexai.init(project=config.GCP_PROJECT, location=config.GCP_REGION)
    # 1ì°¨/2ì°¨ ë¶„ì„ìš© ì¼ë°˜ ëª¨ë¸ (ë¹„ìš© íš¨ìœ¨ì ì¸ Flash ëª¨ë¸ ê¶Œì¥)
    gemini = GenerativeModel('gemini-2.0-flash') 
    print("âœ… (Service) Gemini API ì—°ê²° ì„±ê³µ")
except Exception as e:
    print(f"âš ï¸ (Service) Gemini API ì—°ê²° ì‹¤íŒ¨: {e}")

db = None
try:
    db = firestore.Client(project=config.GCP_PROJECT)
    print("âœ… (Service) Firestore ì—°ê²° ì„±ê³µ")
except Exception as e:
    print(f"âš ï¸ (Service) Firestore ì—°ê²° ì‹¤íŒ¨: {e}")


class AnalysisService:
    def __init__(self):
        self.extractors = {
            'youtube': YoutubeExtractor(),
            'article': ArticleExtractor(),
        }

    def _get_extractor(self, input_type: str) -> BaseExtractor:
        extractor = self.extractors.get(input_type)
        if not extractor:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì…ë ¥ íƒ€ì…: {input_type}")
        return extractor

    # ==================================================================
    # 1ï¸âƒ£ 1ì°¨ ë¶„ì„ (Initial Analysis) - í•œêµ­ì–´ ì‚¬ìš©ì ìµœì í™”
    # ==================================================================
    def analyze_content(self, url: str, input_type: str):
        # ìºì‹œ í™•ì¸
        cached = self._get_cache(url)
        if cached:
            return cached, True

        # ì½˜í…ì¸  ì¶”ì¶œ
        print(f"ğŸ“¥ ì½˜í…ì¸  ì¶”ì¶œ ì¤‘: {url[:50]}...")
        extractor = self._get_extractor(input_type)
        content = extractor.extract(url)
        
        if not content or len(content) < 50:
            raise Exception("ì½˜í…ì¸ ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ê±°ë‚˜ ë‚´ìš©ì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤.")
            
        print(f"âœ… ì¶”ì¶œ ì™„ë£Œ: {len(content)} ê¸€ì")

        # AI ë¶„ì„
        print("ğŸ¤– Geminië¡œ 1ì°¨ ë¶„ì„ ì¤‘ (í•œê¸€ ìš”ì•½ + ì˜ì–´ ê²€ìƒ‰ì–´ ìƒì„±)...")
        result = self._analyze_with_gemini_bridge(content)
        print("âœ… 1ì°¨ ë¶„ì„ ì™„ë£Œ")

        # ìºì‹œ ì €ì¥
        self._set_cache(url, result)
        return result, False

    def _analyze_with_gemini_bridge(self, content: str):
        """
        Gemini í”„ë¡¬í”„íŠ¸: í•œêµ­ì–´ ì…ë ¥ì„ ë°›ì•„ 'í•œêµ­ì–´ ìš”ì•½'ê³¼ 'ì˜ì–´ ê²€ìƒ‰ì–´'ë¥¼ ë™ì‹œ ìƒì„±
        """
        if not gemini:
            raise Exception("Gemini APIë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        content = content[:config.MAX_CONTENT_LENGTH_FIRST_ANALYSIS]

        prompt = f"""
        ë‹¹ì‹ ì€ êµ­ì œ ì •ì„¸ ë° ë¯¸ë””ì–´ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
        ì£¼ì–´ì§„ í…ìŠ¤íŠ¸(ì˜ìƒ ìë§‰ ë˜ëŠ” ê¸°ì‚¬)ë¥¼ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ì •ë³´ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œí•˜ì„¸ìš”.

        [ë¶„ì„ ëŒ€ìƒ í…ìŠ¤íŠ¸]
        {content}

        [í•„ìˆ˜ ìš”êµ¬ì‚¬í•­]
        1. **ì‚¬ìš©ìëŠ” í•œêµ­ì¸ì…ë‹ˆë‹¤.** ëª¨ë“  ì„¤ëª…ê³¼ ìš”ì•½ì€ ìì—°ìŠ¤ëŸ¬ìš´ **í•œêµ­ì–´**ë¡œ ì‘ì„±í•˜ì„¸ìš”.
        2. **ê²€ìƒ‰ í‚¤ì›Œë“œ(search_keywords_en)**ëŠ” ë°˜ë“œì‹œ **ì˜ì–´(English)**ë¡œ ì‘ì„±í•˜ì„¸ìš”. 
           (ì´ìœ : ì „ ì„¸ê³„ ë‰´ìŠ¤(GDELT, Google) ê²€ìƒ‰ ì •í™•ë„ë¥¼ ë†’ì´ê¸° ìœ„í•¨)
        3. **ê´€ë ¨ êµ­ê°€(target_country_codes)**ëŠ” í•´ë‹¹ ì´ìŠˆì™€ ì´í•´ê´€ê³„ê°€ ìˆëŠ” êµ­ê°€ë“¤ì˜ **2ìë¦¬ ISO ì½”ë“œ**ë¡œ ì‘ì„±í•˜ì„¸ìš”.
           (ì˜ˆ: í•œêµ­='KR', ë¯¸êµ­='US', ì¤‘êµ­='CN', ë¶í•œ='KP', ëŸ¬ì‹œì•„='RU', ì¼ë³¸='JP')

        [ì¶œë ¥ í˜•ì‹ (JSON Only)]
        {{
          "title_kr": "ì½˜í…ì¸  ì œëª© ë˜ëŠ” ì£¼ì œ (í•œêµ­ì–´)",
          "summary_kr": "ì „ì²´ ë‚´ìš© ìš”ì•½ (í•œêµ­ì–´ 3ë¬¸ì¥ ë‚´ì™¸)",
          "topics": ["ì£¼ì œ1", "ì£¼ì œ2"], 
          "key_claims": [
            {{
              "claim_kr": "í•µì‹¬ ì£¼ì¥ 1 (í•œêµ­ì–´)",
              "search_keywords_en": ["keyword1", "keyword2", "specific term"],
              "target_country_codes": ["CN", "US"] // ì´ ì£¼ì¥ì— ëŒ€í•´ ì…ì¥ì„ í™•ì¸í•´ë´ì•¼ í•  êµ­ê°€ë“¤
            }},
            {{
              "claim_kr": "í•µì‹¬ ì£¼ì¥ 2 (í•œêµ­ì–´)",
              "search_keywords_en": ["keyword3", "keyword4"],
              "target_country_codes": ["RU", "UA"]
            }}
          ]
        }}

        JSON ì™¸ì— ë‹¤ë¥¸ ë§ì€ í•˜ì§€ ë§ˆì„¸ìš”.
        """

        try:
            response = gemini.generate_content(prompt)
            result_text = response.text.strip().replace('```json', '').replace('```', '').strip()
            return json.loads(result_text)
        except Exception as e:
            print(f"âŒ AI 1ì°¨ ë¶„ì„ ì‹¤íŒ¨: {e}")
            raise Exception(f"AI ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

    # ==================================================================
    # 2ï¸âƒ£ 2ì°¨ ë¶„ì„ (Find Sources) - ì‚¬ìš©ì ì œì•ˆ ì™„ë²½ ë°˜ì˜ + ì»¤ìŠ¤í…€ ê¸°ëŠ¥
    # ==================================================================
    def find_sources_for_claims(
        self, url: str, input_type: str, claims_data: list
    ):
        """
        ì„ íƒëœ ì£¼ì¥ì— ëŒ€í•œ êµì°¨ ê²€ì¦ (Google Search + GDELT ì˜ˆì •)

        Args:
            url: ì›ë³¸ ì½˜í…ì¸  URL
            input_type: ì½˜í…ì¸  íƒ€ì… (youtube/article)
            claims_data: ì£¼ì¥ ì •ë³´ ë¦¬ìŠ¤íŠ¸
                [
                    {
                        "claim_kr": "í•œêµ­ì–´ ì£¼ì¥",
                        "search_keywords_en": ["keyword1", "keyword2"],
                        "target_country_codes": ["US", "CN"]
                    },
                    ...
                ]
        """
        # ì›ë³¸ ì½˜í…ì¸  ë‹¤ì‹œ ì¶”ì¶œ (ì»¨í…ìŠ¤íŠ¸ìš©)
        extractor = self._get_extractor(input_type)
        original_content = extractor.extract(url)

        all_articles = []

        # ê° ì£¼ì¥ë³„ë¡œ ë…ë¦½ì ì¸ ê²€ìƒ‰ ìˆ˜í–‰
        for claim_data in claims_data:
            claim_kr = claim_data.get('claim_kr', '')
            search_keywords = claim_data.get('search_keywords_en', [])
            target_countries = claim_data.get('target_country_codes', [])

            # [ì¶”ê°€ ê¸°ëŠ¥] í‚¤ì›Œë“œê°€ ë¹„ì–´ìˆë‹¤ë©´ (ì˜ˆ: ì‚¬ìš©ì ì§ì ‘ ì…ë ¥), ì¦‰ì„ ìƒì„±
            if not search_keywords and claim_kr:
                print(f"ğŸ¤– ì‚¬ìš©ì ì…ë ¥('{claim_kr}')ì— ëŒ€í•œ í‚¤ì›Œë“œ ìƒì„± ì¤‘...")
                generated_info = self._generate_keywords_on_the_fly(claim_kr)
                search_keywords = generated_info.get('keywords', [claim_kr])
                # ë§Œì•½ íƒ€ê²Ÿ êµ­ê°€ë„ ì—†ë‹¤ë©´ ìƒì„±ëœ ê²ƒ ì‚¬ìš©, ì•„ë‹ˆë©´ ìœ ì§€
                if not target_countries:
                    target_countries = generated_info.get('countries', [])

            # 1. ê¸°ì‚¬ ê²€ìƒ‰ (ì˜ì–´ í‚¤ì›Œë“œ + íƒ€ê²Ÿ êµ­ê°€ ì •ë³´ í™œìš©)
            if search_keywords:
                print(f"ğŸ” '{claim_kr[:15]}...' ê²€ìƒ‰ ì‹œì‘ (í‚¤ì›Œë“œ: {search_keywords}, êµ­ê°€: {target_countries})")
                articles = self._search_real_articles(search_keywords, target_countries)
                all_articles.extend(articles)

        # ì¤‘ë³µ ì œê±° (URL ê¸°ì¤€)
        unique_articles = {v['url']: v for v in all_articles}.values()
        final_articles = list(unique_articles)

        # 2. AI ê²€ì¦ (í•œêµ­ì–´ë¡œ ê²°ê³¼ ë¦¬í¬íŠ¸)
        print("ğŸ¤– Geminië¡œ 2ì°¨ ë¶„ì„ (íŒ©íŠ¸ì²´í¬ & ê´€ì  ë¹„êµ) ì¤‘...")

        # claims_dataì—ì„œ claim_krë§Œ ì¶”ì¶œí•˜ì—¬ AIì—ê²Œ ì „ë‹¬
        selected_claim_texts = [c['claim_kr'] for c in claims_data]

        analysis_result = self._compare_perspectives_with_gemini(
            original_content, selected_claim_texts, final_articles
        )
        print("âœ… 2ì°¨ ë¶„ì„ ì™„ë£Œ")

        return analysis_result, final_articles

    def _generate_keywords_on_the_fly(self, claim_kr: str):
        """ì‚¬ìš©ì ì…ë ¥ ì£¼ì¥ì„ ìœ„í•œ ì˜ì–´ í‚¤ì›Œë“œ ë° íƒ€ê²Ÿ êµ­ê°€ ìƒì„±"""
        if not gemini:
            return {"keywords": [claim_kr], "countries": []}

        try:
            prompt = f"""
            Translate this Korean claim into 2-3 English search keywords for news verification.
            Also suggest 2 relevant country codes (ISO 3166-1 alpha-2).
            Claim: "{claim_kr}"
            Output JSON: {{"keywords": ["kw1", "kw2"], "countries": ["US", "KR"]}}
            """
            response = gemini.generate_content(prompt)
            text = response.text.strip().replace('```json', '').replace('```', '').strip()
            return json.loads(text)
        except Exception as e:
            print(f"âš ï¸ í‚¤ì›Œë“œ ìƒì„± ì‹¤íŒ¨: {e}")
            return {"keywords": [claim_kr], "countries": []}

    def _search_real_articles(self, keywords: list, target_countries: list = None):
        """
        Gemini Google Search Grounding (ìµœì‹  SDK ë¬¸ë²• ì ìš©)

        Args:
            keywords: ì˜ì–´ ê²€ìƒ‰ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
            target_countries: íƒ€ê²Ÿ êµ­ê°€ ì½”ë“œ ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: ["US", "CN"])
        """
        if not keywords:
            return []

        # í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ í‰íƒ„í™”
        flat_keywords = []
        for k in keywords:
            if isinstance(k, list):
                flat_keywords.extend(k)
            else:
                flat_keywords.append(k)

        # ê¸°ë³¸ ì¿¼ë¦¬
        base_query = " ".join(flat_keywords[:7])  # ë„ˆë¬´ ê¸¸ë©´ ì˜ë¦¼, ìµœëŒ€ 7ë‹¨ì–´ ê¶Œì¥
        query = base_query

        # [ìˆ˜ì •] íƒ€ê²Ÿ êµ­ê°€ê°€ ìˆìœ¼ë©´ ì¿¼ë¦¬ì— ì¶”ê°€í•˜ì—¬ ê²€ìƒ‰ ì •í™•ë„ í–¥ìƒ
        if target_countries and len(target_countries) > 0:
            # ì˜ˆ: "North Korea Missile (US OR CN OR KR)"
            country_query = " OR ".join(target_countries)
            query = f"{base_query} ({country_query})"

        print(f"ğŸ” Google Search Query: {query}")

        try:
            # âœ… [ìˆ˜ì •ë¨] ìµœì‹  Vertex AI SDK ë°©ì‹
            search_tool = Tool.from_google_search_retrieval(
                GoogleSearchRetrieval()
            )
            
            # ê²€ìƒ‰ìš© ëª¨ë¸ ë³„ë„ ì´ˆê¸°í™” (Grounding ë„êµ¬ í¬í•¨)
            model = GenerativeModel(
                'gemini-2.0-flash',
                tools=[search_tool]
            )
            
            # Groundingì„ ê°•ì œí•˜ê¸° ìœ„í•œ í”„ë¡¬í”„íŠ¸
            prompt = f"Search for latest news articles about: {query}. Provide details."
            
            response = model.generate_content(prompt)
            
            # TODO: Grounding Metadataì—ì„œ ì‹¤ì œ URL ì¶”ì¶œ ë¡œì§ì„ ê°œì„ í•´ì•¼ í•¨.
            # í˜„ì¬ëŠ” Grounding APIì˜ íŠ¹ì„±ìƒ í…ìŠ¤íŠ¸ ìƒì„±ì— ì§‘ì¤‘ë˜ì–´ ìˆìœ¼ë¯€ë¡œ,
            # ì •í™•í•œ URL ë¦¬ìŠ¤íŠ¸ê°€ í•„ìš”í•˜ë©´ Custom Search JSON APIë¥¼ ë³‘í–‰í•˜ëŠ” ê²ƒì´ ì¢‹ìŒ.
            # ì¼ë‹¨ì€ êµ¬ì¡° ìœ ì§€ë¥¼ ìœ„í•´ ìƒ˜í”Œ ë°ì´í„°(Fallback) ë˜ëŠ” Geminiê°€ ìƒì„±í•œ í…ìŠ¤íŠ¸ ë‚´ ì •ë³´ë¥¼ í™œìš©
            
            # TODO: Grounding ì‘ë‹µ íŒŒì‹± ë¡œì§ ê°œì„  í•„ìš” (í˜„ì¬ëŠ” Fallback ì‚¬ìš©)
            # ì‹¤ì œë¡œëŠ” response.candidates[0].grounding_metadata.search_entry_point ë“±ì„ íŒŒì‹±í•´ì•¼ í•¨

            # ì„ì‹œ: ê²€ìƒ‰ì€ ì„±ê³µí–ˆì§€ë§Œ URLì„ êµ¬ì¡°ì ìœ¼ë¡œ ëª» ê°€ì ¸ì˜¬ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ìƒ˜í”Œ ë°˜í™˜
            # (ì‹¤ì œ í”„ë¡œë•ì…˜ì—ì„œëŠ” Custom Search APIê°€ ë” ì í•©)
            print("âš ï¸ Google Search Grounding ì™„ë£Œ (URL ì¶”ì¶œ ë¡œì§ ë³´ì™„ í•„ìš”)")
            return self._get_sample_articles(flat_keywords, target_countries)

        except Exception as e:
            print(f"âš ï¸ ê²€ìƒ‰ ì‹¤íŒ¨ ({e}). ìƒ˜í”Œ ë°ì´í„° ì‚¬ìš©.")
            return self._get_sample_articles(flat_keywords, target_countries)

    def _compare_perspectives_with_gemini(
        self, original_content: str, claims: list, articles: list
    ):
        if not gemini:
            raise Exception("Gemini APIë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        original_content = original_content[:config.MAX_CONTENT_LENGTH_SECOND_ANALYSIS]
        
        # ê¸°ì‚¬ ëª©ë¡ í…ìŠ¤íŠ¸í™”
        articles_text = "\n".join([
            f"- [{a['source']} ({a['country']})] {a['title']}: {a['snippet']}"
            for a in articles
        ])

        prompt = f"""
        ë‹¹ì‹ ì€ ê°ê´€ì ì¸ íŒ©íŠ¸ì²´ì»¤ì…ë‹ˆë‹¤. ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”.
        **ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.**

        [ê²€ì¦ ëŒ€ìƒ ì£¼ì¥ (ì‚¬ìš©ì ì„ íƒ)]
        {chr(10).join([f'- {c}' for c in claims])}

        [ìˆ˜ì§‘ëœ ê´€ë ¨ ê¸°ì‚¬/ìë£Œ]
        {articles_text}

        [ì§€ì‹œì‚¬í•­]
        1. ê° ì£¼ì¥ì— ëŒ€í•´ ìˆ˜ì§‘ëœ ê¸°ì‚¬ë“¤ì´ **ì§€ì§€(Supporting)**í•˜ëŠ”ì§€, **ë°˜ë°•(Opposing)**í•˜ëŠ”ì§€, ë˜ëŠ” **ì¤‘ë¦½/ê´€ë ¨ì—†ìŒ**ì¸ì§€ ë¶„ì„í•˜ì„¸ìš”.
        2. êµ­ê°€ë³„ ì–¸ë¡ ì˜ ì‹œê° ì°¨ì´ê°€ ìˆë‹¤ë©´ ì§€ì í•´ì£¼ì„¸ìš” (ì˜ˆ: ë¯¸êµ­ ì–¸ë¡ ì€ Aë¼ í•˜ì§€ë§Œ, ì¤‘êµ­ ì–¸ë¡ ì€ Bë¼ í•¨).
        3. ìµœì¢…ì ìœ¼ë¡œ ì´ ì£¼ì¥ì˜ ì‹ ë¢°ë„ë¥¼ 'ë†’ìŒ/ì¤‘ê°„/ë‚®ìŒ/íŒë‹¨ë¶ˆê°€'ë¡œ í‰ê°€í•˜ì„¸ìš”.

        [ì‘ë‹µ í˜•ì‹ (JSON)]
        {{
          "results": [
            {{
              "claim": "ì£¼ì¥ ë‚´ìš©",
              "verdict": "ëŒ€ì²´ë¡œ ì‚¬ì‹¤ / ë…¼ë€ ìˆìŒ / ê±°ì§“",
              "analysis_kr": "ë¶„ì„ ë‚´ìš© (í•œêµ­ì–´ ìƒì„¸ ì„¤ëª…)",
              "perspectives": [
                 {{"country": "US", "stance": "Supporting", "media": "CNN"}},
                 {{"country": "CN", "stance": "Opposing", "media": "Global Times"}}
              ]
            }}
          ]
        }}
        """
        
        try:
            response = gemini.generate_content(prompt)
            result_text = response.text.strip().replace('```json', '').replace('```', '').strip()
            return json.loads(result_text)
        except Exception as e:
            print(f"âŒ AI 2ì°¨ ë¶„ì„ ì‹¤íŒ¨: {e}")
            # ë¹ˆ ê²°ê³¼ ë°˜í™˜í•˜ì—¬ í”„ë¡ íŠ¸ì—”ë“œ ì—ëŸ¬ ë°©ì§€
            return {"results": []}

    # --- ìºì‹œ ë° ìœ í‹¸ë¦¬í‹° ---
    def _get_cache(self, url: str):
        if not db: return None
        try:
            cache_key = hashlib.md5(url.encode()).hexdigest()
            doc = db.collection('cache').document(cache_key).get()
            if doc.exists:
                print(f"âœ… ìºì‹œ íˆíŠ¸: {url[:30]}...")
                return doc.to_dict().get('result')
        except: pass
        return None

    def _set_cache(self, url: str, result):
        if not db: return
        try:
            cache_key = hashlib.md5(url.encode()).hexdigest()
            db.collection('cache').document(cache_key).set({
                'url': url, 'result': result, 'cached_at': datetime.now()
            })
        except: pass

    def _get_sample_articles(self, keywords: list, target_countries: list = None):
        """ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ë°ì´í„°"""
        k = keywords[0] if keywords else "ì´ìŠˆ"

        # íƒ€ê²Ÿ êµ­ê°€ì— ë§ëŠ” ìƒ˜í”Œ ë°ì´í„° ìƒì„±
        sample_sources = []
        if target_countries and len(target_countries) > 0:
            # íƒ€ê²Ÿ êµ­ê°€ë³„ ëŒ€í‘œ ì–¸ë¡ ì‚¬ ë§¤í•‘
            country_media = {
                'US': {'source': 'CNN', 'credibility': 80},
                'UK': {'source': 'BBC', 'credibility': 85},
                'CN': {'source': 'Xinhua', 'credibility': 60},
                'RU': {'source': 'RT', 'credibility': 55},
                'JP': {'source': 'NHK', 'credibility': 75},
                'KR': {'source': 'Yonhap', 'credibility': 75},
                'FR': {'source': 'France 24', 'credibility': 80},
                'DE': {'source': 'DW', 'credibility': 80},
            }
            for country in target_countries[:3]:  # ìµœëŒ€ 3ê°œêµ­
                media = country_media.get(country, {'source': f'{country} News', 'credibility': 70})
                sample_sources.append({
                    'title': f'{media["source"]}: {k} coverage',
                    'snippet': f'{country} perspective on {k}...',
                    'url': '#',
                    'source': media['source'],
                    'country': country,
                    'credibility': media['credibility']
                })
        else:
            # ê¸°ë³¸ ìƒ˜í”Œ (íƒ€ê²Ÿ êµ­ê°€ ì—†ì„ ë•Œ)
            sample_sources = [
                {'title': f'Global view on {k}', 'snippet': 'Western media perspective...', 'url': '#', 'source': 'CNN', 'country': 'US', 'credibility': 80},
                {'title': f'Alternative view on {k}', 'snippet': 'Eastern media perspective...', 'url': '#', 'source': 'Xinhua', 'country': 'CN', 'credibility': 60},
            ]

        return sample_sources