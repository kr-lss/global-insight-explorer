"""
ë¶„ì„ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ì„ ì²˜ë¦¬í•˜ëŠ” ì„œë¹„ìŠ¤ (Facade íŒ¨í„´)
"""
import json
import hashlib
from datetime import datetime
from typing import Dict, List, Any

import vertexai
from vertexai.preview.generative_models import GenerativeModel
from google.cloud import firestore

from app.models.extractor import BaseExtractor, YoutubeExtractor, ArticleExtractor
from app.models.media import get_media_credibility
from app.config import config
from app.prompts import (
    get_first_analysis_prompt,
    get_stance_analysis_prompt,
    get_article_search_prompt,
)

# Gemini ëª¨ë¸ ë¡œë“œ
gemini = None
try:
    vertexai.init(project=config.GCP_PROJECT, location=config.GCP_REGION)
    gemini = GenerativeModel(config.GEMINI_MODEL_ANALYSIS)
    print("âœ… (Service) Gemini API ì—°ê²° ì„±ê³µ")
except Exception as e:
    print(f"âš ï¸ (Service) Gemini API ì—°ê²° ì‹¤íŒ¨: {e}")

# Firestore í´ë¼ì´ì–¸íŠ¸
db = None
try:
    db = firestore.Client(project=config.GCP_PROJECT)
    print("âœ… (Service) Firestore ì—°ê²° ì„±ê³µ")
except Exception as e:
    print(f"âš ï¸ (Service) Firestore ì—°ê²° ì‹¤íŒ¨: {e}")


class AnalysisService:
    def __init__(self):
        self.extractors = {
            'youtube': YoutubeExtractor(),
            'article': ArticleExtractor(),
        }

    def _get_extractor(self, input_type: str) -> BaseExtractor:
        extractor = self.extractors.get(input_type)
        if not extractor:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì…ë ¥ íƒ€ì…: {input_type}")
        return extractor

    # --- 1ì°¨ ë¶„ì„ ---
    def analyze_content(self, url: str, input_type: str):
        # ìºì‹œ í™•ì¸
        cached = self._get_cache(url)
        if cached:
            return cached, True

        # ì½˜í…ì¸  ì¶”ì¶œ
        print(f"ğŸ“¥ ì½˜í…ì¸  ì¶”ì¶œ ì¤‘: {url[:50]}...")
        extractor = self._get_extractor(input_type)
        content = extractor.extract(url)
        print(f"âœ… ì¶”ì¶œ ì™„ë£Œ: {len(content)} ê¸€ì")

        # AI ë¶„ì„
        print("ğŸ¤– Geminië¡œ 1ì°¨ ë¶„ì„ ì¤‘...")
        result = self._analyze_with_gemini(content)
        print("âœ… 1ì°¨ ë¶„ì„ ì™„ë£Œ")

        # ìºì‹œ ì €ì¥
        self._set_cache(url, result)
        return result, False

    def _analyze_with_gemini(self, content: str) -> Dict[str, Any]:
        """1ì°¨ ë¶„ì„: í•µì‹¬ ì£¼ì¥ ì¶”ì¶œ"""
        if not gemini:
            raise Exception("Gemini APIë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì¡°ì ˆ
        truncated_content = content[:config.MAX_CONTENT_LENGTH_FIRST_ANALYSIS]
        prompt = get_first_analysis_prompt(truncated_content)

        try:
            response = gemini.generate_content(prompt)
            return self._parse_json_response(response.text)
        except Exception as e:
            print(f"âŒ AI 1ì°¨ ë¶„ì„ ì‹¤íŒ¨: {e}")
            raise Exception(f"AI ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

    # --- 2ì°¨ ë¶„ì„ ---
    def find_sources_for_claims(
        self, url: str, input_type: str, selected_claims: list, search_keywords: list
    ):
        # ì›ë³¸ ì½˜í…ì¸  ë‹¤ì‹œ ì¶”ì¶œ
        extractor = self._get_extractor(input_type)
        original_content = extractor.extract(url)

        # ì‹¤ì œ ê¸°ì‚¬ ê²€ìƒ‰
        articles = self._search_real_articles(search_keywords)

        # AIë¡œ ê´€ë ¨ì„± ë¶„ì„
        print("ğŸ¤– Geminië¡œ 2ì°¨ ë¶„ì„ (ê´€ë ¨ ê¸°ì‚¬ ë§¤ì¹­) ì¤‘...")
        analysis_result = self._find_related_articles_with_gemini(
            original_content, selected_claims, articles
        )
        print("âœ… 2ì°¨ ë¶„ì„ ì™„ë£Œ")

        return analysis_result, articles

    def _search_real_articles(self, keywords: List[str]) -> List[Dict[str, Any]]:
        """Gemini Google Search Groundingì„ ì‚¬ìš©í•œ ì‹¤ì œ ê¸°ì‚¬ ê²€ìƒ‰"""
        if not keywords:
            return []

        try:
            from vertexai.preview.generative_models import GenerativeModel, Tool, grounding

            query = " ".join(keywords[:5])  # ìµœëŒ€ 5ê°œ í‚¤ì›Œë“œ ê²°í•©
            print(f"ğŸ” Gemini Google Searchë¡œ ê²€ìƒ‰ ì¤‘: {query}")

            # Google Search Grounding ë„êµ¬ ì„¤ì •
            search_tool = Tool.from_google_search_retrieval(
                grounding.GoogleSearchRetrieval()
            )

            # Gemini ëª¨ë¸ì— ê²€ìƒ‰ ë„êµ¬ ì¶”ê°€
            search_model = GenerativeModel(
                config.GEMINI_MODEL_SEARCH, tools=[search_tool]
            )

            # ê²€ìƒ‰ ì¿¼ë¦¬ ì‹¤í–‰
            search_prompt = get_article_search_prompt(query)
            response = search_model.generate_content(search_prompt)

            # JSON íŒŒì‹±
            search_data = self._parse_json_response(response.text)
            articles = self._process_search_results(
                search_data.get('articles', [])
            )

            print(f"âœ… {len(articles)}ê°œ ê¸°ì‚¬ ê²€ìƒ‰ ì™„ë£Œ")
            return articles

        except Exception as e:
            print(f"âš ï¸ ê¸°ì‚¬ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []  # ë¹ˆ ë°°ì—´ ë°˜í™˜ (ìƒ˜í”Œ ë°ì´í„° ëŒ€ì‹ )

    def _process_search_results(
        self, raw_articles: List[Dict]
    ) -> List[Dict[str, Any]]:
        """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì²˜ë¦¬í•˜ê³  ì‹ ë¢°ë„ ì •ë³´ ì¶”ê°€"""
        articles = []
        max_articles = config.MAX_ARTICLES_PER_SEARCH

        for result in raw_articles[:max_articles]:
            source = result.get('source', 'ì¶œì²˜ ë¶ˆëª…')
            credibility_info = get_media_credibility(source)

            articles.append({
                'title': result.get('title', 'ì œëª© ì—†ìŒ'),
                'snippet': result.get('snippet', 'ë‚´ìš© ì—†ìŒ'),
                'url': result.get('url', '#'),
                'source': source,
                'country': credibility_info.get('country', 'Unknown'),
                'credibility': credibility_info.get('credibility', 50),
                'bias': credibility_info.get('bias', 'ì•Œ ìˆ˜ ì—†ìŒ'),
                'published_date': result.get('published_date', 'ë‚ ì§œ ì—†ìŒ'),
            })

        return articles

    def _find_related_articles_with_gemini(
        self, original_content: str, claims: List[str], articles: List[Dict]
    ) -> Dict[str, Any]:
        """ì…ì¥ ê¸°ë°˜ ë¶„ì„ - êµ­ë‚´/êµ­ì œ ì´ìŠˆ ëª¨ë‘ ì ìš© ê°€ëŠ¥"""
        if not gemini:
            raise Exception("Gemini APIë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì¡°ì ˆ
        truncated_content = original_content[:config.MAX_CONTENT_LENGTH_SECOND_ANALYSIS]
        articles_text = self._format_articles_for_ai(
            articles[:config.MAX_ARTICLES_FOR_AI_ANALYSIS]
        )

        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = get_stance_analysis_prompt(truncated_content, claims, articles_text)

        try:
            response = gemini.generate_content(prompt)
            parsed_result = self._parse_json_response(response.text)

            # ìœ íš¨ì„± ê²€ì¦
            self._validate_stance_analysis_result(parsed_result)

            # ê²°ê³¼ë¥¼ ì‚¬ìš©ì ì¹œí™”ì ìœ¼ë¡œ ì¬êµ¬ì¡°í™”
            return self._restructure_by_stance(parsed_result, articles)

        except json.JSONDecodeError as e:
            print(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
            raise Exception(f"AI ì‘ë‹µì„ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
        except Exception as e:
            print(f"âŒ AI 2ì°¨ ë¶„ì„ ì‹¤íŒ¨: {e}")
            raise Exception(f"ì…ì¥ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

    def _restructure_by_stance(
        self, analysis_result: Dict, articles: List[Dict]
    ) -> Dict[str, Any]:
        """AI ë¶„ì„ ê²°ê³¼ë¥¼ ì…ì¥ë³„ë¡œ ê·¸ë£¹í™” (êµ­ë‚´/êµ­ì œ êµ¬ë¶„ ì—†ìŒ)"""
        restructured = []

        for claim_result in analysis_result.get('results', []):
            grouped_articles = self._group_articles_by_stance(
                claim_result.get('article_analyses', []), articles
            )

            restructured.append({
                'claim': claim_result.get('claim'),
                'supporting_evidence': self._create_evidence_section(
                    grouped_articles['supporting'],
                    claim_result.get('stance_summary', {}).get(
                        'common_supporting_arguments', []
                    ),
                ),
                'opposing_evidence': self._create_evidence_section(
                    grouped_articles['opposing'],
                    claim_result.get('stance_summary', {}).get(
                        'common_opposing_arguments', []
                    ),
                ),
                'neutral_coverage': {
                    'count': len(grouped_articles['neutral']),
                    'articles': grouped_articles['neutral'],
                },
                'diversity_metrics': self._calculate_diversity_metrics(
                    grouped_articles
                ),
            })

        return {'results': restructured}

    def _group_articles_by_stance(
        self, article_analyses: List[Dict], articles: List[Dict]
    ) -> Dict[str, List[Dict]]:
        """ê¸°ì‚¬ë“¤ì„ ì…ì¥ë³„ë¡œ ë¶„ë¥˜í•˜ê³  í™•ì‹ ë„ ìˆœìœ¼ë¡œ ì •ë ¬"""
        grouped = {'supporting': [], 'opposing': [], 'neutral': []}

        for analysis in article_analyses:
            article_idx = analysis.get('article_index') - 1
            if article_idx < 0 or article_idx >= len(articles):
                continue

            article = articles[article_idx].copy()
            article['analysis'] = {
                'stance': analysis.get('stance'),
                'confidence': analysis.get('confidence'),
                'key_evidence': analysis.get('key_evidence', []),
                'framing': analysis.get('framing', ''),
            }

            stance = analysis.get('stance')
            if stance in grouped:
                grouped[stance].append(article)

        # í™•ì‹ ë„ ìˆœìœ¼ë¡œ ì •ë ¬
        for stance_list in grouped.values():
            self._sort_by_confidence(stance_list)

        return grouped

    def _create_evidence_section(
        self, articles: List[Dict], common_arguments: List[str]
    ) -> Dict[str, Any]:
        """ì…ì¥ë³„ ì¦ê±° ì„¹ì…˜ ìƒì„±"""
        return {
            'count': len(articles),
            'articles': articles,
            'common_arguments': common_arguments,
        }

    def _calculate_diversity_metrics(
        self, grouped_articles: Dict[str, List[Dict]]
    ) -> Dict[str, Any]:
        """ë‹¤ì–‘ì„± ì§€í‘œ ê³„ì‚°"""
        total = sum(len(articles) for articles in grouped_articles.values())
        return {
            'total_sources': total,
            'stance_distribution': {
                stance: len(articles)
                for stance, articles in grouped_articles.items()
            },
        }

    # --- ê³µí†µ í—¬í¼ í•¨ìˆ˜ ---
    def _parse_json_response(self, response_text: str) -> Dict[str, Any]:
        """AI ì‘ë‹µì—ì„œ JSON íŒŒì‹±"""
        cleaned_text = (
            response_text.strip()
            .replace('```json', '')
            .replace('```', '')
            .strip()
        )
        return json.loads(cleaned_text)

    def _format_articles_for_ai(self, articles: List[Dict]) -> str:
        """ê¸°ì‚¬ ëª©ë¡ì„ AIê°€ ì½ì„ ìˆ˜ ìˆëŠ” í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        return "\n\n".join(
            [
                f"[ê¸°ì‚¬ {i+1}]\n"
                f"ì œëª©: {article.get('title', '')}\n"
                f"ì¶œì²˜: {article.get('source', '')}\n"
                f"ë‚´ìš©: {article.get('snippet', '')}"
                for i, article in enumerate(articles)
            ]
        )

    def _validate_stance_analysis_result(self, result: Dict) -> None:
        """ì…ì¥ ë¶„ì„ ê²°ê³¼ ìœ íš¨ì„± ê²€ì¦"""
        if 'results' not in result:
            raise ValueError("AI ì‘ë‹µì— 'results' í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤.")

        if not isinstance(result['results'], list):
            raise ValueError("AI ì‘ë‹µì˜ 'results'ê°€ ë°°ì—´ì´ ì•„ë‹™ë‹ˆë‹¤.")

    def _sort_by_confidence(self, articles: List[Dict]) -> None:
        """ê¸°ì‚¬ ëª©ë¡ì„ í™•ì‹ ë„ ìˆœìœ¼ë¡œ ì •ë ¬ (in-place)"""
        articles.sort(
            key=lambda x: x.get('analysis', {}).get('confidence', 0),
            reverse=True,
        )

    # --- ìºì‹± í—¬í¼ ---
    def _get_cache(self, url: str):
        if not db:
            return None
        try:
            cache_key = hashlib.md5(url.encode()).hexdigest()
            doc = db.collection('cache').document(cache_key).get()
            if doc.exists:
                print(f"âœ… ìºì‹œ íˆíŠ¸: {url[:50]}...")
                return doc.to_dict().get('result')
            return None
        except Exception as e:
            print(f"âš ï¸ ìºì‹œ ì½ê¸° ì‹¤íŒ¨: {e}")
            return None

    def _set_cache(self, url: str, result):
        if not db:
            return
        try:
            cache_key = hashlib.md5(url.encode()).hexdigest()
            db.collection('cache').document(cache_key).set(
                {'url': url, 'result': result, 'cached_at': datetime.now()}
            )
            print(f"âœ… ìºì‹œ ì €ì¥: {url[:50]}...")
        except Exception as e:
            print(f"âš ï¸ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
