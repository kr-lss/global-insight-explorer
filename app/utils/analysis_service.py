"""
ë¶„ì„ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ì„ ì²˜ë¦¬í•˜ëŠ” ì„œë¹„ìŠ¤ (Global Insight Explorer v2)
- í•œêµ­ì–´ ì‚¬ìš©ì ìµœì í™”
- [New] ì„ë² ë”© ê¸°ë°˜ ìŠ¤ë§ˆíŠ¸ í•„í„°ë§(Smart Filtering) ì ìš©
- [Refactor] Config ê¸°ë°˜ ì„¤ì • ê´€ë¦¬
- [Update] ê¸°ì‚¬ ì œëª© í•œê¸€ ë²ˆì—­ ê¸°ëŠ¥ ì¶”ê°€
"""
import os
import json
import hashlib
from datetime import datetime
import numpy as np  # ë²¡í„° ê³„ì‚°ìš©

import vertexai
from vertexai.generative_models import GenerativeModel, Tool, grounding
from vertexai.language_models import TextEmbeddingModel  # [ì‹ ê·œ] ì„ë² ë”© ëª¨ë¸
from google.cloud import firestore
from google.api_core.exceptions import GoogleAPICallError

from app.models.extractor import BaseExtractor, YoutubeExtractor, ArticleExtractor
from app.models.media import get_media_credibility
from app.config import config
from app.utils.gdelt_search import GDELTSearcher
from app.prompts.analysis_prompts import QUERY_OPTIMIZATION_PROMPT
from concurrent.futures import ThreadPoolExecutor, as_completed

# --- ì´ˆê¸°í™” ---
gemini = None
embedding_model = None
try:
    vertexai.init(project=config.GCP_PROJECT, location=config.GCP_REGION)

    # [ë¦¬íŒ©í† ë§] Configì—ì„œ ëª¨ë¸ëª… ê°€ì ¸ì˜¤ê¸°
    gemini = GenerativeModel(config.GEMINI_MODEL_ANALYSIS)

    # [ì‹ ê·œ] ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
    embedding_model = TextEmbeddingModel.from_pretrained(config.GEMINI_MODEL_EMBEDDING)

    print(f"âœ… (Service) AI ëª¨ë¸ ì—°ê²° ì„±ê³µ: {config.GEMINI_MODEL_ANALYSIS}, {config.GEMINI_MODEL_EMBEDDING}")
except Exception as e:
    print(f"âš ï¸ (Service) AI ëª¨ë¸ ì—°ê²° ì‹¤íŒ¨: {e}")

db = None
try:
    db = firestore.Client(project=config.GCP_PROJECT)
    print("âœ… (Service) Firestore ì—°ê²° ì„±ê³µ")
except Exception as e:
    print(f"âš ï¸ (Service) Firestore ì—°ê²° ì‹¤íŒ¨: {e}")


class AnalysisService:
    def __init__(self):
        self.extractors = {
            'youtube': YoutubeExtractor(),
            'article': ArticleExtractor(),
        }
        self.gdelt = GDELTSearcher()  # GDELT ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™”

    # ==================================================================
    # [ì‹ ê·œ] ì„ë² ë”© ê¸°ë°˜ ìŠ¤ë§ˆíŠ¸ í•„í„°ë§ í—¬í¼ í•¨ìˆ˜
    # ==================================================================
    def _get_embedding(self, text: str):
        """í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°(ìˆ«ì ë°°ì—´)ë¡œ ë³€í™˜"""
        if not embedding_model or not text:
            return None
        try:
            # ì„ë² ë”© ëª¨ë¸ì€ ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ 768ì°¨ì› ë“±ì˜ ë²¡í„°ë¡œ ë³€í™˜í•¨
            embeddings = embedding_model.get_embeddings([text])
            return embeddings[0].values
        except Exception as e:
            print(f"âš ï¸ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            return None

    def _calculate_similarity(self, vec1, vec2):
        """ë‘ ë²¡í„° ê°„ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° (-1.0 ~ 1.0)"""
        if vec1 is None or vec2 is None:
            return 0.0
        try:
            return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))
        except Exception as e:
            print(f"âš ï¸ ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0

    def _translate_to_korean(self, text: str) -> str:
        """
        [New] Geminië¥¼ ì‚¬ìš©í•˜ì—¬ ê¸°ì‚¬ ì œëª©ì„ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ë²ˆì—­
        """
        if not text or not gemini:
            return text
        try:
            prompt = f"""
            Translate the following news headline into natural Korean.
            Do not explain, just provide the translation.
            
            Headline: "{text}"
            Korean translation:
            """
            # ë¹ ë¥¸ ì‘ë‹µì„ ìœ„í•´ temperature ë‚®ì¶¤
            response = gemini.generate_content(prompt, generation_config={"temperature": 0.1})
            return response.text.strip()
        except Exception as e:
            print(f"âš ï¸ ë²ˆì—­ ì‹¤íŒ¨: {e}")
            return text

    def _get_extractor(self, input_type: str) -> BaseExtractor:
        extractor = self.extractors.get(input_type)
        if not extractor:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì…ë ¥ íƒ€ì…: {input_type}")
        return extractor

    # ==================================================================
    # 1ï¸âƒ£ 1ì°¨ ë¶„ì„ (Initial Analysis) - í•œêµ­ì–´ ì‚¬ìš©ì ìµœì í™”
    # ==================================================================
    def analyze_content(self, url: str, input_type: str):
        # ìºì‹œ í™•ì¸
        cached = self._get_cache(url)
        if cached:
            return cached, True

        # ì½˜í…ì¸  ì¶”ì¶œ
        print(f"ğŸ“¥ ì½˜í…ì¸  ì¶”ì¶œ ì¤‘: {url[:50]}...")
        extractor = self._get_extractor(input_type)
        content = extractor.extract(url)
        
        if not content or len(content) < 50:
            raise Exception("ì½˜í…ì¸ ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ê±°ë‚˜ ë‚´ìš©ì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤.")
            
        print(f"âœ… ì¶”ì¶œ ì™„ë£Œ: {len(content)} ê¸€ì")

        # AI ë¶„ì„
        print("ğŸ¤– Geminië¡œ 1ì°¨ ë¶„ì„ ì¤‘ (í•œê¸€ ìš”ì•½ + ì˜ì–´ ê²€ìƒ‰ì–´ ìƒì„±)...")
        result = self._analyze_with_gemini_bridge(content)
        print("âœ… 1ì°¨ ë¶„ì„ ì™„ë£Œ")

        # ìºì‹œ ì €ì¥
        self._set_cache(url, result)
        return result, False

    def _analyze_with_gemini_bridge(self, content: str):
        """
        Gemini í”„ë¡¬í”„íŠ¸: í•œêµ­ì–´ ì…ë ¥ì„ ë°›ì•„ 'í•œêµ­ì–´ ìš”ì•½'ê³¼ 'ì˜ì–´ ê²€ìƒ‰ì–´'ë¥¼ ë™ì‹œ ìƒì„±
        """
        if not gemini:
            raise Exception("Gemini APIë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        content = content[:config.MAX_CONTENT_LENGTH_FIRST_ANALYSIS]

        prompt = f"""
        ë‹¹ì‹ ì€ êµ­ì œ ì •ì„¸ ë° ë¯¸ë””ì–´ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
        ì£¼ì–´ì§„ í…ìŠ¤íŠ¸(ì˜ìƒ ìë§‰ ë˜ëŠ” ê¸°ì‚¬)ë¥¼ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ì •ë³´ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œí•˜ì„¸ìš”.

        [ë¶„ì„ ëŒ€ìƒ í…ìŠ¤íŠ¸]
        {content}

        [í•„ìˆ˜ ìš”êµ¬ì‚¬í•­]
        1. **ì‚¬ìš©ìëŠ” í•œêµ­ì¸ì…ë‹ˆë‹¤.** ëª¨ë“  ì„¤ëª…ê³¼ ìš”ì•½ì€ ìì—°ìŠ¤ëŸ¬ìš´ **í•œêµ­ì–´**ë¡œ ì‘ì„±í•˜ì„¸ìš”.
        2. **ê²€ìƒ‰ í‚¤ì›Œë“œ(search_keywords_en)**ëŠ” ë°˜ë“œì‹œ **ì˜ì–´(English)**ë¡œ ì‘ì„±í•˜ì„¸ìš”.
           - âŒ ì˜ëª»ëœ ì˜ˆ: "ë¶í•œ ë¯¸ì‚¬ì¼", "ê²½ì œ ìœ„ê¸°"
           - âœ… ì˜¬ë°”ë¥¸ ì˜ˆ: "North Korea missile", "economic crisis"
           - ì´ìœ : ì „ ì„¸ê³„ ë‰´ìŠ¤(GDELT, Google) ê²€ìƒ‰ì€ ì˜ì–´ë¡œë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤
        3. **ê´€ë ¨ êµ­ê°€(target_country_codes)**ëŠ” í•´ë‹¹ ì´ìŠˆì™€ ì´í•´ê´€ê³„ê°€ ìˆëŠ” êµ­ê°€ë“¤ì˜ **2ìë¦¬ ISO ì½”ë“œ**ë¡œ ì‘ì„±í•˜ì„¸ìš”.
           (ì˜ˆ: í•œêµ­='KR', ë¯¸êµ­='US', ì¤‘êµ­='CN', ë¶í•œ='KP', ëŸ¬ì‹œì•„='RU', ì¼ë³¸='JP')

        [ì¶œë ¥ ì˜ˆì‹œ]
        {{
          "title_kr": "ë¶í•œ ì‹ í˜• ICBM ë°œì‚¬ì™€ êµ­ì œì‚¬íšŒ ë°˜ì‘",
          "summary_kr": "ë¶í•œì´ ì‹ í˜• ICBM í™”ì„±-18í˜¸ë¥¼ ë°œì‚¬í–ˆìŠµë‹ˆë‹¤. ë¯¸êµ­ê³¼ í•œêµ­ì€ ê°•ë ¥íˆ ê·œíƒ„í–ˆìœ¼ë©°, ìœ ì—” ì•ˆë³´ë¦¬ ê¸´ê¸‰íšŒì˜ê°€ ì†Œì§‘ë˜ì—ˆìŠµë‹ˆë‹¤.",
          "topics": ["ë¶í•œ", "ë¯¸ì‚¬ì¼", "êµ­ì œì •ì¹˜"],
          "key_claims": [
            {{
              "claim_kr": "ë¶í•œì˜ í™”ì„±-18í˜¸ëŠ” ì‚¬ê±°ë¦¬ 15,000kmì˜ ì‹ í˜• ICBMì´ë‹¤",
              "search_keywords_en": ["North Korea", "Hwasong-18", "ICBM", "intercontinental ballistic missile", "15000km range"],
              "target_country_codes": ["KR", "US", "JP", "CN"]
            }},
            {{
              "claim_kr": "ë¯¸êµ­ì€ ì¶”ê°€ ì œì¬ë¥¼ ê²€í†  ì¤‘ì´ë‹¤",
              "search_keywords_en": ["United States", "North Korea sanctions", "additional sanctions", "UN Security Council"],
              "target_country_codes": ["US", "KR", "CN", "RU"]
            }}
          ]
        }}

        [ì¶œë ¥ í˜•ì‹ (JSON Only)]
        ë°˜ë“œì‹œ ìœ„ ì˜ˆì‹œì™€ ë™ì¼í•œ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”.
        search_keywords_enì€ ì ˆëŒ€ë¡œ í•œêµ­ì–´ë¥¼ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
        JSON ì™¸ì— ë‹¤ë¥¸ ë§ì€ í•˜ì§€ ë§ˆì„¸ìš”.
        """

        try:
            response = gemini.generate_content(prompt)
            result_text = response.text.strip().replace('```json', '').replace('```', '').strip()
            analysis_result = json.loads(result_text)

            # âœ… ì˜ì–´ í‚¤ì›Œë“œ ê²€ì¦ ë° ìë™ ìƒì„±
            if 'key_claims' in analysis_result:
                for claim in analysis_result['key_claims']:
                    # search_keywords_enì´ ì—†ê±°ë‚˜ ë¹„ì–´ìˆìœ¼ë©´ claim_krë¡œ ìƒì„± (í´ë°±)
                    if not claim.get('search_keywords_en') or len(claim.get('search_keywords_en', [])) == 0:
                        print(f"âš ï¸ ì˜ì–´ í‚¤ì›Œë“œ ëˆ„ë½ ê°ì§€, claim_krë¡œ ëŒ€ì²´: {claim.get('claim_kr', '')[:30]}...")
                        claim['search_keywords_en'] = [claim.get('claim_kr', '')]
                    else:
                        # í•œê¸€ í¬í•¨ ì—¬ë¶€ í™•ì¸ (ê°„ë‹¨í•œ ìœ ë‹ˆì½”ë“œ ë²”ìœ„ ì²´í¬)
                        keywords = claim.get('search_keywords_en', [])
                        has_korean = any(
                            any('\uac00' <= char <= '\ud7a3' for char in keyword)
                            for keyword in keywords
                        )
                        if has_korean:
                            print(f"âš ï¸ search_keywords_enì— í•œê¸€ ê°ì§€! AIê°€ í”„ë¡¬í”„íŠ¸ë¥¼ ë”°ë¥´ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {keywords}")
                            print(f"   â†’ ì´ í‚¤ì›Œë“œë¡œ GDELT ê²€ìƒ‰ì´ ì‹¤íŒ¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. claim_kr: {claim.get('claim_kr', '')[:50]}")

                    # target_country_codesê°€ ì—†ìœ¼ë©´ ë¹ˆ ë°°ì—´ë¡œ ì´ˆê¸°í™”
                    if 'target_country_codes' not in claim:
                        claim['target_country_codes'] = []

            return analysis_result
        except Exception as e:
            print(f"âŒ AI 1ì°¨ ë¶„ì„ ì‹¤íŒ¨: {e}")
            raise Exception(f"AI ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

    def optimize_search_query(self, user_input: str, context: dict):
        """
        ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ 'ê²€ìƒ‰ í‚¤ì›Œë“œ'ì™€ 'íƒ€ê²Ÿ êµ­ê°€ë“¤'ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
        ì´ì œ ì°¬ì„±/ë°˜ëŒ€ê°€ ì•„ë‹ˆë¼ 'ì–´ëŠ ë‚˜ë¼ì˜ ì‹œê°ì„ ë³¼ ê²ƒì¸ê°€'ë¥¼ ê²°ì •í•©ë‹ˆë‹¤.
        """
        if not gemini:
            raise Exception("Gemini APIë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        prompt = f"""
        ë‹¹ì‹ ì€ êµ­ì œ ë‰´ìŠ¤ íë ˆì´í„°ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ê¸€ë¡œë²Œ ì‹œê° ë¹„êµë¥¼ ìœ„í•œ ê²€ìƒ‰ ì „ëµì„ ìˆ˜ë¦½í•˜ì„¸ìš”.

        [ì§ˆë¬¸] "{user_input}"

        [ë¶„ì„ ëª©í‘œ]
        1. ì´ ì´ìŠˆê°€ **ë‹¨ì¼ êµ­ê°€ ì´ìŠˆ(Case A)**ì¸ì§€ **êµ­ê°€ ê°„/ë‹¤êµ­ì  ì´ìŠˆ(Case B)**ì¸ì§€ íŒë‹¨í•˜ì„¸ìš”.
           - Case A: ì£¼ì²´ê°€ ê¸°ì—…, ê°œì¸, ë˜ëŠ” í•œ êµ­ê°€ ë‚´ë¶€ì˜ ì‚¬ê±´ (ì˜ˆ: ìŠ¤í˜ì´ìŠ¤X ë°œì‚¬, í•œêµ­ ì˜ëŒ€ íŒŒì—…)
           - Case B: ì£¼ì²´ê°€ êµ­ê°€ ì •ë¶€ì´ê±°ë‚˜, ì—¬ëŸ¬ ë‚˜ë¼ê°€ ì–½íŒ ì‚¬ê±´ (ì˜ˆ: ë¯¸ì¤‘ ë¬´ì—­ ì „ìŸ, ìº„ë³´ë””ì•„ ë‚©ì¹˜ ì‚¬ê±´)
        2. **ê²€ìƒ‰í•  êµ­ê°€(target_countries)**ë¥¼ 3~5ê°œ ì„ ì •í•˜ì„¸ìš”.
           - Case A: ë³¸êµ­(Home) + ê²½ìŸêµ­/ê´€ì‹¬êµ­(Interested)
           - Case B: ë‹¹ì‚¬êµ­(Stakeholders) + ì¸ì ‘êµ­/ì˜í–¥ë°›ëŠ” êµ­(Neighbors)
           - **ë°˜ë“œì‹œ 2ìë¦¬ ISO êµ­ê°€ ì½”ë“œ(ì˜ˆ: US, KR, CN, KH, VN)ë¡œ ì¶œë ¥í•˜ì„¸ìš”.**
        3. ê²€ìƒ‰ì— ì‚¬ìš©í•  **ì˜ì–´ í‚¤ì›Œë“œ**ì™€ **GDELT í…Œë§ˆ**ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.
           - ë²”ì£„ ì‚¬ê±´ì˜ ê²½ìš° ë²•ë¥ ì  ìš©ì–´(Trafficking, Scam ë“±)ë¥¼ í¬í•¨í•˜ì„¸ìš”.

        [ì¶œë ¥ í˜•ì‹ (JSON Only)]
        {{
            "issue_type": "single" ë˜ëŠ” "multi_country",
            "primary_country": "KR",  // ì‚¬ê±´ì˜ ì¤‘ì‹¬ êµ­ê°€ (ì—†ìœ¼ë©´ 'Global')
            "topic_en": "Cambodia job scam and kidnapping",
            "gdelt_params": {{
                "keywords": ["human trafficking", "cyber scam", "job fraud", "Cambodia"],
                "themes": ["CRIME_COMMON_ROBBERY", "HUMAN_TRAFFICKING", "MANMADE_DISASTER_IMPLIED"],
                "event_date": "2024-01-01" // ì¶”ì • ë‚ ì§œ
            }},
            "target_countries": [
                {{"code": "KR", "role": "victim", "reason": "í”¼í•´ì êµ­ì "}},
                {{"code": "KH", "role": "source", "reason": "ì‚¬ê±´ ë°œìƒêµ­"}},
                {{"code": "CN", "role": "involved", "reason": "ë²”ì£„ ì—°ë£¨ ë° ê³µì¡°"}},
                {{"code": "VN", "role": "neighbor", "reason": "ì¸ì ‘êµ­ ê´€ì "}}
            ]
        }}
        JSON ì™¸ì— ë‹¤ë¥¸ ë§ì€ í•˜ì§€ ë§ˆì„¸ìš”.
        """

        try:
            print(f"ğŸ¤– ì´ìŠˆ ìœ í˜• ë° íƒ€ê²Ÿ êµ­ê°€ ë¶„ì„ ì¤‘: '{user_input}'")
            response = gemini.generate_content(prompt)
            result_text = response.text.strip().replace('```json', '').replace('```', '').strip()
            return {"success": True, "data": json.loads(result_text)}
        except Exception as e:
            print(f"âš ï¸ ì¿¼ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
            # Fallback (ê¸°ë³¸ê°’: ë¯¸êµ­, í•œêµ­)
            return {
                "success": False,
                "data": {
                    "issue_type": "multi_country",
                    "target_countries": [{"code": "US"}, {"code": "KR"}],
                    "gdelt_params": {"keywords": [user_input], "themes": []}
                }
            }

    # ==================================================================
    # [Phase 1.5 í•µì‹¬] ìŠ¤ë§ˆíŠ¸ í•„í„°ë§ì´ ì ìš©ëœ êµ­ê°€ë³„ ê²€ìƒ‰
    # ==================================================================
    def get_global_perspectives(self, search_params: dict):
        """
        í™•ì •ëœ ì „ëµì— ë”°ë¼ êµ­ê°€ë³„ë¡œ GDELTë¥¼ ì¡°íšŒí•˜ê³ (Loop Search),
        ì„ë² ë”© ê¸°ë°˜ ìŠ¤ë§ˆíŠ¸ í•„í„°ë§ì„ ì ìš©í•˜ì—¬ ê´€ë ¨ì„± ë†’ì€ ê¸°ì‚¬ë§Œ ì„ ë³„í•©ë‹ˆë‹¤.
        """
        gdelt_base_params = search_params.get('gdelt_params', {})
        target_countries = search_params.get('target_countries', [])
        topic_en = search_params.get('topic_en', '')

        # 1. [ê¸°ì¤€ì ] ì£¼ì œ(Topic)ì— ëŒ€í•œ ì„ë² ë”© ìƒì„±
        print(f"ğŸ§  ì£¼ì œ ì„ë² ë”© ìƒì„± ì¤‘: '{topic_en}'")
        topic_embedding = self._get_embedding(topic_en)

        # ìµœì¢… ê²°ê³¼ ì»¨í…Œì´ë„ˆ (í”„ë¡ íŠ¸ì—”ë“œ ì•½ì† í¬ë§·)
        final_response = {
            "status": "success",
            "issue_type": search_params.get('issue_type', 'multi_country'),
            "topic": topic_en,
            "data": {}  # ì—¬ê¸°ì— êµ­ê°€ ì½”ë“œ("US", "KR")ê°€ í‚¤(Key)ë¡œ ë“¤ì–´ê°‘ë‹ˆë‹¤.
        }

        all_collected_urls = set()  # ì¤‘ë³µ ê¸°ì‚¬ ë°©ì§€ìš© (URL)

        # ğŸ”„ êµ­ê°€ë³„ ë£¨í”„ ì‹¤í–‰
        for target in target_countries:
            country_code = target.get('code', 'Unknown')
            role_desc = target.get('reason', '')

            print(f"ğŸŒ [{country_code}] ê²€ìƒ‰ ì‹œì‘ ({role_desc})...")

            # 1. í•´ë‹¹ êµ­ê°€ ì „ìš© íŒŒë¼ë¯¸í„° ì„¤ì •
            current_params = gdelt_base_params.copy()
            current_params['locations'] = [country_code]  # GDELT Location í•„í„° í™œìš©

            # 2. GDELT ê²€ìƒ‰ (ìˆ˜ëŸ‰ì„ ë„‰ë„‰í•˜ê²Œ ê°€ì ¸ì™€ì„œ í•„í„°ë§)
            raw_articles = self.gdelt.search(current_params)

            # 3. [ìŠ¤ë§ˆíŠ¸ í•„í„°ë§] ì„ë² ë”© ìœ ì‚¬ë„ ê²€ì‚¬
            valid_articles = []
            for article in raw_articles:
                if article['url'] in all_collected_urls:
                    continue

                # ì œëª©ì´ ì—†ëŠ” ê²½ìš° ì†ŒìŠ¤ë¡œ ëŒ€ì²´
                title = article.get('title') or article.get('source') or ''

                # ìœ ì‚¬ë„ ê³„ì‚° (ì£¼ì œ <-> ê¸°ì‚¬ ì œëª©)
                score = 0.0
                if topic_embedding:
                    article_embedding = self._get_embedding(title)
                    score = self._calculate_similarity(topic_embedding, article_embedding)
                else:
                    score = 1.0  # ì„ë² ë”© ì‹¤íŒ¨ ì‹œ í†µê³¼

                # [í•„í„°ë§] ê¸°ì¤€ì (config.SIMILARITY_THRESHOLD) ì´ìƒë§Œ í•©ê²©
                if score >= config.SIMILARITY_THRESHOLD:
                    article['relevance_score'] = round(score, 3)
                    valid_articles.append(article)
                    all_collected_urls.add(article['url'])
                    # print(f"   âœ… í•©ê²© (ìœ ì‚¬ë„ {score:.3f}): {title[:50]}...")
                else:
                    pass
                    # print(f"   ğŸ—‘ï¸ ì œì™¸ (ìœ ì‚¬ë„ {score:.3f}): {title[:50]}...")

            # ê´€ë ¨ì„± ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬ (ë†’ì€ ê²Œ ìœ„ë¡œ)
            valid_articles.sort(key=lambda x: x.get('relevance_score', 0), reverse=True)

            # ìƒìœ„ 5ê°œë§Œ ì„ íƒ (ì¿¼í„°ì œ)
            top_articles = valid_articles[:5]

            # 4. ë³¸ë¬¸ ì¶”ì¶œ (ë³‘ë ¬) + [New] ì œëª© ë²ˆì—­
            if top_articles:
                print(f"   â†³ {len(top_articles)}ê°œ ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ ë° ë²ˆì—­")
                full_articles = self._extract_contents_parallel(top_articles)

                # 5. ê²°ê³¼ ì €ì¥
                final_response['data'][country_code] = {
                    "role": role_desc,
                    "count": len(full_articles),
                    "articles": full_articles
                }
            else:
                # ê¸°ì‚¬ê°€ ì—†ëŠ” ê²½ìš°ë„ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ëª…ì‹œ (UI ì²˜ë¦¬ë¥¼ ìœ„í•´)
                final_response['data'][country_code] = {
                    "role": role_desc,
                    "count": 0,
                    "articles": [],
                    "message": "ê´€ë ¨ì„± ë†’ì€ ê¸°ì‚¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
                }

        return final_response

    # ==================================================================
    # 2ï¸âƒ£ 2ì°¨ ë¶„ì„ (Find Sources) - AI ì¶”ë¡  ì—†ì´ ê²€ìƒ‰ë§Œ ìˆ˜í–‰
    # ==================================================================
    def find_sources_for_claims(
        self, url: str, input_type: str, claims_data: list
    ):
        """
        [Step 2] í™•ì •ëœ ê²€ìƒ‰ ì „ëµ(claims_data)ìœ¼ë¡œ ì‹¤ì œ GDELT 5ëŒ€ ìš”ì†Œ ê²€ìƒ‰ ìˆ˜í–‰
        """
        all_results = []
        all_articles = []

        # ê° ì£¼ì¥ë³„ë¡œ ë…ë¦½ì ì¸ ê²€ìƒ‰ ìˆ˜í–‰
        for claim_data in claims_data:
            claim_kr = claim_data.get('claim_kr', '')

            # âœ… NEW: gdelt_params ìš°ì„  ì‚¬ìš© (5ëŒ€ ìš”ì†Œ ê²€ìƒ‰)
            gdelt_params = claim_data.get('gdelt_params')

            if not gdelt_params:
                # Fallback: ê¸°ì¡´ search_keywords_en ë°©ì‹ìœ¼ë¡œ ë³€í™˜
                search_keywords = claim_data.get('search_keywords_en', [])
                target_countries = claim_data.get('target_country_codes', [])

                if not search_keywords:
                    print(f"âš ï¸ ê²€ìƒ‰ íŒŒë¼ë¯¸í„° ì—†ìŒ - ìŠ¤í‚µ: '{claim_kr[:30]}...'")
                    continue

                gdelt_params = {
                    'keywords': search_keywords,
                    'entities': [],
                    'locations': [],
                    'themes': [],
                    'event_date': datetime.now().strftime('%Y-%m-%d')
                }
                print(f"ğŸ” '{claim_kr[:15]}...' ê²€ìƒ‰ (Legacy ëª¨ë“œ: keywords={search_keywords})")
            else:
                print(f"ğŸ” '{claim_kr[:15]}...' ê²€ìƒ‰ (5ëŒ€ ìš”ì†Œ ëª¨ë“œ)")

            # GDELT 5ëŒ€ ìš”ì†Œ ê²€ìƒ‰ ì‹¤í–‰
            articles = self._search_real_articles_with_params(gdelt_params)

            # ê²°ê³¼ êµ¬ì¡°í™”
            result_entry = {
                "claim": claim_kr,
                "searched_keywords": gdelt_params.get('keywords', []),
                "articles": articles
            }
            all_results.append(result_entry)
            all_articles.extend(articles)

        # ì¤‘ë³µ ì œê±° (URL ê¸°ì¤€)
        unique_articles = {v['url']: v for v in all_articles}.values()
        final_articles = list(unique_articles)

        print(f"âœ… ê²€ìƒ‰ ì™„ë£Œ: {len(final_articles)}ê°œ ê¸°ì‚¬ ë°œê²¬")

        # AI ë¶„ì„ ì—†ì´ ê²€ìƒ‰ ê²°ê³¼ë§Œ ë°˜í™˜
        return {"results": all_results}, final_articles

    def _search_real_articles_with_params(self, gdelt_params: dict):
        """
        GDELT 5ëŒ€ ìš”ì†Œ ê²€ìƒ‰ with Google Search Fallback
        """
        if not gdelt_params:
            return []

        # 1ï¸âƒ£ GDELT 5ëŒ€ ìš”ì†Œ ê²€ìƒ‰ ì‹œë„ (ë¬´ë£Œ, ë¹ ë¦„, ê¸€ë¡œë²Œ)
        print(f"ğŸ“Š [1/2] GDELT 5ëŒ€ ìš”ì†Œ ê²€ìƒ‰ ì¤‘...")
        gdelt_results = []
        try:
            gdelt_results = self.gdelt.search(gdelt_params)
        except Exception as e:
            print(f"âš ï¸ GDELT ê²€ìƒ‰ ì‹¤íŒ¨: {e}")

        # 2ï¸âƒ£ ë³‘ë ¬ ë³¸ë¬¸ ì¶”ì¶œ (ThreadPool 10ê°œ ì›Œì»¤)
        if gdelt_results:
            print(f"ğŸ”„ ë³‘ë ¬ ë³¸ë¬¸ ì¶”ì¶œ ì¤‘... ({len(gdelt_results)}ê°œ ê¸°ì‚¬)")
            extracted = self._extract_contents_parallel(gdelt_results)
            print(f"âœ… ì¶”ì¶œ ì™„ë£Œ: {len(extracted)}ê°œ")
            return extracted

        # 3ï¸âƒ£ GDELT ì‹¤íŒ¨ ì‹œ Google Search Grounding í´ë°±
        print(f"âš ï¸ GDELT ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ, Google Search ì‹œë„...")

        # gdelt_paramsì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ (ëª¨ë“  ìš”ì†Œ ê²°í•©)
        all_keywords = []
        all_keywords.extend(gdelt_params.get('keywords', []))
        all_keywords.extend(gdelt_params.get('entities', []))
        all_keywords.extend(gdelt_params.get('locations', []))

        google_results = self._search_google_fallback(all_keywords[:config.MAX_KEYWORDS], [])

        if google_results:
            print(f"âœ… Google Search ì™„ë£Œ: {len(google_results)}ê°œ ë°œê²¬")
            return google_results

        print(f"âš ï¸ Google Searchë„ ê²°ê³¼ ì—†ìŒ")
        return []

    def _extract_contents_parallel(self, articles_meta: list):
        """
        ë³‘ë ¬ ì²˜ë¦¬ë¡œ ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ ë° [New] ì œëª© ë²ˆì—­ (ThreadPool)
        """
        extracted = []
        extractor = self.extractors['article']

        def fetch_one(meta):
            """ë‹¨ì¼ ê¸°ì‚¬ ì¶”ì¶œ (ë³‘ë ¬ ì‹¤í–‰ í•¨ìˆ˜)"""
            try:
                url = meta.get('url', '')
                if not url or url == '#':
                    return None

                # ì œëª©ê³¼ ë³¸ë¬¸ ì¶”ì¶œ
                result = extractor.extract_with_title(url)
                title = result.get('title', '')
                content = result.get('content', '')

                # ë„ˆë¬´ ì§§ìœ¼ë©´ ë¬´ì‹œ
                if not content or len(content) < 100:
                    return None

                # ë©”íƒ€ë°ì´í„°ì— ì œëª©ê³¼ ë³¸ë¬¸ ì¶”ê°€
                meta['title'] = title if title else meta.get('source', 'No title')  # ì œëª©ì´ ì—†ìœ¼ë©´ ì¶œì²˜ë¥¼ ì œëª©ìœ¼ë¡œ
                
                # [New] ì œëª© í•œêµ­ì–´ ë²ˆì—­ ìˆ˜í–‰ (ë³‘ë ¬ ì²˜ë¦¬ì˜ ì´ì  í™œìš©)
                meta['title_kr'] = self._translate_to_korean(meta['title'])
                
                meta['content'] = content
                meta['snippet'] = content[:500]  # ë¯¸ë¦¬ë³´ê¸°

                # ì–¸ë¡ ì‚¬ ì •ë³´ ì¶”ê°€ (êµ­ê°€/ì¶œì²˜ ê¸°ë°˜)
                media_info = get_media_credibility(
                    meta.get('source', ''),
                    meta.get('country', '')
                )

                # êµ­ì˜/ë¯¼ì˜ ì •ë³´ë§Œ ì¶”ê°€
                if media_info:
                    meta['media_type'] = media_info.get('type', 'ì•Œ ìˆ˜ ì—†ìŒ')
                    meta['media_category'] = media_info.get('category', 'ì•Œ ìˆ˜ ì—†ìŒ')

                print(f"âœ… ì¶”ì¶œ/ë²ˆì—­ ì„±ê³µ: {meta.get('source', 'Unknown')}")
                return meta

            except Exception as e:
                print(f"âš ï¸ ì¶”ì¶œ ì‹¤íŒ¨: {meta.get('url', 'unknown')} - {e}")
                return None

        # ThreadPool ë³‘ë ¬ ì‹¤í–‰
        with ThreadPoolExecutor(max_workers=config.THREAD_POOL_WORKERS) as executor:
            futures = [executor.submit(fetch_one, item) for item in articles_meta]

            for future in as_completed(futures):
                result = future.result()
                if result:
                    extracted.append(result)

        return extracted

    def _search_google_fallback(self, keywords: list, target_countries: list = None):
        """Google Search í´ë°± (GDELT ì‹¤íŒ¨ ì‹œ)"""
        if not keywords:
            return []

        base_query = " ".join(keywords[:config.MAX_KEYWORDS])
        query = base_query

        if target_countries and len(target_countries) > 0:
            country_query = " OR ".join(target_countries)
            query = f"{base_query} ({country_query})"

        print(f"ğŸ” Google Search Query: {query}")

        try:
            model = GenerativeModel(config.GEMINI_MODEL_SEARCH)
            prompt = f"""Find recent news articles about: {query}
            Return a JSON list of articles with this structure:
            [ {{"title": "article title", "url": "https://...", "source": "source name"}}, ... ]
            Only return valid JSON."""

            search_tool = Tool(google_search_retrieval=grounding.GoogleSearchRetrieval())
            response = model.generate_content(prompt, tools=[search_tool])

            articles = []
            try:
                import re
                text = response.text
                json_match = re.search(r'\[.*\]', text, re.DOTALL)
                if json_match:
                    parsed = json.loads(json_match.group())
                    for item in parsed[:10]:
                        if isinstance(item, dict) and 'url' in item:
                            item['country'] = target_countries[0] if target_countries else 'Unknown'
                            # êµ¬ê¸€ ê²€ìƒ‰ ê²°ê³¼ë„ ë²ˆì—­
                            item['title_kr'] = self._translate_to_korean(item.get('title', ''))
                            articles.append(item)
            except Exception:
                pass
            
            if not articles and hasattr(response, 'candidates'):
                 for candidate in response.candidates:
                    if hasattr(candidate, 'grounding_metadata'):
                        for chunk in candidate.grounding_metadata.grounding_chunks[:10]:
                            if hasattr(chunk, 'web'):
                                articles.append({
                                    'title': chunk.web.title,
                                    'title_kr': self._translate_to_korean(chunk.web.title),
                                    'url': chunk.web.uri,
                                    'source': 'Google Search',
                                    'country': target_countries[0] if target_countries else 'Unknown'
                                })

            if articles:
                print(f"âœ… Google Searchì—ì„œ {len(articles)}ê°œ URL ì¶”ì¶œ ì„±ê³µ")
                # ë³¸ë¬¸ ì¶”ì¶œì€ ë³„ë„ë¡œ í•´ì•¼ í•¨ (ì—¬ê¸°ì„œëŠ” URLë§Œ ë°˜í™˜í•˜ê±°ë‚˜ ê·¸ëŒ€ë¡œ ì‚¬ìš©)
                return articles

            return []

        except Exception as e:
            print(f"âš ï¸ Google Search ì‹¤íŒ¨: {e}")
            return []

    # --- ìºì‹œ ë° ìœ í‹¸ë¦¬í‹° ---
    def _get_cache(self, url: str):
        if not db: return None
        try:
            cache_key = hashlib.md5(url.encode()).hexdigest()
            doc = db.collection('cache').document(cache_key).get()
            if doc.exists:
                print(f"âœ… ìºì‹œ íˆíŠ¸: {url[:30]}...")
                return doc.to_dict().get('result')
        except: pass
        return None

    def _set_cache(self, url: str, result):
        if not db: return
        try:
            cache_key = hashlib.md5(url.encode()).hexdigest()
            db.collection('cache').document(cache_key).set({
                'url': url, 'result': result, 'cached_at': datetime.now()
            })
        except: pass