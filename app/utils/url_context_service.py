"""
URL Context Service for Web Crawling
ì›¹í˜ì´ì§€ URLì„ ì§ì ‘ Gemini APIì— ì „ë‹¬í•˜ì—¬ ë‚´ìš© ì¶”ì¶œ (í¬ë¡¤ë§ ë¶ˆí•„ìš”)
"""
import json
from google import genai
from google.genai import types
from app.config import config


class URLContextService:
    """URL Context APIë¥¼ ì‚¬ìš©í•œ ì›¹ í¬ë¡¤ë§ ìš°íšŒ"""

    def __init__(self, api_key: str = None):
        """
        Initialize URL Context Service

        Args:
            api_key: Gemini API key (ì—†ìœ¼ë©´ í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜´)
        """
        try:
            # Vertex AI ë°©ì‹ ì‚¬ìš©
            self.client = genai.Client(
                vertexai=True,
                project=config.GCP_PROJECT,
                location=config.GCP_REGION,
                http_options=types.HttpOptions(api_version="v1beta1")
            )
            self.model = "gemini-2.0-flash"
            print("âœ… (URLContextService) Gemini API ì—°ê²° ì„±ê³µ")
        except Exception as e:
            print(f"âš ï¸ (URLContextService) Gemini API ì—°ê²° ì‹¤íŒ¨: {e}")
            self.client = None

    def analyze_webpage(self, url: str, analysis_prompt: str = None) -> dict:
        """
        ì›¹í˜ì´ì§€ë¥¼ ì§ì ‘ ë¶„ì„ (HTML íŒŒì‹± ë¶ˆí•„ìš”)

        Args:
            url: ë¶„ì„í•  ì›¹í˜ì´ì§€ URL
            analysis_prompt: ë¶„ì„ ìš”ì²­ í”„ë¡¬í”„íŠ¸ (ê¸°ë³¸ê°’: ìš”ì•½)

        Returns:
            ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        if not self.client:
            raise Exception("Gemini APIë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        print(f"ğŸŒ URL Context Processing: {url[:50]}...")

        # URL Context ë„êµ¬ í™œì„±í™”
        url_context_tool = types.Tool(url_context=types.UrlContext)

        # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸
        if not analysis_prompt:
            analysis_prompt = f"""
ë‹¤ìŒ ì›¹í˜ì´ì§€ë¥¼ ë¶„ì„í•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”: {url}

ì‘ë‹µ í˜•ì‹:
{{
  "title": "í˜ì´ì§€ ì œëª©",
  "summary": "ì£¼ìš” ë‚´ìš© ìš”ì•½ (3-5ë¬¸ì¥)",
  "key_claims": ["ì£¼ì¥ 1", "ì£¼ì¥ 2", ...],
  "topics": ["ì£¼ì œ1", "ì£¼ì œ2"],
  "related_countries": ["êµ­ê°€1", "êµ­ê°€2"],
  "author": "ì‘ì„±ì (ìˆëŠ” ê²½ìš°)",
  "published_date": "ë°œí–‰ì¼ (ìˆëŠ” ê²½ìš°)"
}}

ë°˜ë“œì‹œ JSON ê°ì²´ë§Œ ì¶œë ¥í•˜ì„¸ìš”.
"""
        else:
            analysis_prompt = f"{analysis_prompt}\n\nURL: {url}\n\në°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”."

        # Geminiì— URL ì „ë‹¬
        response = self.client.models.generate_content(
            model=self.model,
            contents=analysis_prompt,
            config=types.GenerateContentConfig(
                tools=[url_context_tool],
                temperature=0.0,
                response_mime_type="application/json"
            )
        )

        # ë©”íƒ€ë°ì´í„° í™•ì¸ (ë””ë²„ê¹…ìš©)
        if hasattr(response, 'candidates') and len(response.candidates) > 0:
            metadata = getattr(response.candidates[0], 'url_context_metadata', None)
            if metadata:
                print(f"âœ… URL ë¡œë“œ ì„±ê³µ: {metadata}")

        # JSON íŒŒì‹±
        result_text = response.text.strip().replace('```json', '').replace('```', '').strip()
        result = json.loads(result_text)

        print(f"âœ… ì›¹í˜ì´ì§€ ë¶„ì„ ì™„ë£Œ")
        return result

    def analyze_multiple_urls(self, urls: list[str], comparison_prompt: str = None) -> dict:
        """
        ì—¬ëŸ¬ URLì„ í•œ ë²ˆì— ë¹„êµ ë¶„ì„ (ìµœëŒ€ 20ê°œ)

        Args:
            urls: URL ë¦¬ìŠ¤íŠ¸ (ìµœëŒ€ 20ê°œ)
            comparison_prompt: ë¹„êµ ë¶„ì„ í”„ë¡¬í”„íŠ¸

        Returns:
            ë¹„êµ ë¶„ì„ ê²°ê³¼
        """
        if not self.client:
            raise Exception("Gemini APIë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        if len(urls) > 20:
            raise ValueError("ìµœëŒ€ 20ê°œ URLê¹Œì§€ ì§€ì›í•©ë‹ˆë‹¤.")

        print(f"ğŸŒ ë‹¤ì¤‘ URL ë¶„ì„: {len(urls)}ê°œ í˜ì´ì§€...")

        # URL Context ë„êµ¬ í™œì„±í™”
        url_context_tool = types.Tool(url_context=types.UrlContext)

        # ê¸°ë³¸ ë¹„êµ í”„ë¡¬í”„íŠ¸
        if not comparison_prompt:
            urls_text = "\n".join([f"{i+1}. {url}" for i, url in enumerate(urls)])
            comparison_prompt = f"""
ë‹¤ìŒ ì›¹í˜ì´ì§€ë“¤ì„ ë¹„êµ ë¶„ì„í•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:

{urls_text}

ì‘ë‹µ í˜•ì‹:
{{
  "common_topics": ["ê³µí†µ ì£¼ì œ1", "ê³µí†µ ì£¼ì œ2"],
  "different_perspectives": [
    {{
      "topic": "ì£¼ì œ",
      "url1_view": "ì²« ë²ˆì§¸ í˜ì´ì§€ì˜ ê´€ì ",
      "url2_view": "ë‘ ë²ˆì§¸ í˜ì´ì§€ì˜ ê´€ì "
    }}
  ],
  "summary": "ì „ì²´ ë¹„êµ ìš”ì•½",
  "credibility_notes": "ê° ì¶œì²˜ì˜ ì‹ ë¢°ì„± í‰ê°€"
}}

ë°˜ë“œì‹œ JSON ê°ì²´ë§Œ ì¶œë ¥í•˜ì„¸ìš”.
"""

        # Geminiì— ì—¬ëŸ¬ URL ì „ë‹¬
        response = self.client.models.generate_content(
            model=self.model,
            contents=comparison_prompt,
            config=types.GenerateContentConfig(
                tools=[url_context_tool],
                temperature=0.0,
                response_mime_type="application/json"
            )
        )

        # JSON íŒŒì‹±
        result_text = response.text.strip().replace('```json', '').replace('```', '').strip()
        result = json.loads(result_text)

        print(f"âœ… ë‹¤ì¤‘ URL ë¶„ì„ ì™„ë£Œ")
        return result

    def extract_article_content(self, url: str) -> str:
        """
        ê¸°ì‚¬ ë³¸ë¬¸ë§Œ ì¶”ì¶œ (ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ)

        Args:
            url: ê¸°ì‚¬ URL

        Returns:
            ë³¸ë¬¸ í…ìŠ¤íŠ¸
        """
        if not self.client:
            raise Exception("Gemini APIë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        print(f"ğŸ“° ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ: {url[:50]}...")

        url_context_tool = types.Tool(url_context=types.UrlContext)

        prompt = f"""
ë‹¤ìŒ ê¸°ì‚¬ì˜ ë³¸ë¬¸ ë‚´ìš©ë§Œ ì¶”ì¶œí•´ì£¼ì„¸ìš”: {url}

ê´‘ê³ , ë„¤ë¹„ê²Œì´ì…˜, ì‚¬ì´ë“œë°” ë“±ì€ ì œì™¸í•˜ê³  ìˆœìˆ˜í•œ ê¸°ì‚¬ ë‚´ìš©ë§Œ ë°˜í™˜í•˜ì„¸ìš”.
"""

        response = self.client.models.generate_content(
            model=self.model,
            contents=prompt,
            config=types.GenerateContentConfig(
                tools=[url_context_tool],
                temperature=0.0
            )
        )

        print(f"âœ… ë³¸ë¬¸ ì¶”ì¶œ ì™„ë£Œ")
        return response.text.strip()


# ============================================================
# ğŸ”„ ë¹„ë™ê¸° ë³‘ë ¬ ì²˜ë¦¬ ë²„ì „ (Async Processing)
# ì—¬ëŸ¬ ì›¹í˜ì´ì§€ë¥¼ ë™ì‹œì— ë¹ ë¥´ê²Œ ì²˜ë¦¬í•˜ê³  ì‹¶ì„ ë•Œ ì£¼ì„ í•´ì œí•˜ì—¬ ì‚¬ìš©
# ============================================================

# import asyncio
# from tenacity import retry, wait_exponential, stop_after_attempt
#
# class AsyncURLContextService:
#     """ë¹„ë™ê¸° ë°©ì‹ìœ¼ë¡œ ì—¬ëŸ¬ ì›¹í˜ì´ì§€ë¥¼ ë³‘ë ¬ ì²˜ë¦¬"""
#
#     def __init__(self, api_key: str = None):
#         try:
#             self.client = genai.Client(
#                 vertexai=True,
#                 project=config.GCP_PROJECT,
#                 location=config.GCP_REGION,
#                 http_options=types.HttpOptions(api_version="v1beta1")
#             )
#             self.model = "gemini-2.0-flash"
#             print("âœ… (AsyncURLContextService) Gemini API ì—°ê²° ì„±ê³µ")
#         except Exception as e:
#             print(f"âš ï¸ (AsyncURLContextService) ì—°ê²° ì‹¤íŒ¨: {e}")
#             self.client = None
#
#     @retry(
#         wait=wait_exponential(multiplier=2, min=2, max=60),
#         stop=stop_after_attempt(3)
#     )
#     async def analyze_webpage_async(self, url: str, prompt: str) -> dict:
#         """
#         ë‹¨ì¼ ì›¹í˜ì´ì§€ ë¹„ë™ê¸° ë¶„ì„ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
#
#         Args:
#             url: ì›¹í˜ì´ì§€ URL
#             prompt: ë¶„ì„ í”„ë¡¬í”„íŠ¸
#
#         Returns:
#             ë¶„ì„ ê²°ê³¼
#         """
#         if not self.client:
#             raise Exception("Gemini APIë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
#
#         print(f"ğŸŒ ë¹„ë™ê¸° ë¶„ì„ ì‹œì‘: {url[:50]}...")
#
#         url_context_tool = types.Tool(url_context=types.UrlContext)
#         full_prompt = f"{prompt}\n\nURL: {url}\n\në°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”."
#
#         response = await self.client.aio.models.generate_content(
#             model=self.model,
#             contents=full_prompt,
#             config=types.GenerateContentConfig(
#                 tools=[url_context_tool],
#                 temperature=0.0,
#                 response_mime_type="application/json"
#             )
#         )
#
#         result_text = response.text.strip().replace('```json', '').replace('```', '').strip()
#         return json.loads(result_text)
#
#     async def analyze_multiple_webpages(
#         self,
#         urls: list[str],
#         analysis_prompt: str,
#         max_concurrent: int = 10
#     ) -> list[dict]:
#         """
#         ì—¬ëŸ¬ ì›¹í˜ì´ì§€ë¥¼ ë³‘ë ¬ë¡œ ë¶„ì„
#
#         Args:
#             urls: URL ë¦¬ìŠ¤íŠ¸
#             analysis_prompt: ë¶„ì„ í”„ë¡¬í”„íŠ¸
#             max_concurrent: ìµœëŒ€ ë™ì‹œ ì²˜ë¦¬ ìˆ˜
#
#         Returns:
#             ë¶„ì„ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
#         """
#         semaphore = asyncio.Semaphore(max_concurrent)
#
#         async def analyze_with_semaphore(url):
#             async with semaphore:
#                 try:
#                     return await self.analyze_webpage_async(url, analysis_prompt)
#                 except Exception as e:
#                     print(f"âŒ ë¶„ì„ ì‹¤íŒ¨: {url} - {e}")
#                     return None
#
#         tasks = [analyze_with_semaphore(url) for url in urls]
#         results = await asyncio.gather(*tasks)
#
#         # None ì œì™¸
#         return [r for r in results if r is not None]
#
#
# # ì‚¬ìš© ì˜ˆì‹œ:
# # async def main():
# #     service = AsyncURLContextService()
# #     urls = [
# #         "https://news.example.com/article1",
# #         "https://news.example.com/article2",
# #         "https://news.example.com/article3",
# #     ]
# #     prompt = "ì´ ê¸°ì‚¬ì˜ í•µì‹¬ ì£¼ì¥ì„ ì¶”ì¶œí•´ì£¼ì„¸ìš”."
# #     results = await service.analyze_multiple_webpages(urls, prompt, max_concurrent=10)
# #     print(results)
# #
# # asyncio.run(main())
