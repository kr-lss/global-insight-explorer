"""
GDELT Search Service - DOC API ê¸°ë°˜ ì „ë¬¸ ê²€ìƒ‰ ì—”ì§„ (v2 - Bug Fixed)

[Architecture]
- Primary: GDELT DOC 2.0 API (ë³¸ë¬¸ ì „ë¬¸ ê²€ìƒ‰)
- Fallback: BigQuery GKG (ë©”íƒ€ë°ì´í„° ê²€ìƒ‰)

[Design Pattern]
- Strategy Pattern: ê²€ìƒ‰ ì „ëµ êµì²´ ê°€ëŠ¥
- Template Method: ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸ í‘œì¤€í™”

[v2 ìˆ˜ì •ì‚¬í•­]
- timespan íŒŒë¼ë¯¸í„° ê·œê²© ìˆ˜ì • (GDELT ê³µì‹ í˜•ì‹)
- ê¸°ì¡´ ë¡œì§ í˜¸í™˜ì„± (entities/themes â†’ keywords ë³‘í•©)
- API ë ˆë²¨ ì¤‘ë³µ URL ì œê±°
"""

import requests
from abc import ABC, abstractmethod
from dataclasses import dataclass
from typing import List, Dict, Optional, Set
from datetime import datetime, timedelta
from urllib.parse import urlparse, urlunparse
from concurrent.futures import ThreadPoolExecutor, as_completed

from google.cloud import bigquery
from app.config import config


# ============================================================
# Data Models
# ============================================================

@dataclass
class ArticleResult:
    """ê²€ìƒ‰ëœ ê¸°ì‚¬ ë°ì´í„° ëª¨ë¸"""
    url: str
    title: str
    source: str
    date: str
    snippet: str = ""
    country: str = "Unknown"
    tone: float = 0.0
    relevance_score: float = 0.0

    def to_dict(self) -> Dict:
        return {
            'url': self.url,
            'title': self.title,
            'source': self.source,
            'date': self.date,
            'snippet': self.snippet,
            'country': self.country,
            'tone': self.tone,
            'relevance_score': self.relevance_score,
        }


# ============================================================
# URL Utilities (ì¤‘ë³µ ì œê±°ìš©)
# ============================================================

def normalize_url(url: str) -> str:
    """
    URL ì •ê·œí™” - ì¤‘ë³µ íŒë³„ìš©
    ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°ì™€ í”„ë˜ê·¸ë¨¼íŠ¸ë¥¼ ì œê±°í•˜ì—¬ ë™ì¼ ê¸°ì‚¬ íŒë³„
    """
    try:
        parsed = urlparse(url)
        # scheme, netloc, pathë§Œ ìœ ì§€ (query, fragment ì œê±°)
        normalized = urlunparse((
            parsed.scheme,
            parsed.netloc,
            parsed.path.rstrip('/'),  # í›„í–‰ ìŠ¬ë˜ì‹œ ì œê±°
            '', '', ''
        ))
        return normalized.lower()
    except:
        return url.lower()


def deduplicate_articles(articles: List[ArticleResult]) -> List[ArticleResult]:
    """
    ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ì—ì„œ ì¤‘ë³µ URL ì œê±° (ì •ê·œí™” í›„ ë¹„êµ)
    """
    seen_urls: Set[str] = set()
    unique_articles: List[ArticleResult] = []

    for article in articles:
        normalized = normalize_url(article.url)
        if normalized not in seen_urls:
            seen_urls.add(normalized)
            unique_articles.append(article)

    return unique_articles


# ============================================================
# Search Strategy Interface (Strategy Pattern)
# ============================================================

class SearchStrategy(ABC):
    """ê²€ìƒ‰ ì „ëµ ì¸í„°í˜ì´ìŠ¤"""

    @abstractmethod
    def search(self, keywords: List[str], **kwargs) -> List[ArticleResult]:
        """í‚¤ì›Œë“œë¡œ ê¸°ì‚¬ ê²€ìƒ‰"""
        pass

    @abstractmethod
    def is_available(self) -> bool:
        """ê²€ìƒ‰ ì „ëµ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€"""
        pass


# ============================================================
# DOC API Strategy (Primary)
# ============================================================

class GDELTDocAPIStrategy(SearchStrategy):
    """
    GDELT DOC 2.0 API ê²€ìƒ‰ ì „ëµ (Primary)

    ì¥ì :
    - ê¸°ì‚¬ ë³¸ë¬¸ ì „ë¬¸ ê²€ìƒ‰ (Full Text Search)
    - ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸
    - ë¬´ë£Œ

    ë‹¨ì :
    - Rate Limit ì¡´ì¬
    - ìµœê·¼ 3ê°œì›” ë°ì´í„°ë§Œ ê²€ìƒ‰ ê°€ëŠ¥

    [v2 ìˆ˜ì •]
    - timespan íŒŒë¼ë¯¸í„° GDELT ê³µì‹ í˜•ì‹ ì‚¬ìš©
    """

    def __init__(self):
        self.base_url = config.GDELT_DOC_API_URL
        self.timeout = config.GDELT_DOC_TIMEOUT
        self._available = True

    def is_available(self) -> bool:
        return self._available

    def search(self, keywords: List[str], **kwargs) -> List[ArticleResult]:
        """DOC APIë¡œ ê¸°ì‚¬ ë³¸ë¬¸ ì „ë¬¸ ê²€ìƒ‰"""
        if not keywords:
            return []

        try:
            # 1. ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„±
            query = self._build_query(keywords, kwargs.get('domains'))

            # 2. API ìš”ì²­ íŒŒë¼ë¯¸í„°
            params = {
                'query': query,
                'mode': 'artlist',
                'maxrecords': config.GDELT_DOC_MAX_RECORDS,
                'format': 'json',
                'sort': 'DateDesc',
            }

            # [ìˆ˜ì •ë¨] timespanì€ GDELT ê³µì‹ í˜•ì‹ ì‚¬ìš© ("1w", "1m", "3m" ë“±)
            timespan = kwargs.get('timespan', config.GDELT_SEARCH_TIMESPAN)
            if timespan:
                params['timespan'] = timespan

            print(f"ğŸ” [DOC API] ê²€ìƒ‰ ì¿¼ë¦¬: {query[:100]}...")
            print(f"   timespan: {timespan}")

            response = requests.get(
                self.base_url,
                params=params,
                timeout=self.timeout
            )
            response.raise_for_status()

            # 3. ê²°ê³¼ íŒŒì‹±
            data = response.json()
            articles = self._parse_response(data)

            # [ì¶”ê°€ë¨] API ë ˆë²¨ ì¤‘ë³µ ì œê±°
            articles = deduplicate_articles(articles)

            print(f"âœ… [DOC API] {len(articles)}ê°œ ê¸°ì‚¬ ë°œê²¬ (ì¤‘ë³µ ì œê±° í›„)")
            return articles

        except requests.exceptions.Timeout:
            print(f"âš ï¸ [DOC API] íƒ€ì„ì•„ì›ƒ ({self.timeout}ì´ˆ)")
            self._available = False
            return []
        except requests.exceptions.RequestException as e:
            print(f"âš ï¸ [DOC API] ìš”ì²­ ì‹¤íŒ¨: {e}")
            self._available = False
            return []
        except Exception as e:
            print(f"âŒ [DOC API] ì˜ˆì™¸ ë°œìƒ: {e}")
            return []

    def _build_query(self, keywords: List[str], domains: Optional[tuple] = None) -> str:
        """GDELT DOC API ì¿¼ë¦¬ ë¬¸ìì—´ ìƒì„± (ê²½ëŸ‰í™” ë²„ì „)"""

        # [ìˆ˜ì • 1] ìƒìœ„ 3ê°œ í‚¤ì›Œë“œë§Œ ì‚¬ìš© (API ì œí•œ ê³ ë ¤)
        top_keywords = keywords[:3]

        refined_keywords = []
        for kw in top_keywords:
            # [ìˆ˜ì • 2] ì¿¼ë¦¬ê°€ ë„ˆë¬´ ê¸¸ì–´ì§€ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•´ 3ë‹¨ì–´ ì´ìƒì€ í•µì‹¬ë§Œ ì¶”ì¶œí•˜ê±°ë‚˜ ì•ë¶€ë¶„ë§Œ ì‚¬ìš©
            parts = kw.split()
            if len(parts) > 3:
                # ì˜ˆ: "Japan China trade war impact" -> "Japan China trade"
                kw = " ".join(parts[:3])

            # ë”°ì˜´í‘œë¡œ ê°ì‹¸ì„œ êµ¬ë¬¸ ê²€ìƒ‰ (ì •í™•ë„ í–¥ìƒ)
            refined_keywords.append(f'"{kw}"')

        if not refined_keywords:
            return ""

        # [ìˆ˜ì • 3] ë„ë©”ì¸ í•„í„° ì œê±°!
        # (ì¿¼ë¦¬ ê¸¸ì´ í™•ë³´ë¥¼ ìœ„í•´ ì œê±°í•˜ê³ , í’ˆì§ˆ ê´€ë¦¬ëŠ” LLM Judgeì—ê²Œ ìœ„ì„)
        keyword_query = " OR ".join(refined_keywords)
        query = f"({keyword_query})"

        return query

    def _parse_response(self, data: Dict) -> List[ArticleResult]:
        """API ì‘ë‹µì„ ArticleResult ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        articles = []

        if 'articles' not in data:
            return articles

        for item in data['articles']:
            try:
                # ë‚ ì§œ í¬ë§· ë³€í™˜ (20240501T120000Z â†’ 2024-05-01)
                raw_date = item.get('seendate', '')
                formatted_date = self._format_date(raw_date)

                # ë„ë©”ì¸ì—ì„œ ì†ŒìŠ¤ëª… ì¶”ì¶œ
                source = item.get('domain', 'Unknown')

                article = ArticleResult(
                    url=item.get('url', ''),
                    title=item.get('title', ''),
                    source=source,
                    date=formatted_date,
                    snippet=item.get('socialimage', '') or '',
                    country=self._extract_country(item),
                )

                if article.url:  # URLì´ ìˆëŠ” ê²½ìš°ë§Œ ì¶”ê°€
                    articles.append(article)

            except Exception as e:
                print(f"âš ï¸ ê¸°ì‚¬ íŒŒì‹± ì˜¤ë¥˜: {e}")
                continue

        return articles

    def _format_date(self, raw_date: str) -> str:
        """ë‚ ì§œ í¬ë§· ë³€í™˜"""
        if not raw_date:
            return ""
        try:
            # 20240501T120000Z í˜•ì‹
            dt = datetime.strptime(raw_date[:8], '%Y%m%d')
            return dt.strftime('%Y-%m-%d')
        except:
            return raw_date[:10] if len(raw_date) >= 10 else raw_date

    def _extract_country(self, item: Dict) -> str:
        """ë„ë©”ì¸ì—ì„œ êµ­ê°€ ì¶”ë¡ """
        domain = item.get('domain', '')

        # ë„ë©”ì¸ ê¸°ë°˜ êµ­ê°€ ë§¤í•‘
        country_mapping = {
            '.kr': 'KR', '.co.kr': 'KR',
            '.jp': 'JP', '.co.jp': 'JP',
            '.cn': 'CN', '.com.cn': 'CN',
            '.uk': 'GB', '.co.uk': 'GB',
            '.de': 'DE',
            '.fr': 'FR',
            '.ru': 'RU',
        }

        for suffix, country in country_mapping.items():
            if domain.endswith(suffix):
                return country

        return 'US' if domain.endswith('.com') else 'Unknown'


# ============================================================
# BigQuery Strategy (Fallback)
# ============================================================

class GDELTBigQueryStrategy(SearchStrategy):
    """
    GDELT BigQuery GKG ê²€ìƒ‰ ì „ëµ (Fallback)

    ì¥ì :
    - ê³¼ê±° ë°ì´í„° ê²€ìƒ‰ ê°€ëŠ¥
    - êµ¬ì¡°ì  ì¿¼ë¦¬ (í…Œë§ˆ, ì¸ë¬¼, ì¡°ì§)

    ë‹¨ì :
    - ë©”íƒ€ë°ì´í„°ë§Œ ê²€ìƒ‰ (ë³¸ë¬¸ X)
    - ì¿¼ë¦¬ ë¹„ìš© (1TB ë¬´ë£Œ)
    """

    def __init__(self):
        self.client = None
        try:
            self.client = bigquery.Client(project=config.GCP_PROJECT)
            print("âœ… [BigQuery] í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ì„±ê³µ")
        except Exception as e:
            print(f"âš ï¸ [BigQuery] ì—°ê²° ì‹¤íŒ¨: {e}")

    def is_available(self) -> bool:
        return self.client is not None

    def search(self, keywords: List[str], **kwargs) -> List[ArticleResult]:
        """BigQuery GKG í…Œì´ë¸”ì—ì„œ ë©”íƒ€ë°ì´í„° ê²€ìƒ‰"""
        if not self.client or not keywords:
            return []

        try:
            # ì‹œê°„ ë²”ìœ„ ì„¤ì •
            days = kwargs.get('days', config.SEARCH_WINDOW_DAYS)
            end_date = datetime.now()
            start_date = end_date - timedelta(days=days)

            start_int = int(start_date.strftime('%Y%m%d000000'))
            end_int = int(end_date.strftime('%Y%m%d235959'))

            # [ìˆ˜ì •ë¨] URL(DocumentIdentifier) ì™€ì¼ë“œì¹´ë“œ ê²€ìƒ‰ ì „ëµ
            # AllNamesëŠ” ì¼ë°˜ ëª…ì‚¬(trade, war ë“±)ê°€ ì—†ì–´ ê²€ìƒ‰ ì‹¤íŒ¨ í™•ë¥ ì´ ë†’ìŒ.
            # ëŒ€ì‹  URLì—ì„œ ê³µë°±ì„ %ë¡œ ë°”ê¿”ì„œ ìœ ì—°í•˜ê²Œ ê²€ìƒ‰.
            safe_conditions = []
            for kw in keywords[:3]:  # ìƒìœ„ 3ê°œë§Œ
                if "'" in kw:
                    continue  # SQL Injection ë°©ì§€ (ë‹¨ìˆœ)

                # í•µì‹¬: ê³µë°±ì„ %ë¡œ ë³€í™˜ (ì˜ˆ: "trade war" -> "%trade%war%")
                # URLì€ "japan-trade-war" ì²˜ëŸ¼ ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ê³µë°±ìœ¼ë¡œëŠ” ê²€ìƒ‰ ì•ˆë¨
                url_friendly_kw = kw.replace(" ", "%")
                safe_conditions.append(f"DocumentIdentifier LIKE '%{url_friendly_kw}%'")

            if not safe_conditions:
                return []

            keyword_conditions = " OR ".join(safe_conditions)

            # ë„ë©”ì¸ í•„í„° (BigQueryëŠ” ì„±ëŠ¥ ë¬¸ì œì—†ìœ¼ë¯€ë¡œ ìœ ì§€)
            domains = config.TRUSTED_DOMAINS
            domain_filter = ",".join([f"'{d}'" for d in domains])

            query = f"""
            SELECT
                DocumentIdentifier as url,
                SourceCommonName as source,
                FORMAT_DATE('%Y-%m-%d', PARSE_TIMESTAMP('%Y%m%d%H%M%S', CAST(DATE AS STRING))) as date,
                V2Tone as tone,
                Locations
            FROM `gdelt-bq.gdeltv2.gkg_partitioned`
            WHERE DATE >= {start_int}
              AND DATE <= {end_int}
              AND SourceCommonName IN ({domain_filter})
              AND ({keyword_conditions})
              AND DocumentIdentifier IS NOT NULL
            ORDER BY DATE DESC
            LIMIT {config.GDELT_MAX_RESULTS}
            """

            print(f"ğŸ” [BigQuery] ì¿¼ë¦¬ ì‹¤í–‰: {keyword_conditions}")

            results = self.client.query(query).result()
            articles = []

            for row in results:
                article = ArticleResult(
                    url=row.url or '',
                    title='',  # GKGì—ëŠ” ì œëª© ì—†ìŒ
                    source=row.source or 'Unknown',
                    date=str(row.date) if row.date else '',
                    tone=float(row.tone.split(',')[0]) if row.tone else 0.0,
                    country=self._extract_country_from_locations(row.Locations),
                )
                if article.url:
                    articles.append(article)

            # [ì¶”ê°€ë¨] ì¤‘ë³µ ì œê±°
            articles = deduplicate_articles(articles)

            print(f"âœ… [BigQuery] {len(articles)}ê°œ ê¸°ì‚¬ ë°œê²¬ (ì¤‘ë³µ ì œê±° í›„)")
            return articles

        except Exception as e:
            print(f"âŒ [BigQuery] ì¿¼ë¦¬ ì‹¤íŒ¨: {e}")
            return []

    def _extract_country_from_locations(self, locations: str) -> str:
        """Locations í•„ë“œì—ì„œ êµ­ê°€ ì½”ë“œ ì¶”ì¶œ"""
        if not locations:
            return 'Unknown'
        try:
            parts = locations.split('#')
            if len(parts) > 2:
                return parts[2]
        except:
            pass
        return 'Unknown'


# ============================================================
# Main Search Engine (Facade Pattern)
# ============================================================

class GDELTSearcher:
    """
    GDELT í†µí•© ê²€ìƒ‰ ì—”ì§„

    [ì‚¬ìš©ë²•]
    searcher = GDELTSearcher()
    results = searcher.search({'keywords': ['trade war', 'China']})

    [ì „ëµ]
    1. DOC API (Primary) - ë³¸ë¬¸ ì „ë¬¸ ê²€ìƒ‰
    2. BigQuery (Fallback) - DOC API ì‹¤íŒ¨ ì‹œ

    [v2 ìˆ˜ì •]
    - ê¸°ì¡´ ë¡œì§ í˜¸í™˜ì„±: entities, themesë¥¼ keywordsë¡œ ìë™ ë³‘í•©
    """

    def __init__(self):
        # ê²€ìƒ‰ ì „ëµ ì´ˆê¸°í™”
        self.doc_api = GDELTDocAPIStrategy()
        self.bigquery = GDELTBigQueryStrategy()

        print("âœ… [GDELTSearcher] ì´ˆê¸°í™” ì™„ë£Œ")

    def search(self, search_params: dict) -> List[Dict]:
        """
        í†µí•© ê²€ìƒ‰ ë©”ì„œë“œ

        Args:
            search_params: {
                'keywords': ['keyword1', 'keyword2'],  # ê¶Œì¥
                'entities': ['Person', 'Org'],  # ì„ íƒ (ìë™ ë³‘í•©ë¨)
                'themes': ['ECON_TRADE'],  # ì„ íƒ (ìë™ ë³‘í•©ë¨)
                'timespan': '3m',  # ì„ íƒ (DOC APIìš©, ê¸°ë³¸ê°’: configì—ì„œ)
                'days': 30,  # ì„ íƒ (BigQueryìš©)
            }

        Returns:
            ê¸°ì‚¬ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸
        """
        # [ì¶”ê°€ë¨] í˜¸í™˜ì„± ë³´ì™„: entities/themesê°€ ìˆìœ¼ë©´ keywordsë¡œ ë³‘í•©
        keywords = self._merge_search_params(search_params)

        if not keywords:
            print("âš ï¸ ê²€ìƒ‰ í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤")
            return []

        # search_paramsì— ë³‘í•©ëœ keywords ì—…ë°ì´íŠ¸
        merged_params = {**search_params, 'keywords': keywords}

                

        # 1. DOC API ì‹œë„ (Primary)
        if self.doc_api.is_available():
            try:
                # â­• í•´ê²°ì±…: ë”•ì…”ë„ˆë¦¬ì—ì„œ 'keywords' í‚¤ë¥¼ ì œì™¸í•œ ë‚˜ë¨¸ì§€ ì˜µì…˜ë§Œ ë¶„ë¦¬
                api_kwargs = {k: v for k, v in merged_params.items() if k != 'keywords'}
                
                # ë¶„ë¦¬ëœ ì˜µì…˜(**api_kwargs)ë§Œ ì¶”ê°€ë¡œ ì „ë‹¬
                results = self.doc_api.search(keywords, **api_kwargs)
                
                if results:
                    return [r.to_dict() for r in results]
                print("âš ï¸ [DOC API] ê²°ê³¼ ì—†ìŒ, BigQueryë¡œ ì „í™˜")
                
            except Exception as e:
                print(f"âš ï¸ [DOC API] ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
                # ì˜¤ë¥˜ ë°œìƒ ì‹œ BigQueryë¡œ ë„˜ì–´ê°€ë„ë¡ ì˜ˆì™¸ ì²˜ë¦¬
                pass

        # 2. BigQuery Fallback
        if self.bigquery.is_available():
            results = self.bigquery.search(keywords, **merged_params)
            if results:
                return [r.to_dict() for r in results]

        print("âŒ ëª¨ë“  ê²€ìƒ‰ ì „ëµ ì‹¤íŒ¨")
        return []

    def _merge_search_params(self, search_params: dict) -> List[str]:
        """
        [ì¶”ê°€ë¨] ê¸°ì¡´ ë¡œì§ í˜¸í™˜ì„±ì„ ìœ„í•œ íŒŒë¼ë¯¸í„° ë³‘í•©

        entities, themesê°€ ìˆìœ¼ë©´ keywordsì— ë³‘í•©í•˜ì—¬ DOC API ê²€ìƒ‰ì— ì‚¬ìš©
        """
        keywords = list(search_params.get('keywords', []))

        # entities ë³‘í•© (ì¸ë¬¼, ì¡°ì§)
        entities = search_params.get('entities', [])
        if entities:
            keywords.extend(entities)
            print(f"   ğŸ“Œ entities â†’ keywords ë³‘í•©: {entities}")

        # themes ë³‘í•© (GDELT í…Œë§ˆ ì½”ë“œëŠ” ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜)
        themes = search_params.get('themes', [])
        if themes:
            # í…Œë§ˆ ì½”ë“œë¥¼ ê²€ìƒ‰ ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜ (ì˜ˆ: ECON_TRADE â†’ trade)
            theme_keywords = [
                t.replace('_', ' ').lower()
                for t in themes
                if t and len(t) > 2
            ]
            keywords.extend(theme_keywords)
            print(f"   ğŸ“Œ themes â†’ keywords ë³‘í•©: {themes} â†’ {theme_keywords}")

        # locations ë³‘í•© (êµ­ê°€ëª…)
        locations = search_params.get('locations', [])
        if locations:
            keywords.extend(locations)
            print(f"   ğŸ“Œ locations â†’ keywords ë³‘í•©: {locations}")

        # ì¤‘ë³µ ì œê±° ë° ë¹ˆ ë¬¸ìì—´ í•„í„°ë§
        keywords = list(set(k.strip() for k in keywords if k and len(k.strip()) > 1))

        print(f"   ğŸ”‘ ìµœì¢… ê²€ìƒ‰ í‚¤ì›Œë“œ: {keywords}")
        return keywords

    def search_with_fallback(self, search_params: dict) -> List[Dict]:
        """
        Fallbackì´ ë³´ì¥ëœ ê²€ìƒ‰ (í•­ìƒ ê²°ê³¼ ë°˜í™˜ ì‹œë„)
        """
        results = self.search(search_params)

        if not results:
            # í‚¤ì›Œë“œ í™•ì¥ ì‹œë„: ì²« ë²ˆì§¸ í‚¤ì›Œë“œë§Œìœ¼ë¡œ ì¬ê²€ìƒ‰
            keywords = self._merge_search_params(search_params)
            if keywords:
                simplified_params = {**search_params, 'keywords': keywords[:1]}
                results = self.search(simplified_params)

        return results
